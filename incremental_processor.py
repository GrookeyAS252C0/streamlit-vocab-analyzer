#!/usr/bin/env python3
"""
PDFãƒ•ã‚©ãƒ«ãƒ€ã‚’ç›£è¦–ã—ã€æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦OCRå‡¦ç†ï¼‹èªå½™åˆ†æã‚’å®Ÿè¡Œã—ã€
extraction_results_pure_english.jsonã«å¢—åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import json
import logging
from pathlib import Path
from typing import List, Dict, Set
from pdf_text_extractor import PDFTextExtractor
from vocabulary_analyzer_multi import MultiVocabularyAnalyzer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class IncrementalProcessor:
    def __init__(self, pdf_folder: str = "PDF", extraction_results_file: str = "extraction_results_pure_english.json"):
        """
        å¢—åˆ†å‡¦ç†ã‚¯ãƒ©ã‚¹
        
        Args:
            pdf_folder: PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€
            extraction_results_file: æŠ½å‡ºçµæœã‚’ä¿å­˜ã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«
        """
        self.pdf_folder = Path(pdf_folder)
        self.extraction_results_file = Path(extraction_results_file)
        self.extractor = PDFTextExtractor()
        self.analyzer = MultiVocabularyAnalyzer()
        
    def get_processed_files(self) -> Set[str]:
        """
        æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        
        Returns:
            å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚»ãƒƒãƒˆ
        """
        if not self.extraction_results_file.exists():
            logger.info(f"{self.extraction_results_file} ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™")
            return set()
        
        try:
            with open(self.extraction_results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            processed_files = set()
            for entry in data.get('extracted_data', []):
                source_file = entry.get('source_file', '')
                if source_file:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã‚’æŠ½å‡ºï¼ˆãƒ‘ã‚¹ã‚’é™¤å»ï¼‰
                    filename = Path(source_file).name
                    processed_files.add(filename)
            
            logger.info(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(processed_files)}")
            return processed_files
            
        except (json.JSONDecodeError, FileNotFoundError) as e:
            logger.warning(f"extraction_results_pure_english.jsonã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return set()
    
    def get_pdf_files(self) -> List[Path]:
        """
        PDFãƒ•ã‚©ãƒ«ãƒ€å†…ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
        
        Returns:
            PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        if not self.pdf_folder.exists():
            logger.error(f"PDFãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.pdf_folder}")
            return []
        
        pdf_files = list(self.pdf_folder.glob("*.pdf"))
        logger.info(f"PDFãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(pdf_files)}")
        return pdf_files
    
    def detect_unprocessed_files(self) -> List[Path]:
        """
        æœªå‡¦ç†ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º
        
        Returns:
            æœªå‡¦ç†PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        processed_files = self.get_processed_files()
        pdf_files = self.get_pdf_files()
        
        unprocessed_files = []
        for pdf_file in pdf_files:
            if pdf_file.name not in processed_files:
                unprocessed_files.append(pdf_file)
        
        logger.info(f"æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(unprocessed_files)}")
        for file in unprocessed_files:
            logger.info(f"  ğŸ“„ æœªå‡¦ç†: {file.name}")
        
        return unprocessed_files
    
    def process_single_file(self, pdf_file: Path) -> Dict:
        """
        å˜ä¸€PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’OCRå‡¦ç†
        
        Args:
            pdf_file: å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            æŠ½å‡ºçµæœã®è¾æ›¸
        """
        logger.info(f"ğŸš€ OCRå‡¦ç†é–‹å§‹: {pdf_file.name}")
        
        try:
            # OCRå‡¦ç†å®Ÿè¡Œ
            extracted_data = self.extractor.process_pdf(
                str(pdf_file), 
                enhancement_level="aggressive"
            )
            
            if extracted_data and extracted_data.get('extracted_words'):
                logger.info(f"âœ… OCRå‡¦ç†å®Œäº†: {pdf_file.name} - {len(extracted_data['extracted_words'])}èªæŠ½å‡º")
                return extracted_data
            else:
                logger.warning(f"âš ï¸  OCRå‡¦ç†ã§æœ‰åŠ¹ãªè‹±èªãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {pdf_file.name}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {pdf_file.name} - {str(e)}")
            return None
    
    def append_to_extraction_results(self, new_data: Dict) -> bool:
        """
        æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’extraction_results_pure_english.jsonã«è¿½è¨˜
        
        Args:
            new_data: è¿½åŠ ã™ã‚‹ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            if self.extraction_results_file.exists():
                with open(self.extraction_results_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                # æ–°è¦ä½œæˆ
                existing_data = {
                    "extraction_summary": {
                        "total_source_files": 0,
                        "total_words_extracted": 0,
                        "average_ocr_confidence": 0.0,
                        "processing_level": "aggressive",
                        "extraction_method": "pure_english_only",
                        "japanese_content": "completely_ignored"
                    },
                    "extracted_data": []
                }
            
            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            existing_data["extracted_data"].append(new_data)
            
            # ã‚µãƒãƒªãƒ¼ã‚’æ›´æ–°
            all_entries = existing_data["extracted_data"]
            total_files = len(all_entries)
            total_words = sum(entry.get("word_count", 0) for entry in all_entries)
            confidences = [entry.get("ocr_confidence", 0.0) for entry in all_entries if entry.get("ocr_confidence")]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
            
            existing_data["extraction_summary"].update({
                "total_source_files": total_files,
                "total_words_extracted": total_words,
                "average_ocr_confidence": avg_confidence
            })
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(self.extraction_results_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿è¿½è¨˜å®Œäº†: {new_data['source_file']}")
            logger.info(f"   ğŸ“Š ç¾åœ¨ã®ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
            logger.info(f"   ğŸ“ˆ ç¾åœ¨ã®ç·å˜èªæ•°: {total_words:,}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿è¿½è¨˜ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def run_vocabulary_analysis(self) -> bool:
        """
        èªå½™åˆ†æã‚’å®Ÿè¡Œ
        
        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            logger.info("ğŸ” èªå½™åˆ†æé–‹å§‹...")
            self.analyzer.run_analysis()
            logger.info("âœ… èªå½™åˆ†æå®Œäº†")
            return True
        except Exception as e:
            logger.error(f"âŒ èªå½™åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
            return False
    
    def process_all_unprocessed(self) -> int:
        """
        ã™ã¹ã¦ã®æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        
        Returns:
            å‡¦ç†ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°
        """
        unprocessed_files = self.detect_unprocessed_files()
        
        if not unprocessed_files:
            logger.info("âœ… æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“")
            return 0
        
        processed_count = 0
        
        for pdf_file in unprocessed_files:
            logger.info(f"ğŸ“„ å‡¦ç†ä¸­ ({processed_count + 1}/{len(unprocessed_files)}): {pdf_file.name}")
            
            # OCRå‡¦ç†
            extracted_data = self.process_single_file(pdf_file)
            
            if extracted_data:
                # ãƒ‡ãƒ¼ã‚¿è¿½è¨˜
                if self.append_to_extraction_results(extracted_data):
                    processed_count += 1
                else:
                    logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿è¿½è¨˜å¤±æ•—: {pdf_file.name}")
            else:
                logger.warning(f"âš ï¸  å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {pdf_file.name}")
        
        if processed_count > 0:
            logger.info(f"ğŸ” æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¿½åŠ ã•ã‚ŒãŸãŸã‚èªå½™åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            self.run_vocabulary_analysis()
        
        logger.info(f"ğŸ‰ å‡¦ç†å®Œäº†: {processed_count}/{len(unprocessed_files)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã—ãŸ")
        return processed_count


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ å¢—åˆ†PDFãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼é–‹å§‹")
    print("=" * 60)
    
    processor = IncrementalProcessor()
    
    # æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡ºã¨å‡¦ç†
    processed_count = processor.process_all_unprocessed()
    
    if processed_count > 0:
        print(f"\nâœ… {processed_count} ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print("ğŸ“Š æœ€æ–°ã®åˆ†æçµæœ:")
        print("   â€¢ extraction_results_pure_english.json - æŠ½å‡ºãƒ‡ãƒ¼ã‚¿")
        print("   â€¢ multi_vocabulary_analysis_report.json - èªå½™åˆ†æçµæœ")
    else:
        print("\nâœ… ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™")
    
    print("=" * 60)
    print("ğŸ¯ ä»Šå¾Œã®ä½¿ç”¨æ–¹æ³•:")
    print("   1. æ–°ã—ã„PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’PDF/ãƒ•ã‚©ãƒ«ãƒ€ã«è¿½åŠ ")
    print("   2. ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (python incremental_processor.py) ã‚’å®Ÿè¡Œ")
    print("   3. è‡ªå‹•çš„ã«æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãŒå‡¦ç†ã•ã‚Œã¾ã™")


if __name__ == "__main__":
    main()