#!/usr/bin/env python3
"""
æœ€æ–°ã®OCRçµæœã¨èªå½™åˆ†æçµæœã‹ã‚‰Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path

def extract_university_name(source_file):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨åã‚’æŠ½å‡º"""
    filename = Path(source_file).stem
    
    # å¤§å­¦åãƒ»å­¦éƒ¨åã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
    if "æ—©ç¨²ç”°å¤§å­¦" in filename:
        if "æ³•å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ³•å­¦éƒ¨"
        elif "æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨"
        elif "å•†å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_å•†å­¦éƒ¨"
        elif "æ–‡å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ–‡å­¦éƒ¨"
        elif "æ–‡åŒ–æ§‹æƒ³å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ–‡åŒ–æ§‹æƒ³å­¦éƒ¨"
        elif "æ•™è‚²å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ•™è‚²å­¦éƒ¨"
        elif "ç¤¾ä¼šç§‘å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_ç¤¾ä¼šç§‘å­¦éƒ¨"
        elif "äººé–“ç§‘å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_äººé–“ç§‘å­¦éƒ¨"
        elif "å›½éš›æ•™é¤Šå­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_å›½éš›æ•™é¤Šå­¦éƒ¨"
        elif "ç†å·¥å­¦éƒ¨" in filename or "åŸºå¹¹ç†å·¥" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_ç†å·¥å­¦éƒ¨"
    elif "æ±äº¬å¤§å­¦" in filename:
        return "æ±äº¬å¤§å­¦"
    
    return filename

def load_extraction_results():
    """OCRæŠ½å‡ºçµæœã®èª­ã¿è¾¼ã¿"""
    extraction_file = "../../extraction_results_pure_english.json"
    if not os.path.exists(extraction_file):
        print(f"âŒ OCRçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {extraction_file}")
        return None
    
    try:
        with open(extraction_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ OCRçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_vocabulary_analysis():
    """èªå½™åˆ†æçµæœã®èª­ã¿è¾¼ã¿"""
    vocab_file = "../../multi_vocabulary_analysis_report.json"
    if not os.path.exists(vocab_file):
        print(f"âŒ èªå½™åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vocab_file}")
        return None
    
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ èªå½™åˆ†æèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def create_streamlit_data():
    """Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    print("ğŸ”„ Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    extraction_data = load_extraction_results()
    vocab_data = load_vocabulary_analysis()
    
    if not extraction_data or not vocab_data:
        return False
    
    # åŸºæœ¬æ§‹é€ ä½œæˆ
    streamlit_data = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "source_files": ["extraction_results_pure_english.json", "multi_vocabulary_analysis_report.json"],
            "data_version": "2.0"
        }
    }
    
    # overall_summaryä½œæˆ
    extraction_summary = extraction_data.get("extraction_summary", {})
    streamlit_data["overall_summary"] = {
        "total_source_files": extraction_summary.get("total_source_files", 0),
        "total_words_extracted": extraction_summary.get("total_words_extracted", 0),
        "average_ocr_confidence": round(extraction_summary.get("average_ocr_confidence", 0) * 100, 2),
        "analysis_timestamp": datetime.now().isoformat(),
        "vocabulary_books": ["Target 1900", "Target 1400", "ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª", "LEAP", "é‰„å£"]
    }
    
    # vocabulary_summaryä½œæˆ
    multi_vocab_coverage = vocab_data.get("multi_vocabulary_coverage", {})
    vocab_coverage = multi_vocab_coverage.get("vocabulary_coverage", {})
    streamlit_data["vocabulary_summary"] = vocab_coverage
    
    # university_analysisä½œæˆ
    university_analysis = {}
    extracted_data = extraction_data.get("extracted_data", [])
    university_vocab_data = vocab_data.get("universities_analysis", {})
    
    for item in extracted_data:
        source_file = item.get("source_file", "")
        university_name = extract_university_name(source_file)
        
        # OCRãƒ‡ãƒ¼ã‚¿
        ocr_info = {
            "source_file": source_file,
            "total_words": item.get("word_count", 0),
            "unique_words": len(item.get("extracted_words", [])),
            "ocr_confidence": round(item.get("ocr_confidence", 0) * 100, 2),
            "pages_processed": item.get("pages_processed", 0)
        }
        
        # èªå½™åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        if university_name in university_vocab_data:
            vocab_coverage = university_vocab_data[university_name].get("vocabulary_coverage", {})
            ocr_info["vocabulary_coverage"] = vocab_coverage
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®èªå½™ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰
            ocr_info["vocabulary_coverage"] = {
                "Target 1900": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0},
                "Target 1400": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0},
                "ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0},
                "LEAP": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0},
                "é‰„å£": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0}
            }
        
        university_analysis[university_name] = ocr_info
    
    streamlit_data["university_analysis"] = university_analysis
    
    # é »å‡ºå˜èªãƒ‡ãƒ¼ã‚¿
    streamlit_data["top_frequent_words"] = vocab_data.get("top_frequent_words", {})
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    output_file = "../data/analysis_data.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(streamlit_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {output_file}")
        print(f"ğŸ“Š å¤§å­¦æ•°: {len(university_analysis)}")
        print(f"ğŸ“š å˜èªå¸³æ•°: {len(streamlit_data['vocabulary_summary'])}")
        print(f"ğŸ“ˆ ç·å˜èªæ•°: {streamlit_data['overall_summary']['total_words_extracted']:,}")
        
        # å¤§å­¦ãƒªã‚¹ãƒˆè¡¨ç¤º
        print("\nğŸ« å«ã¾ã‚Œã‚‹å¤§å­¦ãƒ»å­¦éƒ¨:")
        for i, univ in enumerate(university_analysis.keys(), 1):
            print(f"  {i}. {univ}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ Streamlitç”¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
    
    if create_streamlit_data():
        print("\nğŸ‰ å‡¦ç†å®Œäº†!")
    else:
        print("\nâŒ å‡¦ç†å¤±æ•—")
        sys.exit(1)

if __name__ == "__main__":
    main()