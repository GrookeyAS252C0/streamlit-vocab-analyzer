#!/usr/bin/env python3
"""
æœ€æ–°ã®OCRçµæœã¨èªå½™åˆ†æçµæœã‹ã‚‰Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
"""

import json
import os
import sys
import re
from datetime import datetime
from pathlib import Path

def extract_university_name(source_file):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨åã‚’æŠ½å‡º"""
    filename = Path(source_file).stem
    
    # å¤§å­¦åãƒ»å­¦éƒ¨åã®æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
    if "æ—©ç¨²ç”°å¤§å­¦" in filename:
        if "æ³•å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ³•å­¦éƒ¨"
        elif "æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨"
        elif "å•†å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_å•†å­¦éƒ¨"
        elif "æ–‡å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ–‡å­¦éƒ¨"
        elif "æ–‡åŒ–æ§‹æƒ³å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ–‡åŒ–æ§‹æƒ³å­¦éƒ¨"
        elif "æ•™è‚²å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_æ•™è‚²å­¦éƒ¨"
        elif "ç¤¾ä¼šç§‘å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_ç¤¾ä¼šç§‘å­¦éƒ¨"
        elif "äººé–“ç§‘å­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_äººé–“ç§‘å­¦éƒ¨"
        elif "å›½éš›æ•™é¤Šå­¦éƒ¨" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_å›½éš›æ•™é¤Šå­¦éƒ¨"
        elif "ç†å·¥å­¦éƒ¨" in filename or "åŸºå¹¹ç†å·¥" in filename:
            return "æ—©ç¨²ç”°å¤§å­¦_ç†å·¥å­¦éƒ¨"
    elif "æ±äº¬å¤§å­¦" in filename:
        return "æ±äº¬å¤§å­¦"
    elif "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦" in filename:
        if "åŒ»å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_åŒ»å­¦éƒ¨"
        elif "è–¬å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_è–¬å­¦éƒ¨"
        elif "çµŒæ¸ˆå­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_çµŒæ¸ˆå­¦éƒ¨"
        elif "å•†å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_å•†å­¦éƒ¨"
        elif "æ³•å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_æ³•å­¦éƒ¨"
        elif "æ–‡å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_æ–‡å­¦éƒ¨"
        elif "ç†å·¥å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_ç†å·¥å­¦éƒ¨"
        elif "ç’°å¢ƒæƒ…å ±å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_ç’°å¢ƒæƒ…å ±å­¦éƒ¨"
        elif "ç·åˆæ”¿ç­–å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_ç·åˆæ”¿ç­–å­¦éƒ¨"
        elif "çœ‹è­·åŒ»ç™‚å­¦éƒ¨" in filename:
            return "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_çœ‹è­·åŒ»ç™‚å­¦éƒ¨"
    
    return filename

def calculate_sentence_stats(english_passages):
    """è‹±æ–‡ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æ–‡ã®çµ±è¨ˆã‚’è¨ˆç®—"""
    if not english_passages:
        return {"total_sentences": 0, "avg_words_per_sentence": 0.0, "total_words_in_sentences": 0}
    
    total_sentences = 0
    total_words = 0
    
    for passage in english_passages:
        # æ–‡ã‚’åˆ†å‰²ï¼ˆ.ã€!ã€?ã§çµ‚ã‚ã‚‹æ–‡ã‚’æ¤œå‡ºï¼‰
        sentences = re.split(r'[.!?]+', passage)
        # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»ã—ã€æ„å‘³ã®ã‚ã‚‹æ–‡ã®ã¿ã‚«ã‚¦ãƒ³ãƒˆï¼ˆçŸ­ã™ãã‚‹æ–‡ã¯é™¤å¤–ï¼‰
        valid_sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        total_sentences += len(valid_sentences)
        
        # å„æ–‡ã®å˜èªæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for sentence in valid_sentences:
            words = sentence.split()
            total_words += len(words)
    
    avg_words_per_sentence = total_words / total_sentences if total_sentences > 0 else 0.0
    
    return {
        "total_sentences": total_sentences,
        "avg_words_per_sentence": round(avg_words_per_sentence, 1),
        "total_words_in_sentences": total_words
    }

def create_university_consolidated_data(university_analysis):
    """å¤§å­¦çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå­¦éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼‰"""
    consolidated = {}
    
    # å¤§å­¦ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    university_groups = {}
    for univ_name, univ_data in university_analysis.items():
        if "_" in univ_name:
            base_univ = univ_name.split("_")[0]
        else:
            base_univ = univ_name
        
        if base_univ not in university_groups:
            university_groups[base_univ] = []
        university_groups[base_univ].append((univ_name, univ_data))
    
    # è¤‡æ•°å­¦éƒ¨ãŒã‚ã‚‹å¤§å­¦ã®ã¿çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    for base_univ, departments in university_groups.items():
        if len(departments) > 1:  # è¤‡æ•°å­¦éƒ¨ãŒã‚ã‚‹å ´åˆã®ã¿
            print(f"ğŸ”„ {base_univ}ã®çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­... ({len(departments)}å­¦éƒ¨)")
            
            # åŸºæœ¬çµ±è¨ˆã®åˆç®—
            total_words = sum([data.get("total_words", 0) for _, data in departments])
            total_unique_words = sum([data.get("unique_words", 0) for _, data in departments])
            total_pages = sum([data.get("pages_processed", 0) for _, data in departments])
            total_sentences = sum([data.get("total_sentences", 0) for _, data in departments])
            
            # å¹³å‡å€¤ã®è¨ˆç®—
            avg_confidence = sum([data.get("ocr_confidence", 0) for _, data in departments]) / len(departments)
            total_words_in_sentences = sum([data.get("avg_words_per_sentence", 0) * data.get("total_sentences", 0) for _, data in departments])
            avg_words_per_sentence = total_words_in_sentences / total_sentences if total_sentences > 0 else 0
            
            # èªå½™ã‚«ãƒãƒ¬ãƒƒã‚¸ã®çµ±åˆï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
            vocabulary_coverage = {}
            for vocab_name in ["Target 1900", "Target 1400", "ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª", "LEAP", "é‰„å£"]:
                total_matched = 0
                weighted_coverage = 0
                weighted_precision = 0
                total_weight = 0
                
                for _, data in departments:
                    dept_coverage = data.get("vocabulary_coverage", {}).get(vocab_name, {})
                    matched_count = dept_coverage.get("matched_words_count", 0)
                    coverage_rate = dept_coverage.get("target_coverage_rate", 0)
                    precision = dept_coverage.get("extraction_precision", 0)
                    dept_words = data.get("total_words", 0)
                    
                    total_matched += matched_count
                    if dept_words > 0:
                        weighted_coverage += coverage_rate * dept_words
                        weighted_precision += precision * dept_words
                        total_weight += dept_words
                
                # é‡ã¿ä»˜ãå¹³å‡ã‚’è¨ˆç®—
                avg_coverage = weighted_coverage / total_weight if total_weight > 0 else 0
                avg_precision = weighted_precision / total_weight if total_weight > 0 else 0
                
                vocabulary_coverage[vocab_name] = {
                    "matched_words_count": total_matched,
                    "target_coverage_rate": round(avg_coverage, 2),
                    "extraction_precision": round(avg_precision, 2)
                }
            
            # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            consolidated_key = f"{base_univ}ï¼ˆå…¨å­¦éƒ¨ï¼‰"
            consolidated[consolidated_key] = {
                "source_file": f"{base_univ}_çµ±åˆãƒ‡ãƒ¼ã‚¿",
                "total_words": total_words,
                "unique_words": total_unique_words,
                "ocr_confidence": round(avg_confidence, 2),
                "pages_processed": total_pages,
                "total_sentences": total_sentences,
                "avg_words_per_sentence": round(avg_words_per_sentence, 1),
                "vocabulary_coverage": vocabulary_coverage,
                "is_consolidated": True,  # çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ©ã‚°
                "department_count": len(departments),
                "departments": [name for name, _ in departments]
            }
            
            print(f"âœ… {consolidated_key}: {len(departments)}å­¦éƒ¨çµ±åˆå®Œäº†")
    
    return consolidated

def load_extraction_results():
    """OCRæŠ½å‡ºçµæœã®èª­ã¿è¾¼ã¿"""
    extraction_file = "/Users/takashikemmoku/Desktop/wordsearch/extraction_results_pure_english.json"
    if not os.path.exists(extraction_file):
        print(f"âŒ OCRçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {extraction_file}")
        return None
    
    try:
        with open(extraction_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ OCRçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def load_vocabulary_analysis():
    """èªå½™åˆ†æçµæœã®èª­ã¿è¾¼ã¿"""
    vocab_file = "/Users/takashikemmoku/Desktop/wordsearch/multi_vocabulary_analysis_report.json"
    if not os.path.exists(vocab_file):
        print(f"âŒ èªå½™åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {vocab_file}")
        return None
    
    try:
        with open(vocab_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ èªå½™åˆ†æèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def create_streamlit_data():
    """Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    print("ğŸ”„ Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹...")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    extraction_data = load_extraction_results()
    vocab_data = load_vocabulary_analysis()
    
    if not extraction_data or not vocab_data:
        return False
    
    # åŸºæœ¬æ§‹é€ ä½œæˆ
    streamlit_data = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "source_files": ["extraction_results_pure_english.json", "multi_vocabulary_analysis_report.json"],
            "data_version": "2.0"
        }
    }
    
    # overall_summaryä½œæˆï¼ˆå®Ÿéš›ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—ï¼‰
    extraction_summary = extraction_data.get("extraction_summary", {})
    
    # å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆã‚’è¨ˆç®—
    extracted_data = extraction_data.get("extracted_data", [])
    
    # å„ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰å˜èªæ•°ã‚’è¨ˆç®—
    total_words_calculated = 0
    for item in extracted_data:
        extracted_words_count = len(item.get("extracted_words", []))
        # æ–‡ç« çµ±è¨ˆãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚‚è€ƒæ…®
        if "total_sentences" in item and "avg_words_per_sentence" in item:
            sentence_word_count = int(item.get("total_sentences", 0) * item.get("avg_words_per_sentence", 0))
            total_words_calculated += max(extracted_words_count, sentence_word_count)
        else:
            total_words_calculated += extracted_words_count
    
    total_files = len(extracted_data)
    
    # OCRä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆ0ã§ãªã„å€¤ã®ã¿ï¼‰
    ocr_confidences = [item.get("ocr_confidence", 0) for item in extracted_data if item.get("ocr_confidence", 0) > 0]
    avg_ocr_confidence = sum(ocr_confidences) / len(ocr_confidences) if ocr_confidences else 0
    
    streamlit_data["overall_summary"] = {
        "total_source_files": total_files,
        "total_words_extracted": total_words_calculated,
        "average_ocr_confidence": round(avg_ocr_confidence * 100, 2) if avg_ocr_confidence < 1 else round(avg_ocr_confidence, 2),
        "analysis_timestamp": datetime.now().isoformat(),
        "vocabulary_books": ["Target 1900", "Target 1400", "ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª", "LEAP", "é‰„å£"]
    }
    
    # vocabulary_summaryä½œæˆ
    multi_vocab_coverage = vocab_data.get("multi_vocabulary_coverage", {})
    vocab_coverage = multi_vocab_coverage.get("vocabulary_coverage", {})
    streamlit_data["vocabulary_summary"] = vocab_coverage
    
    # university_analysisä½œæˆ
    university_analysis = {}
    extracted_data = extraction_data.get("extracted_data", [])
    university_vocab_data = vocab_data.get("university_analysis", {})
    
    print(f"ğŸ” èªå½™åˆ†æãƒ‡ãƒ¼ã‚¿ã®å¤§å­¦ã‚­ãƒ¼: {list(university_vocab_data.keys())}")
    
    for item in extracted_data:
        source_file = item.get("source_file", "")
        university_name = extract_university_name(source_file)
        
        # æ–‡ç« çµ±è¨ˆã‚’å–å¾—ã¾ãŸã¯è¨ˆç®—
        if "total_sentences" in item and "avg_words_per_sentence" in item:
            # æ—¢ã«è¨ˆç®—æ¸ˆã¿ã®å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            sentence_stats = {
                "total_sentences": item.get("total_sentences", 0),
                "avg_words_per_sentence": item.get("avg_words_per_sentence", 0.0)
            }
        else:
            # æœªè¨ˆç®—ã®å ´åˆã¯æ–°ãŸã«è¨ˆç®—
            english_passages = item.get("english_passages", []) or item.get("pure_english_text", [])
            sentence_stats = calculate_sentence_stats(english_passages)
        
        # å˜èªæ•°ã®è¨ˆç®—ï¼ˆextracted_wordsã®æ•°ã¨avg_words_per_sentence * total_sentencesã®æœ€å¤§å€¤ã‚’ä½¿ç”¨ï¼‰
        extracted_words_count = len(item.get("extracted_words", []))
        sentence_word_count = int(sentence_stats["total_sentences"] * sentence_stats["avg_words_per_sentence"])
        total_words = max(extracted_words_count, sentence_word_count)
        
        # OCRãƒ‡ãƒ¼ã‚¿
        ocr_info = {
            "source_file": source_file,
            "total_words": total_words,
            "unique_words": extracted_words_count,
            "ocr_confidence": round(item.get("ocr_confidence", 0) * 100, 2),
            "pages_processed": item.get("pages_processed", 0),
            "total_sentences": sentence_stats["total_sentences"],
            "avg_words_per_sentence": sentence_stats["avg_words_per_sentence"]
        }
        
        # èªå½™åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼ˆè¤‡æ•°ã®ã‚­ãƒ¼å½¢å¼ã‚’è©¦è¡Œï¼‰
        vocab_coverage = None
        potential_keys = [
            university_name,  # æ—©ç¨²ç”°å¤§å­¦_æ³•å­¦éƒ¨
            university_name.split('_')[0],  # æ—©ç¨²ç”°å¤§å­¦
            source_file.replace('.pdf', ''),  # å…ƒãƒ•ã‚¡ã‚¤ãƒ«å
            source_file  # PDFãƒ•ã‚¡ã‚¤ãƒ«å
        ]
        
        for key in potential_keys:
            if key in university_vocab_data:
                vocab_coverage = university_vocab_data[key].get("vocabulary_coverage", {})
                print(f"âœ… ãƒãƒƒãƒ: {university_name} -> {key}")
                break
        
        if vocab_coverage:
            ocr_info["vocabulary_coverage"] = vocab_coverage
        else:
            print(f"âš ï¸  èªå½™ãƒ‡ãƒ¼ã‚¿ãªã—: {university_name}")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®èªå½™ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰
            ocr_info["vocabulary_coverage"] = {
                "Target 1900": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0},
                "Target 1400": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0},
                "ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0},
                "LEAP": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0},
                "é‰„å£": {"matched_words_count": 0, "target_coverage_rate": 0.0, "extraction_precision": 0.0}
            }
        
        university_analysis[university_name] = ocr_info
    
    # å¤§å­¦çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    university_consolidated = create_university_consolidated_data(university_analysis)
    print(f"ğŸ” çµ±åˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆçµæœ: {len(university_consolidated)}ä»¶")
    for key in university_consolidated.keys():
        print(f"  - {key}")
    
    # å­¦éƒ¨åˆ¥ãƒ‡ãƒ¼ã‚¿ã¨çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    combined_analysis = {**university_analysis, **university_consolidated}
    print(f"ğŸ” çµåˆå¾Œãƒ‡ãƒ¼ã‚¿: å­¦éƒ¨{len(university_analysis)} + çµ±åˆ{len(university_consolidated)} = ç·è¨ˆ{len(combined_analysis)}")
    streamlit_data["university_analysis"] = combined_analysis
    
    # å…¨ä½“ã®æ–‡ç« çµ±è¨ˆã‚’è¨ˆç®—ï¼ˆå­¦éƒ¨åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ï¼‰
    total_sentences = sum([info.get("total_sentences", 0) for info in university_analysis.values()])
    total_words_in_sentences = sum([info.get("avg_words_per_sentence", 0) * info.get("total_sentences", 0) for info in university_analysis.values()])
    overall_avg_words = total_words_in_sentences / total_sentences if total_sentences > 0 else 0
    
    streamlit_data["sentence_statistics"] = {
        "total_sentences": total_sentences,
        "overall_avg_words_per_sentence": round(overall_avg_words, 1)
    }
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    output_file = "data/analysis_data.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(streamlit_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {output_file}")
        print(f"ğŸ“Š å­¦éƒ¨åˆ¥ãƒ‡ãƒ¼ã‚¿: {len(university_analysis)}")
        print(f"ğŸ« çµ±åˆãƒ‡ãƒ¼ã‚¿: {len(university_consolidated)}")
        print(f"ğŸ“Š ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(combined_analysis)}")
        print(f"ğŸ“š å˜èªå¸³æ•°: {len(streamlit_data['vocabulary_summary'])}")
        print(f"ğŸ“ˆ ç·å˜èªæ•°: {streamlit_data['overall_summary']['total_words_extracted']:,}")
        print(f"ğŸ“ ç·æ–‡æ•°: {streamlit_data['sentence_statistics']['total_sentences']:,}")
        print(f"ğŸ“– å¹³å‡èªæ•°/æ–‡: {streamlit_data['sentence_statistics']['overall_avg_words_per_sentence']:.1f}")
        
        # å­¦éƒ¨åˆ¥ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
        print("\nğŸ« å­¦éƒ¨åˆ¥ãƒ‡ãƒ¼ã‚¿:")
        for i, univ in enumerate(university_analysis.keys(), 1):
            sentences = university_analysis[univ].get('total_sentences', 0)
            avg_words = university_analysis[univ].get('avg_words_per_sentence', 0)
            print(f"  {i}. {univ}: {sentences}æ–‡, {avg_words:.1f}èª/æ–‡")
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
        if university_consolidated:
            print("\nğŸ›ï¸ å¤§å­¦çµ±åˆãƒ‡ãƒ¼ã‚¿:")
            for i, (univ, data) in enumerate(university_consolidated.items(), 1):
                sentences = data.get('total_sentences', 0)
                avg_words = data.get('avg_words_per_sentence', 0)
                dept_count = data.get('department_count', 0)
                print(f"  {i}. {univ}: {sentences}æ–‡, {avg_words:.1f}èª/æ–‡ ({dept_count}å­¦éƒ¨çµ±åˆ)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ Streamlitç”¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
    
    if create_streamlit_data():
        print("\nğŸ‰ å‡¦ç†å®Œäº†!")
    else:
        print("\nâŒ å‡¦ç†å¤±æ•—")
        sys.exit(1)

if __name__ == "__main__":
    main()