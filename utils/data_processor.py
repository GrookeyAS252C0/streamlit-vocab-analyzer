#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
æ—¢å­˜ã®åˆ†æçµæœJSONã‚’Streamlitç”¨ã«è»½é‡åŒ–ãƒ»æ¨™æº–åŒ–
"""

import json
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

def extract_streamlit_data(multi_vocab_report_path: str, output_path: str = None) -> Dict:
    """
    æ—¢å­˜ã®è¤‡æ•°å˜èªå¸³åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    
    Args:
        multi_vocab_report_path: è¤‡æ•°å˜èªå¸³åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‘ã‚¹
        output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        
    Returns:
        Streamlitç”¨ã®è»½é‡åŒ–ãƒ‡ãƒ¼ã‚¿
    """
    
    # å…ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(multi_vocab_report_path, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    metadata = original_data.get('analysis_metadata', {})
    extraction_summary = metadata.get('extraction_summary', {})
    
    # å…¨ä½“ã‚µãƒãƒªãƒ¼
    overall_summary = {
        "total_source_files": extraction_summary.get('total_source_files', 0),
        "total_words_extracted": extraction_summary.get('total_words_extracted', 0),
        "average_ocr_confidence": round(extraction_summary.get('average_ocr_confidence', 0) * 100, 2),
        "analysis_timestamp": metadata.get('analysis_timestamp', ''),
        "vocabulary_books": metadata.get('vocabulary_books', [])
    }
    
    # è¤‡æ•°å˜èªå¸³ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿
    vocab_coverage = original_data.get('multi_vocabulary_coverage', {}).get('vocabulary_coverage', {})
    
    # å˜èªå¸³åˆ¥ã‚µãƒãƒªãƒ¼ï¼ˆè»½é‡åŒ–ï¼‰
    vocabulary_summary = {}
    for vocab_name, vocab_data in vocab_coverage.items():
        vocabulary_summary[vocab_name] = {
            "target_total_words": vocab_data.get('target_total_words', 0),
            "matched_words_count": vocab_data.get('matched_words_count', 0),
            "target_coverage_rate": vocab_data.get('target_coverage_rate', 0),
            "extraction_precision": vocab_data.get('extraction_precision', 0)
        }
    
    # å¤§å­¦åˆ¥åˆ†æãƒ‡ãƒ¼ã‚¿
    university_analysis = original_data.get('university_analysis', {})
    
    # å¤§å­¦åˆ¥ãƒ‡ãƒ¼ã‚¿ã®è»½é‡åŒ–
    university_summary = {}
    for univ_name, univ_data in university_analysis.items():
        # å„å˜èªå¸³ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’è»½é‡åŒ–
        vocab_coverage_light = {}
        vocab_coverage_data = univ_data.get('vocabulary_coverage', {})
        
        for vocab_name, vocab_stats in vocab_coverage_data.items():
            vocab_coverage_light[vocab_name] = {
                "matched_words_count": vocab_stats.get('matched_words_count', 0),
                "target_coverage_rate": vocab_stats.get('target_coverage_rate', 0),
                "extraction_precision": vocab_stats.get('extraction_precision', 0)
            }
        
        university_summary[univ_name] = {
            "source_file": univ_data.get('source_file', ''),
            "total_words": univ_data.get('total_words', 0),
            "unique_words": univ_data.get('unique_words', 0),
            "ocr_confidence": round(univ_data.get('ocr_confidence', 0) * 100, 2),
            "pages_processed": univ_data.get('pages_processed', 0),
            "vocabulary_coverage": vocab_coverage_light
        }
    
    # æœ€é »å‡ºå˜èªï¼ˆä¸Šä½10èªã®ã¿ï¼‰
    word_frequencies = original_data.get('multi_vocabulary_coverage', {}).get('word_frequencies', {})
    top_words = dict(list(word_frequencies.items())[:10])
    
    # Streamlitç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    streamlit_data = {
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "source_file": multi_vocab_report_path,
            "data_version": "1.0"
        },
        "overall_summary": overall_summary,
        "vocabulary_summary": vocabulary_summary,
        "university_analysis": university_summary,
        "top_frequent_words": top_words
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    if output_path is None:
        output_path = "streamlit-vocab-analyzer/data/analysis_data.json"
    
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(streamlit_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_file}")
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼:")
    print(f"  - å¤§å­¦æ•°: {len(university_summary)}")
    print(f"  - å˜èªå¸³æ•°: {len(vocabulary_summary)}")
    print(f"  - ç·å˜èªæ•°: {overall_summary['total_words_extracted']:,}")
    print(f"  - å¹³å‡OCRä¿¡é ¼åº¦: {overall_summary['average_ocr_confidence']:.1f}%")
    
    return streamlit_data

def create_university_metadata():
    """
    å¤§å­¦ãƒ»å­¦éƒ¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    """
    university_metadata = {
        "universities": {
            "æ—©ç¨²ç”°å¤§å­¦_æ³•å­¦éƒ¨": {
                "full_name": "æ—©ç¨²ç”°å¤§å­¦ æ³•å­¦éƒ¨",
                "short_name": "æ—©ç¨²ç”°æ³•",
                "category": "ç§ç«‹",
                "region": "é–¢æ±",
                "color": "#8B0000"
            },
            "æ—©ç¨²ç”°å¤§å­¦_æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨": {
                "full_name": "æ—©ç¨²ç”°å¤§å­¦ æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨",
                "short_name": "æ—©ç¨²ç”°æ”¿çµŒ",
                "category": "ç§ç«‹",
                "region": "é–¢æ±", 
                "color": "#DC143C"
            },
            "æ±äº¬å¤§å­¦": {
                "full_name": "æ±äº¬å¤§å­¦",
                "short_name": "æ±å¤§",
                "category": "å›½ç«‹",
                "region": "é–¢æ±",
                "color": "#191970"
            }
        },
        "vocabulary_books": {
            "Target 1900": {"color": "#FF6B6B", "description": "å®šç•ªã®å¤§å­¦å—é¨“å˜èªå¸³"},
            "Target 1400": {"color": "#4ECDC4", "description": "åŸºç¤ãƒ¬ãƒ™ãƒ«é‡è¦–ã®å˜èªå¸³"},
            "ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª": {"color": "#45B7D1", "description": "ã‚·ã‚¹ãƒ†ãƒãƒ†ã‚£ãƒƒã‚¯ãªå­¦ç¿’"},
            "LEAP": {"color": "#96CEB4", "description": "4æŠ€èƒ½å¯¾å¿œå˜èªå¸³"},
            "é‰„å£": {"color": "#FFEAA7", "description": "é›£é–¢å¤§å­¦å¯¾ç­–å˜èªå¸³"}
        }
    }
    
    metadata_path = "streamlit-vocab-analyzer/data/university_metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(university_metadata, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å¤§å­¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¾ã—ãŸ: {metadata_path}")
    return university_metadata

if __name__ == "__main__":
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›
    input_file = "multi_vocabulary_analysis_report.json"
    
    print("ğŸ”„ Streamlitç”¨ãƒ‡ãƒ¼ã‚¿å¤‰æ›é–‹å§‹...")
    
    # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿å¤‰æ›
    streamlit_data = extract_streamlit_data(input_file)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    university_metadata = create_university_metadata()
    
    print("âœ… ãƒ‡ãƒ¼ã‚¿å¤‰æ›å®Œäº†ï¼")