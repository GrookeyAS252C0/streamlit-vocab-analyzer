#!/usr/bin/env python3
"""
è¤‡æ•°å˜èªå¸³ã¨ã®ä¸€è‡´ç‡åˆ†æãƒ„ãƒ¼ãƒ«
extraction_results_pure_english.jsonã‹ã‚‰æŠ½å‡ºã—ãŸå˜èªã¨è¤‡æ•°ã®å˜èªå¸³ã®æ¯”è¼ƒåˆ†æ
å¯¾å¿œå˜èªå¸³: Target 1900, Target 1400, ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª, LEAP, é‰„å£
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict, Counter
import logging

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import pandas as pd

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MultiVocabularyAnalyzer:
    def __init__(self):
        """
        è¤‡æ•°å˜èªå¸³ã¨ã®ä¸€è‡´ç‡åˆ†æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
        """
        # NLTK ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        self._download_nltk_data()
        
        # LemmatizeråˆæœŸåŒ–
        self.lemmatizer = WordNetLemmatizer()
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
        self.stop_words = set(stopwords.words('english'))
        
        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹å˜èªå¸³ã®å®šç¾©
        self.vocabulary_books = {
            'Target 1900': {
                'file': 'target1900.csv',
                'word_column': 'word'
            },
            'Target 1400': {
                'file': 'target1400.csv', 
                'word_column': 'å˜èª'
            },
            'ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª': {
                'file': 'ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èª.csv',
                'word_column': 'è‹±èª'
            },
            'LEAP': {
                'file': 'LEAP.csv',
                'word_column': 'è‹±èª'
            },
            'é‰„å£': {
                'file': 'é‰„å£.csv',
                'word_column': 'è‹±èª'
            }
        }
        
    def _download_nltk_data(self):
        """å¿…è¦ãªNLTKãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        required_data = ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger']
        
        for data_name in required_data:
            try:
                nltk.data.find(f'tokenizers/{data_name}' if data_name == 'punkt' 
                              else f'corpora/{data_name}' if data_name in ['stopwords', 'wordnet'] 
                              else f'taggers/{data_name}')
            except LookupError:
                logger.info(f"Downloading NLTK data: {data_name}")
                nltk.download(data_name)
    
    def load_vocabulary_lists(self) -> Dict[str, Set[str]]:
        """
        ã™ã¹ã¦ã®å˜èªå¸³ã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            {å˜èªå¸³å: æ­£è¦åŒ–æ¸ˆã¿å˜èªã‚»ãƒƒãƒˆ}ã®è¾æ›¸
        """
        vocabulary_sets = {}
        
        for book_name, config in self.vocabulary_books.items():
            try:
                file_path = config['file']
                word_column = config['word_column']
                
                logger.info(f"{book_name} ã‚’èª­ã¿è¾¼ã¿ä¸­: {file_path}")
                
                # CSVèª­ã¿è¾¼ã¿ï¼ˆBOMå¯¾å¿œï¼‰
                df = pd.read_csv(file_path, encoding='utf-8-sig')
                
                if word_column not in df.columns:
                    logger.error(f"{book_name}: '{word_column}'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                words = set()
                for word in df[word_column].dropna():
                    word = str(word).strip().lower()
                    if word:
                        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                        cleaned_word = re.sub(r'[^\w]', '', word.lower())
                        if len(cleaned_word) >= 2 and not cleaned_word.isdigit():
                            # lemmatizationï¼ˆåŸå½¢åŒ–ï¼‰
                            lemmatized = self.lemmatizer.lemmatize(cleaned_word, pos='v')  # å‹•è©ã¨ã—ã¦
                            lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')    # åè©ã¨ã—ã¦
                            words.add(lemmatized)
                
                vocabulary_sets[book_name] = words
                logger.info(f"{book_name}: {len(words)}èªèª­ã¿è¾¼ã¿å®Œäº†")
                
            except FileNotFoundError:
                logger.error(f"{book_name}ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config['file']}")
                continue
            except Exception as e:
                logger.error(f"{book_name}èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        return vocabulary_sets
    
    def load_extracted_data(self, file_path: str) -> Tuple[List[str], Dict]:
        """
        extraction_results_pure_english.jsonã‹ã‚‰æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            file_path: æŠ½å‡ºçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            (æ­£è¦åŒ–æ¸ˆã¿æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆ, å…ƒãƒ‡ãƒ¼ã‚¿)
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å…¨ã¦ã®æŠ½å‡ºå˜èªã‚’åé›†
            all_extracted_words = []
            for item in data.get('extracted_data', []):
                words = item.get('extracted_words', [])
                all_extracted_words.extend(words)
            
            # æ­£è¦åŒ–ï¼ˆlemmatizationä»˜ãï¼‰
            normalized_words = []
            for word in all_extracted_words:
                # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                cleaned_word = re.sub(r'[^\w]', '', word.lower())
                if len(cleaned_word) >= 2 and not cleaned_word.isdigit():
                    # lemmatizationï¼ˆåŸå½¢åŒ–ï¼‰
                    lemmatized = self.lemmatizer.lemmatize(cleaned_word, pos='v')  # å‹•è©ã¨ã—ã¦
                    lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')    # åè©ã¨ã—ã¦
                    normalized_words.append(lemmatized)
            
            logger.info(f"æŠ½å‡ºã•ã‚ŒãŸç·å˜èªæ•°: {len(all_extracted_words)}")
            logger.info(f"æ­£è¦åŒ–å¾Œãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {len(set(normalized_words))}")
            
            return normalized_words, data
            
        except FileNotFoundError:
            logger.error(f"æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            raise
        except Exception as e:
            logger.error(f"æŠ½å‡ºçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _normalize_word(self, word: str) -> str:
        """
        å˜èªã®æ­£è¦åŒ–å‡¦ç†
        
        Args:
            word: æ­£è¦åŒ–ã™ã‚‹å˜èª
            
        Returns:
            æ­£è¦åŒ–æ¸ˆã¿å˜èªï¼ˆç©ºæ–‡å­—åˆ—ã®å ´åˆã¯é™¤å¤–å¯¾è±¡ï¼‰
        """
        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        word = re.sub(r'[^\w]', '', word.lower())
        
        # çŸ­ã™ãã‚‹å˜èªã‚„æ•°å­—ã®ã¿ã®å˜èªã‚’é™¤å¤–
        if len(word) < 2 or word.isdigit():
            return ""
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
        if word in self.stop_words:
            return ""
        
        # ãƒ¬ãƒ³ãƒåŒ–ï¼ˆåŸå½¢åŒ–ï¼‰
        try:
            lemmatized = self.lemmatizer.lemmatize(word, pos='v')  # å‹•è©ã¨ã—ã¦
            lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')  # åè©ã¨ã—ã¦
            return lemmatized
        except:
            return word
    
    def calculate_multi_vocabulary_coverage(self, vocabulary_sets: Dict[str, Set[str]], extracted_words: List[str]) -> Dict:
        """
        è¤‡æ•°å˜èªå¸³ã§ã®ä¸€è‡´ç‡çµ±è¨ˆã®è¨ˆç®—
        
        Args:
            vocabulary_sets: {å˜èªå¸³å: å˜èªã‚»ãƒƒãƒˆ}ã®è¾æ›¸
            extracted_words: æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆ
            
        Returns:
            è¤‡æ•°å˜èªå¸³ã®çµ±è¨ˆæƒ…å ±è¾æ›¸
        """
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯æŠ½å‡ºå˜èª
        unique_extracted = set(extracted_words)
        word_frequencies = Counter(extracted_words)
        
        multi_stats = {
            'extracted_total_words': len(extracted_words),
            'extracted_unique_words': len(unique_extracted),
            'vocabulary_coverage': {},
            'word_frequencies': dict(word_frequencies.most_common(50))
        }
        
        # å„å˜èªå¸³ã¨ã®æ¯”è¼ƒåˆ†æ
        for book_name, target_words in vocabulary_sets.items():
            # ä¸€è‡´å˜èª
            matched_words = target_words.intersection(unique_extracted)
            
            # çµ±è¨ˆè¨ˆç®—
            target_coverage_rate = len(matched_words) / len(target_words) * 100 if target_words else 0
            extraction_precision = len(matched_words) / len(unique_extracted) * 100 if unique_extracted else 0
            
            # é »åº¦åˆ†æ
            matched_frequencies = {word: word_frequencies[word] for word in matched_words}
            
            book_stats = {
                'target_total_words': len(target_words),
                'matched_words_count': len(matched_words),
                'target_coverage_rate': round(target_coverage_rate, 2),
                'extraction_precision': round(extraction_precision, 2),
                'matched_words': sorted(list(matched_words)),
                'unmatched_from_target': sorted(list(target_words - matched_words)),
                'unmatched_from_extracted': sorted(list(unique_extracted - target_words)),
                'matched_word_frequencies': dict(sorted(matched_frequencies.items(), 
                                                       key=lambda x: x[1], reverse=True)[:30])
            }
            
            multi_stats['vocabulary_coverage'][book_name] = book_stats
            logger.info(f"{book_name}: ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ {target_coverage_rate:.2f}%, æŠ½å‡ºç²¾åº¦ {extraction_precision:.2f}%")
        
        return multi_stats
    
    def analyze_by_frequency_tiers(self, target_words: Set[str], extracted_words: List[str]) -> Dict:
        """
        é »åº¦å±¤åˆ¥ã®åˆ†æ
        
        Args:
            target_words: Target 1900å˜èªã‚»ãƒƒãƒˆ
            extracted_words: æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆ
            
        Returns:
            é »åº¦å±¤åˆ¥çµ±è¨ˆ
        """
        word_frequencies = Counter(extracted_words)
        unique_extracted = set(extracted_words)
        matched_words = target_words.intersection(unique_extracted)
        
        # é »åº¦ã«ã‚ˆã‚‹å±¤åˆ†ã‘
        frequency_tiers = {
            'high_frequency': [],    # 10å›ä»¥ä¸Š
            'medium_frequency': [],  # 3-9å›
            'low_frequency': []      # 1-2å›
        }
        
        for word in unique_extracted:
            freq = word_frequencies[word]
            if freq >= 10:
                frequency_tiers['high_frequency'].append(word)
            elif freq >= 3:
                frequency_tiers['medium_frequency'].append(word)
            else:
                frequency_tiers['low_frequency'].append(word)
        
        # å„å±¤ã§ã®ä¸€è‡´ç‡è¨ˆç®—
        tier_analysis = {}
        for tier_name, tier_words in frequency_tiers.items():
            tier_set = set(tier_words)
            tier_matched = target_words.intersection(tier_set)
            
            tier_analysis[tier_name] = {
                'total_words': len(tier_words),
                'matched_words': len(tier_matched),
                'coverage_rate': round(len(tier_matched) / len(tier_words) * 100, 2) if tier_words else 0,
                'matched_word_list': sorted(list(tier_matched))
            }
        
        return tier_analysis
    
    def analyze_by_university_multi(self, vocabulary_sets: Dict[str, Set[str]], original_data: Dict) -> Dict:
        """
        å¤§å­¦åˆ¥ã®è¤‡æ•°å˜èªå¸³èªå½™åˆ†æ
        
        Args:
            vocabulary_sets: {å˜èªå¸³å: å˜èªã‚»ãƒƒãƒˆ}ã®è¾æ›¸
            original_data: æŠ½å‡ºçµæœã®å…ƒãƒ‡ãƒ¼ã‚¿
            
        Returns:
            å¤§å­¦åˆ¥è¤‡æ•°å˜èªå¸³åˆ†æçµæœ
        """
        university_analysis = {}
        
        for item in original_data.get('extracted_data', []):
            source_file = item.get('source_file', '')
            words = item.get('extracted_words', [])
            
            # å¤§å­¦åã‚’æŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ï¼‰
            university_name = self._extract_university_name(source_file)
            
            # å˜èªã‚’æ­£è¦åŒ–ï¼ˆlemmatizationä»˜ãï¼‰
            normalized_words = []
            for word in words:
                cleaned_word = re.sub(r'[^\w]', '', word.lower())
                if len(cleaned_word) >= 2 and not cleaned_word.isdigit():
                    # lemmatizationï¼ˆåŸå½¢åŒ–ï¼‰
                    lemmatized = self.lemmatizer.lemmatize(cleaned_word, pos='v')  # å‹•è©ã¨ã—ã¦
                    lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')    # åè©ã¨ã—ã¦
                    normalized_words.append(lemmatized)
            
            unique_words = set(normalized_words)
            word_frequencies = Counter(normalized_words)
            
            # å„å˜èªå¸³ã¨ã®ä¸€è‡´åˆ†æ
            vocabulary_coverage = {}
            for book_name, target_words in vocabulary_sets.items():
                matched_words = target_words.intersection(unique_words)
                
                # çµ±è¨ˆè¨ˆç®—
                word_count = len(normalized_words)
                unique_count = len(unique_words)
                matched_count = len(matched_words)
                coverage_rate = (matched_count / len(target_words) * 100) if target_words else 0
                precision = (matched_count / unique_count * 100) if unique_count else 0
                
                # é »åº¦åˆ†æ
                matched_frequencies = {word: word_frequencies[word] for word in matched_words}
                
                vocabulary_coverage[book_name] = {
                    'target_total_words': len(target_words),
                    'matched_words_count': matched_count,
                    'target_coverage_rate': round(coverage_rate, 2),
                    'extraction_precision': round(precision, 2),
                    'matched_words': sorted(list(matched_words)),
                    'matched_word_frequencies': dict(sorted(matched_frequencies.items(), 
                                                           key=lambda x: x[1], reverse=True)[:10])
                }
            
            university_analysis[university_name] = {
                'source_file': source_file,
                'total_words': len(normalized_words),
                'unique_words': len(unique_words),
                'vocabulary_coverage': vocabulary_coverage,
                'word_frequencies': dict(word_frequencies.most_common(20)),
                'ocr_confidence': item.get('ocr_confidence', 0),
                'pages_processed': item.get('pages_processed', 0)
            }
        
        return university_analysis
    
    def _extract_university_name(self, filename: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤§å­¦åãƒ»å­¦éƒ¨åã‚’æŠ½å‡º
        
        Args:
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            
        Returns:
            å¤§å­¦åãƒ»å­¦éƒ¨å
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤§å­¦åãƒ»å­¦éƒ¨åã‚’æŠ½å‡ºï¼ˆå­¦éƒ¨åˆ¥ã«åˆ†é›¢ï¼‰
        if 'æ—©ç¨²ç”°å¤§å­¦' in filename:
            if 'æ³•å­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_æ³•å­¦éƒ¨'
            elif 'æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨'
            elif 'å•†å­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_å•†å­¦éƒ¨'
            elif 'æ–‡å­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_æ–‡å­¦éƒ¨'
            elif 'ç†å·¥å­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_ç†å·¥å­¦éƒ¨'
            else:
                return 'æ—©ç¨²ç”°å¤§å­¦'
        elif 'æ±äº¬å¤§å­¦' in filename:
            return 'æ±äº¬å¤§å­¦'
        elif 'æ…¶æ‡‰ç¾©å¡¾å¤§å­¦' in filename or 'æ…¶æ‡‰' in filename:
            return 'æ…¶æ‡‰ç¾©å¡¾å¤§å­¦'
        elif 'äº¬éƒ½å¤§å­¦' in filename:
            return 'äº¬éƒ½å¤§å­¦'
        elif 'ä¸€æ©‹å¤§å­¦' in filename:
            return 'ä¸€æ©‹å¤§å­¦'
        elif 'å¤§é˜ªå¤§å­¦' in filename:
            return 'å¤§é˜ªå¤§å­¦'
        elif 'æ˜æ²»å¤§å­¦' in filename:
            return 'æ˜æ²»å¤§å­¦'
        elif 'ç«‹æ•™å¤§å­¦' in filename:
            return 'ç«‹æ•™å¤§å­¦'
        elif 'ä¸Šæ™ºå¤§å­¦' in filename:
            return 'ä¸Šæ™ºå¤§å­¦'
        elif 'é’å±±å­¦é™¢å¤§å­¦' in filename:
            return 'é’å±±å­¦é™¢å¤§å­¦'
        else:
            # ãã®ä»–ã®å ´åˆã¯æœ€åˆã®å˜èªã‚’ä½¿ç”¨
            base_name = filename.replace('.pdf', '')
            parts = base_name.split('_')
            return parts[0] if parts else filename

    def generate_multi_vocabulary_report(
        self, 
        extraction_file: str, 
        output_file: str = None
    ) -> Dict:
        """
        è¤‡æ•°å˜èªå¸³åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        
        Args:
            extraction_file: æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            
        Returns:
            åˆ†æçµæœè¾æ›¸
        """
        logger.info("è¤‡æ•°å˜èªå¸³åˆ†æé–‹å§‹...")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        vocabulary_sets = self.load_vocabulary_lists()
        extracted_words, original_data = self.load_extracted_data(extraction_file)
        
        # è¤‡æ•°å˜èªå¸³ã§ã®ä¸€è‡´ç‡åˆ†æ
        multi_stats = self.calculate_multi_vocabulary_coverage(vocabulary_sets, extracted_words)
        
        # å¤§å­¦åˆ¥åˆ†æï¼ˆå„å˜èªå¸³ã«å¯¾ã—ã¦ï¼‰
        university_analysis = self.analyze_by_university_multi(vocabulary_sets, original_data)
        
        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = {
            'analysis_metadata': {
                'extraction_file': extraction_file,
                'analysis_timestamp': pd.Timestamp.now().isoformat(),
                'vocabulary_books': list(vocabulary_sets.keys()),
                'extraction_summary': original_data.get('extraction_summary', {})
            },
            'multi_vocabulary_coverage': multi_stats,
            'university_analysis': university_analysis,
            'recommendations': self._generate_multi_recommendations(multi_stats)
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if output_file is None:
            output_file = "multi_vocabulary_analysis_report.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ†æå®Œäº†ã€‚çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        
        # çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        self._print_multi_summary(multi_stats)
        
        # å¤§å­¦åˆ¥ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        self._print_university_multi_summary(university_analysis)
        
        return report
    
    def _generate_multi_recommendations(self, multi_stats: Dict) -> List[str]:
        """
        è¤‡æ•°å˜èªå¸³åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ç”Ÿæˆ
        """
        recommendations = []
        
        # å„å˜èªå¸³ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã‚’ç¢ºèª
        vocabulary_coverage = multi_stats['vocabulary_coverage']
        
        # Target 1900ã®çµæœ
        if 'Target 1900' in vocabulary_coverage:
            target1900_coverage = vocabulary_coverage['Target 1900']['target_coverage_rate']
            if target1900_coverage < 20:
                recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä½ã„ã§ã™ã€‚åŸºæœ¬èªå½™ã®å¼·åŒ–ãŒå¿…è¦ã§ã™ã€‚")
            elif target1900_coverage < 40:
                recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯ä¸­ç¨‹åº¦ã§ã™ã€‚é‡è¦å˜èªã®å¾©ç¿’ã«é‡ç‚¹ã‚’ç½®ã„ã¦ãã ã•ã„ã€‚")
            else:
                recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯è‰¯å¥½ã§ã™ã€‚ä¸Šç´šèªå½™å­¦ç¿’ã«é€²ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        # æœ€ã‚‚ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã®é«˜ã„å˜èªå¸³ã‚’ç‰¹å®š
        best_book = max(vocabulary_coverage.items(), key=lambda x: x[1]['target_coverage_rate'])
        recommendations.append(f"æœ€ã‚‚é©åˆæ€§ãŒé«˜ã„å˜èªå¸³: {best_book[0]} (ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {best_book[1]['target_coverage_rate']:.1f}%)")
        
        # è¤‡æ•°å˜èªå¸³ã§ã®ç·åˆçš„ãªæ¨å¥¨
        high_coverage_books = [name for name, data in vocabulary_coverage.items() 
                              if data['target_coverage_rate'] > 25]
        
        if len(high_coverage_books) >= 3:
            recommendations.append("è¤‡æ•°ã®å˜èªå¸³ã§é«˜ã„ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚èªå½™åŠ›ã¯ååˆ†ã§ã™ã€‚")
        elif len(high_coverage_books) >= 1:
            recommendations.append(f"{', '.join(high_coverage_books)}ã§ã®å­¦ç¿’ã‚’é‡ç‚¹çš„ã«è¡Œã†ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        else:
            recommendations.append("å…¨èˆ¬çš„ã«èªå½™åŠ›å¼·åŒ–ãŒå¿…è¦ã§ã™ã€‚åŸºç¤å˜èªå¸³ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        return recommendations
    
    def _generate_recommendations(self, basic_stats: Dict, frequency_analysis: Dict) -> List[str]:
        """
        åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ç”Ÿæˆ
        """
        recommendations = []
        
        coverage_rate = basic_stats['target_coverage_rate']
        precision = basic_stats['extraction_precision']
        
        if coverage_rate < 30:
            recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä½ã„ã§ã™ã€‚ã‚ˆã‚Šå¤šæ§˜ãªæ•™æã§ã®å­¦ç¿’ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        elif coverage_rate < 50:
            recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯ä¸­ç¨‹åº¦ã§ã™ã€‚é‡è¦å˜èªã®å¾©ç¿’ã«é‡ç‚¹ã‚’ç½®ã„ã¦ãã ã•ã„ã€‚")
        else:
            recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯è‰¯å¥½ã§ã™ã€‚å¿œç”¨ãƒ¬ãƒ™ãƒ«ã®èªå½™å­¦ç¿’ã«é€²ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        if precision < 40:
            recommendations.append("æŠ½å‡ºã•ã‚ŒãŸå˜èªã®å¤šããŒTarget 1900å¤–ã§ã™ã€‚åŸºç¤èªå½™ã®å¼·åŒ–ãŒå¿…è¦ã§ã™ã€‚")
        
        # é »åº¦åˆ†æã«åŸºã¥ãæ¨å¥¨
        high_freq = frequency_analysis.get('high_frequency', {})
        if high_freq.get('coverage_rate', 0) > 80:
            recommendations.append("é«˜é »åº¦å˜èªã®Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯å„ªç§€ã§ã™ã€‚")
        else:
            recommendations.append("é »å‡ºå˜èªã§Target 1900å¤–ã®èªå½™ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®ç¿’å¾—ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚")
        
        return recommendations
    
    def _print_summary(self, stats: Dict):
        """åˆ†æçµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š Target 1900 èªå½™åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        print(f"Target 1900ç·å˜èªæ•°: {stats['target_total_words']:,}")
        print(f"æŠ½å‡ºå˜èªç·æ•°: {stats['extracted_total_words']:,}")
        print(f"æŠ½å‡ºãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {stats['extracted_unique_words']:,}")
        print(f"ä¸€è‡´å˜èªæ•°: {stats['matched_words_count']:,}")
        print()
        print(f"ğŸ¯ Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {stats['target_coverage_rate']:.2f}%")
        print(f"ğŸ” æŠ½å‡ºèªå½™ç²¾åº¦: {stats['extraction_precision']:.2f}%")
        print()
        print("ä¸Šä½ä¸€è‡´å˜èª(é »åº¦é †):")
        for word, freq in list(stats['matched_word_frequencies'].items())[:10]:
            print(f"  â€¢ {word}: {freq}å›")
        print("="*60)
        
    def _print_multi_summary(self, multi_stats: Dict):
        """è¤‡æ•°å˜èªå¸³åˆ†æçµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "="*70)
        print("ğŸ“š è¤‡æ•°å˜èªå¸³èªå½™åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("="*70)
        print(f"æŠ½å‡ºå˜èªç·æ•°: {multi_stats['extracted_total_words']:,}")
        print(f"æŠ½å‡ºãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {multi_stats['extracted_unique_words']:,}")
        print()
        
        # å„å˜èªå¸³ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡è¡¨ç¤º
        print("ğŸ“Š å˜èªå¸³åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡:")
        for book_name, data in multi_stats['vocabulary_coverage'].items():
            print(f"  â€¢ {book_name}: {data['target_coverage_rate']:.2f}% "
                  f"(ä¸€è‡´: {data['matched_words_count']:,}/{data['target_total_words']:,}èª, "
                  f"ç²¾åº¦: {data['extraction_precision']:.1f}%)")
        
        print("\nğŸ”¥ æœ€é »å‡ºå˜èª:")
        for word, freq in list(multi_stats['word_frequencies'].items())[:10]:
            print(f"  â€¢ {word}: {freq}å›")
        print("="*70)
    
    def _print_university_multi_summary(self, university_analysis: Dict):
        """å¤§å­¦ãƒ»å­¦éƒ¨åˆ¥è¤‡æ•°å˜èªå¸³åˆ†æçµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ« å¤§å­¦ãƒ»å­¦éƒ¨åˆ¥ è¤‡æ•°å˜èªå¸³èªå½™åˆ†æçµæœ")
        print("="*80)
        
        for university_name, data in university_analysis.items():
            print(f"\nã€{university_name}ã€‘")
            print(f"  ãƒ•ã‚¡ã‚¤ãƒ«å: {data['source_file']}")
            print(f"  ç·å˜èªæ•°: {data['total_words']:,}")
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {data['unique_words']:,}")
            print(f"  OCRä¿¡é ¼åº¦: {data['ocr_confidence']:.1%}")
            print(f"  å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {data['pages_processed']}")
            print()
            
            # å„å˜èªå¸³ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡è¡¨ç¤º
            print(f"  ğŸ“š å˜èªå¸³åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡:")
            for book_name, book_data in data['vocabulary_coverage'].items():
                print(f"    â€¢ {book_name}: {book_data['target_coverage_rate']:.2f}% "
                      f"(ä¸€è‡´: {book_data['matched_words_count']:,}èª, "
                      f"ç²¾åº¦: {book_data['extraction_precision']:.1f}%)")
        
        print("="*80)
    
    def _print_university_summary(self, university_analysis: Dict):
        """å¤§å­¦ãƒ»å­¦éƒ¨åˆ¥åˆ†æçµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ« å¤§å­¦ãƒ»å­¦éƒ¨åˆ¥ Target 1900 èªå½™åˆ†æçµæœ")
        print("="*60)
        
        for university_name, data in university_analysis.items():
            print(f"\nã€{university_name}ã€‘")
            print(f"  ãƒ•ã‚¡ã‚¤ãƒ«å: {data['source_file']}")
            print(f"  ç·å˜èªæ•°: {data['total_words']:,}")
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {data['unique_words']:,}")
            print(f"  Target 1900ä¸€è‡´æ•°: {data['matched_words_count']:,}")
            print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {data['target_coverage_rate']:.2f}%")
            print(f"  æŠ½å‡ºç²¾åº¦: {data['extraction_precision']:.2f}%")
            print(f"  OCRä¿¡é ¼åº¦: {data['ocr_confidence']:.1%}")
            print(f"  å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {data['pages_processed']}")
            
            # ä¸Šä½é »å‡ºå˜èªï¼ˆä¸€è‡´èªã®ã¿ï¼‰
            matched_words = set(data['matched_words'])
            top_matched = []
            for word, freq in data['word_frequencies'].items():
                if word in matched_words:
                    top_matched.append((word, freq))
                if len(top_matched) >= 5:
                    break
            
            if top_matched:
                print(f"  ä¸Šä½ä¸€è‡´å˜èª: {', '.join([f'{word}({freq})' for word, freq in top_matched])}")
        
        print("="*60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    analyzer = MultiVocabularyAnalyzer()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
    extraction_file = "extraction_results_pure_english.json"
    output_file = "multi_vocabulary_analysis_report.json"
    
    try:
        # è¤‡æ•°å˜èªå¸³åˆ†æå®Ÿè¡Œ
        report = analyzer.generate_multi_vocabulary_report(
            extraction_file=extraction_file,
            output_file=output_file
        )
        
        print(f"\nâœ… è¤‡æ•°å˜èªå¸³åˆ†æå®Œäº†ï¼è©³ç´°ã¯ {output_file} ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        
    except Exception as e:
        logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        raise

if __name__ == "__main__":
    main()