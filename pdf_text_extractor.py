import os
import re
from pathlib import Path
from typing import List, Dict
import logging

import cv2
import numpy as np
from pdf2image import convert_from_path
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
from langdetect import detect, DetectorFactory
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import openai
from dotenv import load_dotenv
from tqdm import tqdm

# è¨€èªæ¤œå‡ºã®çµæœã‚’ä¸€å®šã«ã™ã‚‹
DetectorFactory.seed = 0

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PDFTextExtractor:
    def __init__(self, openai_api_key: str = None):
        """
        PDFã‹ã‚‰è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹
        
        Args:
            openai_api_key: OpenAI APIã‚­ãƒ¼ï¼ˆæ ¡æ­£ç”¨ï¼‰
        """
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
        load_dotenv()
        
        if openai_api_key:
            openai.api_key = openai_api_key
        elif os.getenv('OPENAI_API_KEY'):
            openai.api_key = os.getenv('OPENAI_API_KEY')
        
        # NLTK ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords')
        
        self.english_stopwords = set(stopwords.words('english'))
        
    def preprocess_image(self, image: Image.Image, enhancement_level: str = "standard") -> List[Image.Image]:
        """
        OCRå‰ã®é«˜åº¦ãªç”»åƒå‰å‡¦ç†ï¼ˆè¤‡æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼‰
        
        Args:
            image: PIL Image ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            enhancement_level: å‡¦ç†ãƒ¬ãƒ™ãƒ«ï¼ˆ"light", "standard", "aggressive"ï¼‰
            
        Returns:
            å‡¦ç†æ¸ˆã¿ç”»åƒã®ãƒªã‚¹ãƒˆ
        """
        processed_images = []
        
        # å…ƒç”»åƒã‚’ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«
        if image.mode != 'L':
            base_image = image.convert('L')
        else:
            base_image = image.copy()
        
        # OpenCVå½¢å¼ã«å¤‰æ›
        cv_image = cv2.cvtColor(np.array(base_image), cv2.COLOR_GRAY2BGR)
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        
        # 1. æ¨™æº–å‡¦ç†
        processed_images.append(self._standard_preprocessing(gray))
        
        if enhancement_level in ["standard", "aggressive"]:
            # 2. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ç‰ˆ
            processed_images.append(self._contrast_enhanced_preprocessing(gray))
            
            # 3. ãƒã‚¤ã‚ºé™¤å»å¼·åŒ–ç‰ˆ
            processed_images.append(self._noise_reduction_preprocessing(gray))
            
            # 4. è§£åƒåº¦å‘ä¸Šç‰ˆ
            processed_images.append(self._resolution_enhanced_preprocessing(gray))
        
        if enhancement_level == "aggressive":
            # 5. å½¢æ…‹å­¦çš„å‡¦ç†ç‰ˆ
            processed_images.append(self._morphological_preprocessing(gray))
            
            # 6. ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–äºŒå€¤åŒ–ç‰ˆ
            processed_images.append(self._adaptive_threshold_preprocessing(gray))
        
        return processed_images
    
    def _standard_preprocessing(self, gray_image) -> Image.Image:
        """æ¨™æº–å‰å‡¦ç†"""
        # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼ã§ãƒã‚¤ã‚ºé™¤å»
        blurred = cv2.GaussianBlur(gray_image, (3, 3), 0)
        
        # OTSUäºŒå€¤åŒ–
        _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        return Image.fromarray(binary)
    
    def _contrast_enhanced_preprocessing(self, gray_image) -> Image.Image:
        """ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–å‰å‡¦ç†"""
        # CLAHE (Contrast Limited Adaptive Histogram Equalization)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray_image)
        
        # ã‚¬ãƒ³ãƒè£œæ­£
        gamma = 1.2
        lookup_table = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in np.arange(0, 256)]).astype("uint8")
        gamma_corrected = cv2.LUT(enhanced, lookup_table)
        
        # äºŒå€¤åŒ–
        _, binary = cv2.threshold(gamma_corrected, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        return Image.fromarray(binary)
    
    def _noise_reduction_preprocessing(self, gray_image) -> Image.Image:
        """ãƒã‚¤ã‚ºé™¤å»å¼·åŒ–å‰å‡¦ç†"""
        # ãƒã‚¤ãƒ©ãƒ†ãƒ©ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ã§ã‚¨ãƒƒã‚¸ä¿æŒã—ãªãŒã‚‰ãƒã‚¤ã‚ºé™¤å»
        denoised = cv2.bilateralFilter(gray_image, 9, 75, 75)
        
        # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã§è¿½åŠ ãƒã‚¤ã‚ºé™¤å»
        denoised = cv2.medianBlur(denoised, 3)
        
        # ã‚·ãƒ£ãƒ¼ãƒ—åŒ–ã‚«ãƒ¼ãƒãƒ«
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(denoised, -1, kernel)
        
        # äºŒå€¤åŒ–
        _, binary = cv2.threshold(sharpened, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        return Image.fromarray(binary)
    
    def _resolution_enhanced_preprocessing(self, gray_image) -> Image.Image:
        """è§£åƒåº¦å‘ä¸Šå‰å‡¦ç†"""
        # 2å€ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        height, width = gray_image.shape
        upscaled = cv2.resize(gray_image, (width*2, height*2), interpolation=cv2.INTER_CUBIC)
        
        # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼
        blurred = cv2.GaussianBlur(upscaled, (3, 3), 0)
        
        # äºŒå€¤åŒ–
        _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        return Image.fromarray(binary)
    
    def _morphological_preprocessing(self, gray_image) -> Image.Image:
        """å½¢æ…‹å­¦çš„å‡¦ç†å‰å‡¦ç†"""
        # äºŒå€¤åŒ–
        _, binary = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼æ¼”ç®—ã§ãƒã‚¤ã‚ºé™¤å»
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
        
        # ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
        opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        # ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆæ–‡å­—ã®ç©´åŸ‹ã‚ï¼‰
        closed = cv2.morphologyEx(opened, cv2.MORPH_CLOSE, kernel)
        
        return Image.fromarray(closed)
    
    def _adaptive_threshold_preprocessing(self, gray_image) -> Image.Image:
        """ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–äºŒå€¤åŒ–å‰å‡¦ç†"""
        # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼
        blurred = cv2.GaussianBlur(gray_image, (5, 5), 0)
        
        # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–äºŒå€¤åŒ–
        adaptive = cv2.adaptiveThreshold(
            blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )
        
        return Image.fromarray(adaptive)
    
    def extract_text_from_image(self, image: Image.Image, enhancement_level: str = "standard") -> str:
        """
        è¤‡æ•°OCRã‚¨ãƒ³ã‚¸ãƒ³ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        
        Args:
            image: PIL Image ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            enhancement_level: å‰å‡¦ç†ãƒ¬ãƒ™ãƒ«
            
        Returns:
            æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # è¤‡æ•°ã®å‰å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ç”Ÿæˆ
            processed_images = self.preprocess_image(image, enhancement_level)
            
            all_results = []
            
            # å„å‰å‡¦ç†ç”»åƒã«å¯¾ã—ã¦è¤‡æ•°ã®OCRè¨­å®šã§å®Ÿè¡Œ
            for i, processed_image in enumerate(processed_images):
                # Tesseractè¨­å®šã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
                ocr_configs = [
                    r'--oem 3 --psm 6 -l eng',  # æ¨™æº–
                    r'--oem 3 --psm 8 -l eng',  # å˜èªãƒ¬ãƒ™ãƒ«
                    r'--oem 3 --psm 13 -l eng', # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆè¡Œ
                    r'--oem 1 --psm 6 -l eng',  # LSTM OCR
                ]
                
                for j, config in enumerate(ocr_configs):
                    try:
                        text = pytesseract.image_to_string(processed_image, config=config)
                        if text.strip():
                            all_results.append({
                                'text': text.strip(),
                                'preprocessing': i,
                                'config': j,
                                'confidence': self._estimate_ocr_confidence(text)
                            })
                    except Exception as e:
                        logger.debug(f"OCRè¨­å®š {j} å¤±æ•—: {e}")
                        continue
            
            if not all_results:
                return ""
            
            # æœ€é«˜ä¿¡é ¼åº¦ã®çµæœã‚’é¸æŠ
            best_result = max(all_results, key=lambda x: x['confidence'])
            return best_result['text']
        
        except Exception as e:
            logger.error(f"OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    def _estimate_ocr_confidence(self, text: str) -> float:
        """
        OCRçµæœã®ä¿¡é ¼åº¦ã‚’æ¨å®š
        
        Args:
            text: OCRçµæœãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ (0-1)
        """
        if not text.strip():
            return 0.0
        
        score = 0.0
        
        # è‹±èªæ–‡å­—ã®å‰²åˆ
        english_chars = sum(1 for c in text if c.isalpha() and c.isascii())
        total_chars = len(text.replace(' ', '').replace('\n', ''))
        if total_chars > 0:
            score += (english_chars / total_chars) * 0.4
        
        # è¾æ›¸å˜èªã®å‰²åˆï¼ˆç°¡æ˜“ï¼‰
        words = re.findall(r'\b[a-zA-Z]{2,}\b', text)
        if words:
            # åŸºæœ¬çš„ãªè‹±å˜èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            valid_words = sum(1 for word in words if len(word) >= 2 and word.isalpha())
            score += (valid_words / len(words)) * 0.3
        
        # æ–‡å­—åˆ—ã®é•·ã•ï¼ˆé•·ã„ã»ã©ä¿¡é ¼æ€§ãŒé«˜ã„å‚¾å‘ï¼‰
        length_score = min(len(text) / 100, 1.0) * 0.2
        score += length_score
        
        # ç‰¹æ®Šæ–‡å­—ã‚„æ•°å­—ã®é©åº¦ãªå­˜åœ¨
        special_ratio = len(re.findall(r'[.,!?;:]', text)) / max(len(text), 1)
        if 0.01 <= special_ratio <= 0.1:
            score += 0.1
        
        return min(score, 1.0)
    
    def detect_text_regions(self, image: Image.Image) -> List[tuple]:
        """
        ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã‚’æ¤œå‡º
        
        Args:
            image: PIL Image ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
        Returns:
            ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸã®åº§æ¨™ãƒªã‚¹ãƒˆ [(x, y, w, h), ...]
        """
        try:
            # OpenCVå½¢å¼ã«å¤‰æ›
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
            
            # ãƒ†ã‚­ã‚¹ãƒˆæ¤œå‡ºã®ãŸã‚ã®å‰å‡¦ç†
            blurred = cv2.GaussianBlur(gray, (3, 3), 0)
            _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # è¼ªéƒ­æ¤œå‡º
            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            text_regions = []
            for contour in contours:
                x, y, w, h = cv2.boundingRect(contour)
                
                # ã‚µã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå°ã•ã™ãã‚‹ãƒ»å¤§ãã™ãã‚‹é ˜åŸŸã‚’é™¤å¤–ï¼‰
                if 10 <= w <= image.width * 0.8 and 8 <= h <= image.height * 0.5:
                    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãƒã‚§ãƒƒã‚¯ï¼ˆæ¥µç«¯ã«ç´°ã„ãƒ»å¤ªã„é ˜åŸŸã‚’é™¤å¤–ï¼‰
                    aspect_ratio = w / h
                    if 0.5 <= aspect_ratio <= 20:
                        text_regions.append((x, y, w, h))
            
            # é ˜åŸŸã‚’ä¸Šã‹ã‚‰ä¸‹ã€å·¦ã‹ã‚‰å³ã®é †ã«ã‚½ãƒ¼ãƒˆ
            text_regions.sort(key=lambda region: (region[1], region[0]))
            
            return text_regions
            
        except Exception as e:
            logger.debug(f"ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def is_english_text(self, text: str) -> bool:
        """
        ãƒ†ã‚­ã‚¹ãƒˆãŒè‹±èªã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆå¼·åŒ–ç‰ˆï¼‰
        
        Args:
            text: åˆ¤å®šå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            è‹±èªã®å ´åˆTrue
        """
        if not text.strip():
            return False
        
        # æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æ˜ç¢ºã«é™¤å¤–
        if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text):
            return False
        
        # æ•°å¼ã‚„è¨˜å·ã®ã¿ã®å ´åˆã¯é™¤å¤–
        if re.match(r'^[\d\s\+\-\*\/\=\(\)\[\]]+$', text.strip()):
            return False
        
        try:
            # è‹±èªæ–‡å­—ã®å‰²åˆã‚’ãƒã‚§ãƒƒã‚¯
            english_chars = sum(c.isalpha() and c.isascii() for c in text)
            total_alpha_chars = sum(c.isalpha() for c in text)
            
            if total_alpha_chars == 0:
                return False
            
            english_ratio = english_chars / total_alpha_chars
            
            # è‹±èªæ–‡å­—ã®å‰²åˆãŒä½ã„å ´åˆã¯é™¤å¤–
            if english_ratio < 0.7:
                return False
            
            # è‹±èªã‚‰ã—ã„å˜èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            words = re.findall(r'\b[a-zA-Z]{2,}\b', text.lower())
            if not words:
                return False
            
            # ä¸€èˆ¬çš„ãªè‹±èªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
            english_patterns = [
                r'\b(the|and|or|but|in|on|at|to|for|of|with|by)\b',
                r'\b(is|are|was|were|be|been|being|have|has|had)\b',
                r'\b(can|could|should|would|will|shall|may|might|must)\b',
                r'\b(this|that|these|those|here|there|where|when|what|who|how|why)\b'
            ]
            
            pattern_matches = sum(1 for pattern in english_patterns if re.search(pattern, text.lower()))
            
            # langdetectã«ã‚ˆã‚‹è¨€èªåˆ¤å®š
            try:
                detected_lang = detect(text)
                is_detected_english = detected_lang == 'en'
            except:
                is_detected_english = False
            
            # ç·åˆåˆ¤å®š
            return (english_ratio >= 0.8 and len(words) >= 1) or (english_ratio >= 0.7 and pattern_matches >= 1) or is_detected_english
            
        except Exception as e:
            logger.debug(f"è¨€èªåˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def extract_english_words(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‹±å˜èªã‚’æŠ½å‡º
        
        Args:
            text: å‡¦ç†å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            è‹±å˜èªã®ãƒªã‚¹ãƒˆ
        """
        if not self.is_english_text(text):
            return []
        
        # å˜èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        words = word_tokenize(text.lower())
        
        # è‹±å˜èªã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        english_words = []
        for word in words:
            # ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®ã¿ã€2æ–‡å­—ä»¥ä¸Šã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
            if (re.match(r'^[a-z]+$', word) and 
                len(word) >= 2 and 
                word not in self.english_stopwords):
                english_words.append(word)
        
        return english_words
    
    def correct_ocr_with_llm(self, ocr_text: str) -> str:
        """
        LLMã‚’ä½¿ã£ã¦OCRçµæœã‚’æ ¡æ­£ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        
        Args:
            ocr_text: OCRçµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æ ¡æ­£ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not ocr_text.strip():
            return ocr_text
        
        # OpenAI APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯OCRãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
        if not hasattr(openai, 'api_key') or not openai.api_key:
            return ocr_text
        
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai.api_key)
            
            # ã‚ˆã‚Šå…·ä½“çš„ã§è©³ç´°ãªæŒ‡ç¤ºã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""You are an expert English text corrector specializing in fixing OCR recognition errors in academic texts.

TASK: Fix OCR errors in the following English text from a university entrance exam. 

SPECIFIC INSTRUCTIONS:
1. Correct common OCR errors like:
   - 'cnough' â†’ 'enough'
   - 'bady' â†’ 'body' 
   - 'rn' â†’ 'm'
   - '1' â†’ 'l' or 'I'
   - '0' â†’ 'o' or 'O'
   - Fragmented words
   - Missing spaces between words
   - Extra spaces within words

2. COMPLETELY IGNORE and REMOVE:
   - Japanese text (hiragana, katakana, kanji)
   - Mathematical formulas and equations
   - Numbers used as question numbers or labels
   - Formatting artifacts

3. OUTPUT REQUIREMENTS:
   - Return ONLY the corrected English text
   - Maintain original sentence structure where possible
   - Ensure proper spacing and punctuation
   - If no valid English content exists, return empty string

OCR TEXT:
{ocr_text}

CORRECTED ENGLISH TEXT:"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1500,
                temperature=0.05,  # Lower temperature for more consistent results
                top_p=0.9
            )
            
            corrected_text = response.choices[0].message.content.strip()
            
            # åŸºæœ¬çš„ãªå¾Œå‡¦ç†
            corrected_text = self._post_process_corrected_text(corrected_text)
            
            return corrected_text
            
        except Exception as e:
            logger.warning(f"LLMæ ¡æ­£ã‚¨ãƒ©ãƒ¼: {e}")
            return ocr_text
    
    def _post_process_corrected_text(self, text: str) -> str:
        """
        LLMæ ¡æ­£å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¾Œå‡¦ç†
        
        Args:
            text: æ ¡æ­£ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            å¾Œå‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        # å¤šé‡ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
        text = re.sub(r'\s+', ' ', text)
        
        # è¡Œé ­è¡Œæœ«ã®ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # æ˜ã‚‰ã‹ã«è‹±èªã§ãªã„è¡Œã‚’é™¤å»
        english_lines = []
        for line in lines:
            # åŸºæœ¬çš„ãªè‹±èªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            if re.search(r'[a-zA-Z]', line) and len(line) > 1:
                # æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                if not re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', line):
                    english_lines.append(line)
        
        return '\n'.join(english_lines)
    
    def extract_pure_english_only(self, ocr_text: str) -> str:
        """
        ç´”ç²‹ãªè‹±èªã®ã¿ã‚’æŠ½å‡ºï¼ˆæ—¥æœ¬èªãƒ»è„šæ³¨ãƒ»å•é¡Œç•ªå·ç­‰å®Œå…¨é™¤å»ï¼‰
        
        Args:
            ocr_text: OCRçµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ç´”ç²‹ãªè‹±èªãƒ†ã‚­ã‚¹ãƒˆã®ã¿
        """
        if not ocr_text.strip():
            return ""
        
        # OpenAI APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™
        if not hasattr(openai, 'api_key') or not openai.api_key:
            return ""
        
        try:
            from openai import OpenAI
            client = OpenAI(api_key=openai.api_key)
            
            prompt = f"""You are an expert text processor specialized in extracting PURE ENGLISH TEXT ONLY from Japanese university entrance exam materials.

CRITICAL TASK: Extract ONLY pure English text. COMPLETELY IGNORE ALL JAPANESE content.

WHAT TO EXTRACT (ENGLISH ONLY):
- English reading passages
- English dialogue
- English essay text
- English story content
- English article text

WHAT TO COMPLETELY IGNORE AND REMOVE:
- ALL Japanese text (hiragana: ã‚ã„ã†, katakana: ã‚¢ã‚¤ã‚¦, kanji: æ¼¢å­—)
- ALL Japanese footnotes and annotations
- Question numbers (1), (2), A), B), etc.
- Instructions in Japanese
- Page numbers and headers
- Copyright notices like "Â© Obunsha"
- Author names in Japanese
- Publication information
- OCR artifacts and garbled text
- Mathematical formulas
- Isolated random letters or symbols
- Translation notes or vocabulary lists
- Answer choice indicators

TEXT PROCESSING:
- Remove ALL line breaks within paragraphs
- Join sentences into flowing paragraphs
- Remove extra spaces
- Fix OCR errors: 'cnough'â†’'enough', 'bady'â†’'body', 'rn'â†’'m'
- Keep proper punctuation and capitalization
- Maintain natural paragraph breaks between different passages

OUTPUT REQUIREMENTS:
- Return ONLY continuous English prose
- NO Japanese characters whatsoever
- NO question numbers or markers
- NO footnotes or annotations
- Separate distinct passages with double line breaks
- If no pure English content exists, return empty string

OCR INPUT:
{ocr_text}

PURE ENGLISH OUTPUT:"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=2500,
                temperature=0.02  # Very low temperature for consistency
            )
            
            pure_english = response.choices[0].message.content.strip()
            
            # å³æ ¼ãªå¾Œå‡¦ç†ã§æ—¥æœ¬èªã‚’å®Œå…¨é™¤å»
            pure_english = self._strict_english_filter(pure_english)
            
            return pure_english
            
        except Exception as e:
            logger.error(f"ç´”ç²‹è‹±èªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            logger.error(f"OCRãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®100æ–‡å­—: {ocr_text[:100]}")
            return ""
    
    def _strict_english_filter(self, text: str) -> str:
        """
        å³æ ¼ãªè‹±èªãƒ•ã‚£ãƒ«ã‚¿ã§æ—¥æœ¬èªã‚’å®Œå…¨é™¤å»
        
        Args:
            text: å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ç´”ç²‹è‹±èªãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return ""
        
        # æ—¥æœ¬èªæ–‡å­—ã‚’å«ã‚€è¡Œã‚’å®Œå…¨é™¤å»
        lines = text.split('\n')
        english_only_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ—¥æœ¬èªæ–‡å­—ãƒã‚§ãƒƒã‚¯ï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰
            if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', line):
                continue
                
            # å•é¡Œç•ªå·ã‚„è¨˜å·ã®ã¿ã®è¡Œã‚’é™¤å»
            if re.match(r'^[\d\s\(\)\[\]A-Z\.]+$', line) and len(line) < 10:
                continue
                
            # æ„å‘³ã®ã‚ã‚‹è‹±èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            english_chars = re.findall(r'[a-zA-Z]', line)
            if len(english_chars) < 3:  # è‹±å­—ãŒ3æ–‡å­—æœªæº€ã¯é™¤å»
                continue
                
            # æ˜ã‚‰ã‹ãªOCRã‚¨ãƒ©ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
            if len(line) > 5 and not re.search(r'\s', line):  # ã‚¹ãƒšãƒ¼ã‚¹ãªã—ã®é•·ã„æ–‡å­—åˆ—
                continue
                
            english_only_lines.append(line)
        
        # æ®µè½ã®å†æ§‹æˆ
        result = self._reconstruct_paragraphs(english_only_lines)
        
        return result.strip()
    
    def _reconstruct_paragraphs(self, lines: List[str]) -> str:
        """
        è‹±èªè¡Œã‹ã‚‰è‡ªç„¶ãªæ®µè½ã‚’å†æ§‹æˆ
        """
        if not lines:
            return ""
        
        paragraphs = []
        current_paragraph = ""
        
        for line in lines:
            # æ–°ã—ã„æ®µè½ã®é–‹å§‹ã‚’åˆ¤å®š
            if (current_paragraph and 
                (line[0].isupper() and current_paragraph.endswith('.')) or
                line.startswith('In ') or line.startswith('The ') or line.startswith('When ') or
                line.startswith('Last ') or line.startswith('After ') or line.startswith('My ')):
                
                if current_paragraph:
                    paragraphs.append(current_paragraph)
                current_paragraph = line
            else:
                # ç¾åœ¨ã®æ®µè½ã«è¿½åŠ 
                if current_paragraph:
                    current_paragraph += " " + line
                else:
                    current_paragraph = line
        
        # æœ€å¾Œã®æ®µè½ã‚’è¿½åŠ 
        if current_paragraph:
            paragraphs.append(current_paragraph)
        
        # æ®µè½é–“ã¯äºŒé‡æ”¹è¡Œã§åŒºåˆ‡ã‚‹
        result = "\n\n".join(paragraphs)
        
        # æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        result = re.sub(r' +', ' ', result)  # å¤šé‡ã‚¹ãƒšãƒ¼ã‚¹é™¤å»
        result = re.sub(r'\n{3,}', '\n\n', result)  # 3ã¤ä»¥ä¸Šã®æ”¹è¡Œã‚’2ã¤ã«
        
        return result
    
    def _post_process_clean_text(self, text: str) -> str:
        """
        æŠ½å‡ºã•ã‚ŒãŸã‚¯ãƒªãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å¾Œå‡¦ç†
        
        Args:
            text: ã‚¯ãƒªãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æœ€çµ‚å‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return ""
        
        # è¤‡æ•°ã®æ”¹è¡Œã‚’å˜ä¸€ã®æ”¹è¡Œã«
        text = re.sub(r'\n+', '\n', text)
        
        # è¡Œé ­è¡Œæœ«ã®ç©ºç™½ã‚’é™¤å»
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        # æ®µè½å†…ã®æ”¹è¡Œã‚’é™¤å»ã—ã€æ–‡ç« ã‚’é€£ç¶šã•ã›ã‚‹
        cleaned_lines = []
        current_paragraph = ""
        
        for line in lines:
            # æ–°ã—ã„æ®µè½ã®é–‹å§‹ã‚’åˆ¤å®šï¼ˆå¤§æ–‡å­—ã§å§‹ã¾ã‚‹ã€ã¾ãŸã¯å‰ã®è¡ŒãŒãƒ”ãƒªã‚ªãƒ‰ã§çµ‚ã‚ã‚‹ï¼‰
            if (current_paragraph and 
                (line[0].isupper() and current_paragraph.endswith('.')) or
                len(current_paragraph) == 0):
                if current_paragraph:
                    cleaned_lines.append(current_paragraph)
                current_paragraph = line
            else:
                # ç¾åœ¨ã®æ®µè½ã«è¿½åŠ 
                if current_paragraph:
                    current_paragraph += " " + line
                else:
                    current_paragraph = line
        
        # æœ€å¾Œã®æ®µè½ã‚’è¿½åŠ 
        if current_paragraph:
            cleaned_lines.append(current_paragraph)
        
        # æ®µè½é–“ã¯äºŒé‡æ”¹è¡Œã§åŒºåˆ‡ã‚‹
        result = "\n\n".join(cleaned_lines)
        
        # å¤šé‡ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
        result = re.sub(r' +', ' ', result)
        
        return result.strip()
    
    def process_pdf(self, pdf_path: str, enhancement_level: str = "standard") -> Dict[str, any]:
        """
        å˜ä¸€PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ï¼‰
        
        Args:
            pdf_path: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            enhancement_level: å‡¦ç†ãƒ¬ãƒ™ãƒ« ("light", "standard", "aggressive")
            
        Returns:
            æŠ½å‡ºçµæœã®è¾æ›¸
        """
        logger.info(f"å‡¦ç†é–‹å§‹: {pdf_path} (ãƒ¬ãƒ™ãƒ«: {enhancement_level})")
        
        result = {
            'source_file': Path(pdf_path).name,
            'full_path': pdf_path,
            'pages_processed': 0,
            'pure_english_text': [],
            'extracted_words': [],
            'processing_stats': {
                'total_ocr_attempts': 0,
                'successful_extractions': 0,
                'average_confidence': 0.0,
                'enhancement_level': enhancement_level
            },
            'error': None
        }
        
        try:
            # PDFã‚’ç”»åƒã«å¤‰æ›
            images = convert_from_path(pdf_path, dpi=300)
            result['pages_processed'] = len(images)
            
            all_extracted_words = []
            confidence_scores = []
            
            for i, image in enumerate(tqdm(images, desc=f"Processing {Path(pdf_path).name}")):
                logger.info(f"ãƒšãƒ¼ã‚¸ {i+1}/{len(images)} å‡¦ç†ä¸­...")
                
                # ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸæ¤œå‡ºã‚’è©¦è¡Œ
                text_regions = self.detect_text_regions(image)
                
                if text_regions and enhancement_level == "aggressive":
                    # é ˜åŸŸã”ã¨ã«å‡¦ç†
                    for region in text_regions:
                        x, y, w, h = region
                        region_image = image.crop((x, y, x+w, y+h))
                        
                        # è¤‡æ•°æ‰‹æ³•ã§OCRå®Ÿè¡Œ
                        ocr_text = self.extract_text_from_image(region_image, enhancement_level)
                        result['processing_stats']['total_ocr_attempts'] += 1
                        
                        if ocr_text.strip():
                            # ç´”ç²‹ãªè‹±èªã®ã¿ã‚’æŠ½å‡º
                            pure_english = self.extract_pure_english_only(ocr_text)
                            if pure_english:
                                result['pure_english_text'].append(pure_english)
                                
                                # è‹±å˜èªã‚’æŠ½å‡º
                                words = self._extract_words_from_text(pure_english)
                                all_extracted_words.extend(words)
                            
                            result['processing_stats']['successful_extractions'] += 1
                            
                            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨˜éŒ²
                            confidence = self._estimate_ocr_confidence(ocr_text)
                            confidence_scores.append(confidence)
                else:
                    # é€šå¸¸ã®å…¨ä½“ç”»åƒå‡¦ç†
                    ocr_text = self.extract_text_from_image(image, enhancement_level)
                    result['processing_stats']['total_ocr_attempts'] += 1
                    
                    if ocr_text.strip():
                        # ç´”ç²‹ãªè‹±èªã®ã¿ã‚’æŠ½å‡º
                        pure_english = self.extract_pure_english_only(ocr_text)
                        if pure_english:
                            result['pure_english_text'].append(pure_english)
                            
                            # è‹±å˜èªã‚’æŠ½å‡º
                            words = self._extract_words_from_text(pure_english)
                            all_extracted_words.extend(words)
                        
                        result['processing_stats']['successful_extractions'] += 1
                        
                        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨˜éŒ²
                        confidence = self._estimate_ocr_confidence(ocr_text)
                        confidence_scores.append(confidence)
            
            # é‡è¤‡å˜èªã‚’é™¤å»ï¼ˆé »åº¦ã‚‚è€ƒæ…®ï¼‰
            result['extracted_words'] = self._deduplicate_words_with_frequency(all_extracted_words)
            
            # çµ±è¨ˆæƒ…å ±æ›´æ–°
            if confidence_scores:
                result['processing_stats']['average_confidence'] = sum(confidence_scores) / len(confidence_scores)
        
        except Exception as e:
            logger.error(f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {pdf_path} - {e}")
            result['error'] = str(e)
        
        logger.info(f"å‡¦ç†å®Œäº†: {pdf_path} - {len(result['extracted_words'])}å˜èªæŠ½å‡º (å¹³å‡ä¿¡é ¼åº¦: {result['processing_stats']['average_confidence']:.3f})")
        return result
    
    def _extract_words_from_text(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‹±èªå˜èªã‚’æŠ½å‡ºï¼ˆå†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼‰
        """
        words = []
        for line in text.split('\n'):
            line = line.strip()
            if line and self.is_english_text(line):
                line_words = self.extract_english_words(line)
                words.extend(line_words)
        return words
    
    def _deduplicate_words_with_frequency(self, words: List[str]) -> List[str]:
        """
        é »åº¦ã‚’è€ƒæ…®ã—ãŸé‡è¤‡é™¤å»
        """
        from collections import Counter
        
        # å˜èªã®é »åº¦ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        word_counts = Counter(words)
        
        # é »åº¦ã®é«˜ã„å˜èªã‚’å„ªå…ˆï¼ˆåŒã˜å˜èªã®ç•°ãªã‚‹ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ±åˆï¼‰
        cleaned_words = set()
        for word, count in word_counts.most_common():
            # çŸ­ç¸®å½¢ã‚„é¡ä¼¼å˜èªã®ãƒã‚§ãƒƒã‚¯
            should_add = True
            for existing_word in list(cleaned_words):
                # ãƒ¬ãƒ¼ãƒ™ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è·é›¢ã§ã®é¡ä¼¼ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                if self._are_similar_words(word, existing_word):
                    if count > word_counts.get(existing_word, 0):
                        cleaned_words.discard(existing_word)
                        cleaned_words.add(word)
                    should_add = False
                    break
            
            if should_add:
                cleaned_words.add(word)
        
        return list(cleaned_words)
    
    def _are_similar_words(self, word1: str, word2: str) -> bool:
        """
        å˜èªã®é¡ä¼¼æ€§ã‚’åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
        """
        if abs(len(word1) - len(word2)) > 2:
            return False
        
        # å®Œå…¨ä¸€è‡´
        if word1 == word2:
            return True
        
        # æ–‡å­—ã®80%ä»¥ä¸ŠãŒä¸€è‡´
        common_chars = sum(1 for i, c in enumerate(word1) if i < len(word2) and c == word2[i])
        similarity = common_chars / max(len(word1), len(word2))
        
        return similarity > 0.8
    
    def process_pdf_folder(self, folder_path: str, output_file: str = None, enhancement_level: str = "standard") -> List[Dict]:
        """
        ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        
        Args:
            folder_path: PDFãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
            output_file: çµæœä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            enhancement_level: å‡¦ç†ãƒ¬ãƒ™ãƒ« ("light", "standard", "aggressive")
            
        Returns:
            å…¨PDFã®å‡¦ç†çµæœãƒªã‚¹ãƒˆ
        """
        pdf_folder = Path(folder_path)
        if not pdf_folder.exists():
            raise FileNotFoundError(f"ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {folder_path}")
        
        pdf_files = list(pdf_folder.glob("*.pdf"))
        if not pdf_files:
            logger.warning(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {folder_path}")
            return []
        
        logger.info(f"{len(pdf_files)}å€‹ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†é–‹å§‹ï¼ˆå‡¦ç†ãƒ¬ãƒ™ãƒ«: {enhancement_level}ï¼‰")
        
        results = []
        total_words = 0
        total_confidence = 0.0
        
        for pdf_file in pdf_files:
            result = self.process_pdf(str(pdf_file), enhancement_level)
            results.append(result)
            
            total_words += len(result['extracted_words'])
            if result['processing_stats']['average_confidence'] > 0:
                total_confidence += result['processing_stats']['average_confidence']
        
        # å…¨ä½“çµ±è¨ˆã®è¨ˆç®—
        avg_confidence = total_confidence / len([r for r in results if r['processing_stats']['average_confidence'] > 0])
        
        logger.info(f"å…¨ä½“å‡¦ç†å®Œäº†: {total_words}å˜èªæŠ½å‡ºã€å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}")
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆç´”ç²‹è‹±èªç‰ˆã®ã¿ï¼‰
        if output_file:
            # ç´”ç²‹è‹±èªæŠ½å‡ºç”¨ã®JSONãƒ•ã‚¡ã‚¤ãƒ«å
            pure_english_file = "extraction_results_pure_english.json"
            
            # ç´”ç²‹è‹±èªæŠ½å‡ºç”¨ã®çµ±è¨ˆæƒ…å ±
            pure_english_results = {
                'extraction_summary': {
                    'total_source_files': len(pdf_files),
                    'total_words_extracted': total_words,
                    'average_ocr_confidence': avg_confidence,
                    'processing_level': enhancement_level,
                    'extraction_method': 'pure_english_only',
                    'japanese_content': 'completely_ignored'
                },
                'extracted_data': []
            }
            
            # å„PDFã®ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†
            for result in results:
                if result['pure_english_text'] or result['extracted_words']:
                    pure_data = {
                        'source_file': result['source_file'],
                        'english_passages': result['pure_english_text'],
                        'word_count': len(result['extracted_words']),
                        'extracted_words': result['extracted_words'],
                        'ocr_confidence': result['processing_stats']['average_confidence'],
                        'pages_processed': result['pages_processed']
                    }
                    pure_english_results['extracted_data'].append(pure_data)
            
            self.save_results(pure_english_results, pure_english_file)
            logger.info(f"ç´”ç²‹è‹±èªãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {pure_english_file}")
        
        return results
    
    def save_results(self, results: List[Dict], output_file: str):
        """
        å‡¦ç†çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            results: å‡¦ç†çµæœã®ãƒªã‚¹ãƒˆ
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        import json
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

if __name__ == "__main__":
    # ä½¿ç”¨ä¾‹ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    extractor = PDFTextExtractor()
    
    # PDFãƒ•ã‚©ãƒ«ãƒ€ã‚’å‡¦ç†ï¼ˆå¼·åŒ–ãƒ¬ãƒ™ãƒ«ã‚’é¸æŠå¯èƒ½ï¼‰
    pdf_folder = "./PDF"
    enhancement_level = "aggressive"  # "light", "standard", "aggressive"
    
    print(f"OCRç²¾åº¦å‘ä¸Šæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦PDFå‡¦ç†ã‚’é–‹å§‹...")
    print(f"å‡¦ç†ãƒ¬ãƒ™ãƒ«: {enhancement_level}")
    print(f"ä¸»ãªæ”¹å–„ç‚¹:")
    print(f"- 6ç¨®é¡ã®é«˜åº¦ãªç”»åƒå‰å‡¦ç†")
    print(f"- 4ã¤ã®OCRè¨­å®šã«ã‚ˆã‚‹è¤‡æ•°å®Ÿè¡Œ")
    print(f"- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹æœ€é©çµæœé¸æŠ")
    print(f"- å¼·åŒ–ã•ã‚ŒãŸLLMæ ¡æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    print(f"- ãƒ†ã‚­ã‚¹ãƒˆé ˜åŸŸæ¤œå‡ºã«ã‚ˆã‚‹å±€æ‰€å‡¦ç†")
    print(f"- æ—¥æœ¬èªå®Œå…¨é™¤å¤–è¨­å®š")
    print()
    
    results = extractor.process_pdf_folder(pdf_folder, "pure_english_output", enhancement_level)
    
    # å…¨ä½“ã®çµ±è¨ˆã‚’è¡¨ç¤ºï¼ˆç´”ç²‹è‹±èªæŠ½å‡ºç‰ˆï¼‰
    if results:
        total_words = sum(len(result['extracted_words']) for result in results)
        all_words = set()
        confidence_scores = []
        
        for result in results:
            all_words.update(result['extracted_words'])
            if result['processing_stats']['average_confidence'] > 0:
                confidence_scores.append(result['processing_stats']['average_confidence'])
        
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        
        print(f"\n=== ç´”ç²‹è‹±èªæŠ½å‡ºçµæœ ===")
        print(f"å‡¦ç†PDFãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results)}")
        print(f"æŠ½å‡ºå˜èªç·æ•°: {total_words}")
        print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {len(all_words)}")
        print(f"å¹³å‡OCRä¿¡é ¼åº¦: {avg_confidence:.3f}")
        print(f"å‡¦ç†ãƒ¬ãƒ™ãƒ«: {enhancement_level}")
        print(f"æ—¥æœ¬èªé™¤å»: å®Œå…¨")
        
        # ç´”ç²‹è‹±èªæ–‡ç« ã®çµ±è¨ˆ
        total_passages = sum(len(result.get('pure_english_text', [])) for result in results)
        print(f"æŠ½å‡ºã•ã‚ŒãŸç´”ç²‹è‹±èªæ–‡ç« æ•°: {total_passages}")
        
        # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        print(f"\n=== å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« ===")
        print(f"ğŸ“„ extraction_results_pure_english.json")
        
        # å„PDFã®è©³ç´°çµæœ
        print(f"\n=== å„PDFã®è©³ç´° ===")
        for result in results:
            pdf_name = result['source_file']
            stats = result['processing_stats']
            passage_count = len(result.get('pure_english_text', []))
            print(f"{pdf_name}: {len(result['extracted_words'])}å˜èª, {passage_count}æ–‡ç« , ä¿¡é ¼åº¦: {stats['average_confidence']:.3f}")
        
        # ç´”ç²‹è‹±èªæ–‡ç« ã®ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        print(f"\n=== ç´”ç²‹è‹±èªæ–‡ç« ã‚µãƒ³ãƒ—ãƒ« ===")
        for result in results:
            passages = result.get('pure_english_text', [])
            if passages:
                print(f"\nã€{result['source_file']}ã€‘")
                for i, passage in enumerate(passages[:2]):  # æœ€åˆã®2ã¤ã®æ–‡ç« ã‚’è¡¨ç¤º
                    print(f"æ–‡ç« {i+1}: {passage[:300]}{'...' if len(passage) > 300 else ''}")
                    print()
    else:
        print("å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")