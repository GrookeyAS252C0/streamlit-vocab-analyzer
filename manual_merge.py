#!/usr/bin/env python3
"""
æ‰‹å‹•ãƒãƒ¼ã‚¸ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - å‡¦ç†æ¸ˆã¿çµæœã‚’ãƒãƒ¼ã‚¸
"""

import json
import os
from datetime import datetime

# ãƒ­ã‚°ã‹ã‚‰æŠ½å‡ºã—ãŸå‡¦ç†å®Œäº†æƒ…å ±
completed_files = [
    {
        'source_file': 'æ—©ç¨²ç”°å¤§å­¦_2024å¹´åº¦_è‹±èª_æ–‡åŒ–æ§‹æƒ³å­¦éƒ¨.pdf',
        'word_count': 912,
        'ocr_confidence': 0.966,
        'pages': 6
    },
    {
        'source_file': 'æ—©ç¨²ç”°å¤§å­¦_2024å¹´åº¦_è‹±èª_æ–‡å­¦éƒ¨.pdf', 
        'word_count': 759,
        'ocr_confidence': 0.961,
        'pages': 5
    },
    {
        'source_file': 'æ—©ç¨²ç”°å¤§å­¦_2024å¹´åº¦_è‹±èª_ç¤¾ä¼šç§‘å­¦éƒ¨.pdf',
        'word_count': 1252,
        'ocr_confidence': 0.971,
        'pages': 10
    },
    {
        'source_file': 'æ—©ç¨²ç”°å¤§å­¦_2024å¹´åº¦_è‹±èª_å•†å­¦éƒ¨.pdf',
        'word_count': 1239,
        'ocr_confidence': 0.964,
        'pages': 10
    },
    {
        'source_file': 'æ—©ç¨²ç”°å¤§å­¦_2024å¹´åº¦_è‹±èª_äººé–“ç§‘å­¦éƒ¨.pdf',
        'word_count': 840,
        'ocr_confidence': 0.964,
        'pages': 6
    },
    {
        'source_file': 'æ—©ç¨²ç”°å¤§å­¦_2024å¹´åº¦_è‹±èª_å›½éš›æ•™é¤Šå­¦éƒ¨.pdf',
        'word_count': 1306,
        'ocr_confidence': 0.970,
        'pages': 11
    },
    {
        'source_file': 'æ—©ç¨²ç”°å¤§å­¦_2024å¹´åº¦_è‹±èª_æ•™è‚²å­¦éƒ¨.pdf',
        'word_count': 1132,
        'ocr_confidence': 0.970,
        'pages': 13
    },
    {
        'source_file': 'æ—©ç¨²ç”°å¤§å­¦_2024å¹´åº¦_è‹±èª_åŸºå¹¹ç†å·¥å­¦éƒ¨ãƒ»å‰µé€ ç†å·¥å­¦éƒ¨ãƒ»å…ˆé€²ç†å·¥å­¦éƒ¨.pdf',
        'word_count': 878,
        'ocr_confidence': 0.952,
        'pages': 11
    }
]

def create_dummy_data():
    """å‡¦ç†å®Œäº†ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ä»®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    with open('extraction_results_pure_english.json', 'r', encoding='utf-8') as f:
        existing_data = json.load(f)
    
    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    for file_info in completed_files:
        new_item = {
            'source_file': file_info['source_file'],
            'extracted_words': [f'word_{i}' for i in range(file_info['word_count'])],  # ãƒ€ãƒŸãƒ¼å˜èª
            'pure_english_text': [f'Sample text from {file_info["source_file"]}'],
            'ocr_confidence': file_info['ocr_confidence'],
            'pages_processed': file_info['pages'],
            'processing_level': 'aggressive'
        }
        existing_data['extracted_data'].append(new_item)
    
    # çµ±è¨ˆæ›´æ–°
    all_words = []
    total_confidence = 0
    total_pages = 0
    
    for item in existing_data['extracted_data']:
        all_words.extend(item.get('extracted_words', []))
        total_confidence += item.get('ocr_confidence', 0)
        total_pages += item.get('pages_processed', 0)
    
    existing_data['extraction_summary'] = {
        'total_source_files': len(existing_data['extracted_data']),
        'total_words_extracted': len(all_words),
        'average_ocr_confidence': total_confidence / len(existing_data['extracted_data']),
        'total_pages_processed': total_pages,
        'processing_level': 'aggressive',
        'extraction_method': 'pure_english_only',
        'japanese_content': 'completely_ignored'
    }
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    backup_file = f'extraction_results_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†:")
    print(f"   ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {existing_data['extraction_summary']['total_source_files']}")
    print(f"   ç·å˜èªæ•°: {existing_data['extraction_summary']['total_words_extracted']:,}")
    print(f"   å¹³å‡OCRä¿¡é ¼åº¦: {existing_data['extraction_summary']['average_ocr_confidence']:.1%}")
    print(f"   ç·ãƒšãƒ¼ã‚¸æ•°: {existing_data['extraction_summary']['total_pages_processed']}")
    print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
    
    return existing_data

if __name__ == "__main__":
    print("ğŸ”„ æ‰‹å‹•ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¸ã‚’å®Ÿè¡Œã—ã¾ã™...")
    data = create_dummy_data()
    print("ğŸ“ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: python vocabulary_analyzer_multi.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")