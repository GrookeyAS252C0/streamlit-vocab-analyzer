#!/usr/bin/env python3
"""
ReadingAssist Analyzer - Streamlit Web Application
è‹±æ–‡èª­è§£ã«ãŠã‘ã‚‹å˜èªå¸³æœ‰åŠ¹æ€§ã¨æ–‡ç« æ§‹é€ ã®ç·åˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
"""

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import json
import sys
import os
from pathlib import Path
import logging

# ãƒ‘ã‚¹ã®è¨­å®š
current_dir = Path(__file__).parent
project_root = current_dir.parent
sys.path.append(str(project_root))

from core.text_analyzer import TextAnalyzer
from core.vocab_analyzer import VocabularyAnalyzer
from core.grammar_analyzer import GrammarAnalyzer
from core.sentence_analyzer import SentenceAnalyzer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Streamlitè¨­å®š
st.set_page_config(
    page_title="ReadingAssist Analyzer",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #4CAF50 0%, #45a049 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #4CAF50;
        margin: 0.5rem 0;
    }
    .analysis-section {
        background: #ffffff;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }
    .recommendation-box {
        background: #e8f5e8;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #4CAF50;
        margin: 1rem 0;
    }
    .challenge-box {
        background: #fff3cd;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #ffc107;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

class ReadingAssistApp:
    def __init__(self):
        """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–"""
        self.initialize_session_state()
        self.load_config()
        self.initialize_analyzers()
    
    def initialize_session_state(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–"""
        if 'analysis_result' not in st.session_state:
            st.session_state.analysis_result = None
        if 'input_text' not in st.session_state:
            st.session_state.input_text = ""
        if 'analysis_in_progress' not in st.session_state:
            st.session_state.analysis_in_progress = False
    
    def load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        config_path = project_root / "config" / "settings.json"
        try:
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
            else:
                self.config = {}
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.config = {}
    
    def initialize_analyzers(self):
        """åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–"""
        try:
            self.text_analyzer = TextAnalyzer(self.config.get('analysis', {}))
            logger.info("åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        except Exception as e:
            logger.error(f"åˆ†æã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            st.error("åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        self.render_header()
        self.render_sidebar()
        self.render_main_content()
    
    def render_header(self):
        """ãƒ˜ãƒƒãƒ€ãƒ¼ã®æç”»"""
        st.markdown("""
        <div class="main-header">
            <h1>ğŸ“š ReadingAssist Analyzer</h1>
            <p>è‹±æ–‡èª­è§£ã«ãŠã‘ã‚‹å˜èªå¸³æœ‰åŠ¹æ€§ã¨æ–‡ç« æ§‹é€ ã®ç·åˆåˆ†æ</p>
        </div>
        """, unsafe_allow_html=True)
    
    def render_sidebar(self):
        """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®æç”»"""
        with st.sidebar:
            st.header("ğŸ“‹ åˆ†æè¨­å®š")
            
            # ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›æ–¹æ³•ã®é¸æŠ
            input_method = st.radio(
                "å…¥åŠ›æ–¹æ³•ã‚’é¸æŠ",
                ["ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å…¥åŠ›", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ"]
            )
            
            if input_method == "ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å…¥åŠ›":
                self.handle_text_input()
            elif input_method == "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
                self.handle_file_upload()
            else:
                self.handle_sample_text()
            
            # åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³
            st.subheader("ğŸ”§ åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³")
            
            enable_vocab = st.checkbox("èªå½™åˆ†æ", value=True)
            enable_grammar = st.checkbox("æ–‡æ³•åˆ†æ", value=True)
            enable_sentence = st.checkbox("æ–‡æ§‹é€ åˆ†æ", value=True)
            
            # åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸš€ åˆ†æå®Ÿè¡Œ", type="primary", use_container_width=True):
                self.run_analysis(enable_vocab, enable_grammar, enable_sentence)
            
            # åˆ†æçµæœã®ã‚¯ãƒªã‚¢
            if st.button("ğŸ—‘ï¸ çµæœã‚¯ãƒªã‚¢"):
                st.session_state.analysis_result = None
                st.session_state.input_text = ""
                st.rerun()
    
    def handle_text_input(self):
        """ãƒ†ã‚­ã‚¹ãƒˆç›´æ¥å…¥åŠ›ã®å‡¦ç†"""
        text_input = st.text_area(
            "åˆ†æã—ãŸã„è‹±æ–‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            height=200,
            placeholder="Enter English text here..."
        )
        
        if text_input:
            st.session_state.input_text = text_input
            
            # ãƒ†ã‚­ã‚¹ãƒˆçµ±è¨ˆã®è¡¨ç¤º
            word_count = len(text_input.split())
            char_count = len(text_input)
            
            st.metric("æ–‡å­—æ•°", f"{char_count:,}")
            st.metric("å˜èªæ•°", f"{word_count:,}")
    
    def handle_file_upload(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å‡¦ç†"""
        uploaded_file = st.file_uploader(
            "ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=['txt', 'csv', 'json'],
            help="å¯¾å¿œå½¢å¼: .txt, .csv, .json"
        )
        
        if uploaded_file is not None:
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®èª­ã¿è¾¼ã¿
                if uploaded_file.type == "text/plain":
                    content = str(uploaded_file.read(), "utf-8")
                else:
                    content = uploaded_file.read().decode("utf-8")
                
                st.session_state.input_text = content
                
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                st.success(f"ãƒ•ã‚¡ã‚¤ãƒ« '{uploaded_file.name}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                st.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(content)} æ–‡å­—")
                
            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def handle_sample_text(self):
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†"""
        sample_files = {
            "å­¦è¡“æ–‡ç« ã‚µãƒ³ãƒ—ãƒ«": "data/sample_texts/sample_academic.txt",
            "ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚µãƒ³ãƒ—ãƒ«": "data/sample_texts/sample_news.txt",
            "å°èª¬ã‚µãƒ³ãƒ—ãƒ«": "data/sample_texts/sample_fiction.txt"
        }
        
        selected_sample = st.selectbox(
            "ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸æŠ",
            list(sample_files.keys())
        )
        
        if st.button("ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿"):
            sample_path = project_root / sample_files[selected_sample]
            
            try:
                if sample_path.exists():
                    with open(sample_path, 'r', encoding='utf-8') as f:
                        sample_content = f.read()
                    
                    st.session_state.input_text = sample_content
                    st.success(f"'{selected_sample}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                else:
                    # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
                    default_text = """
                    The advancement of artificial intelligence has revolutionized numerous industries. 
                    Machine learning algorithms can now process vast amounts of data to identify patterns 
                    and make predictions with remarkable accuracy. However, the implementation of these 
                    technologies requires careful consideration of ethical implications and potential 
                    societal impacts.
                    """
                    st.session_state.input_text = default_text.strip()
                    st.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")
                    
            except Exception as e:
                st.error(f"ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def run_analysis(self, enable_vocab, enable_grammar, enable_sentence):
        """åˆ†æã®å®Ÿè¡Œ"""
        if not st.session_state.input_text.strip():
            st.warning("åˆ†æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        
        if not any([enable_vocab, enable_grammar, enable_sentence]):
            st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’é¸æŠã—ã¦ãã ã•ã„")
            return
        
        # åˆ†æå®Ÿè¡Œ
        with st.spinner("åˆ†æã‚’å®Ÿè¡Œä¸­..."):
            try:
                st.session_state.analysis_in_progress = True
                
                # çµ±åˆåˆ†æã®å®Ÿè¡Œ
                result = self.text_analyzer.analyze_text_comprehensive(
                    st.session_state.input_text,
                    str(project_root / "data" / "vocabulary_books")
                )
                
                # å¿…è¦ãªåˆ†æçµæœã®ã¿æŠ½å‡º
                filtered_result = {}
                
                if enable_vocab:
                    filtered_result['vocabulary_analysis'] = result.get('vocabulary_analysis', {})
                
                if enable_grammar:
                    filtered_result['grammar_analysis'] = result.get('grammar_analysis', {})
                
                if enable_sentence:
                    filtered_result['sentence_analysis'] = result.get('sentence_analysis', {})
                
                # çµ±åˆçµæœã¯å¸¸ã«å«ã‚ã‚‹
                filtered_result['comprehensive_assessment'] = result.get('comprehensive_assessment', {})
                filtered_result['integrated_report'] = result.get('integrated_report', {})
                filtered_result['basic_statistics'] = result.get('basic_statistics', {})
                filtered_result['metadata'] = result.get('metadata', {})
                
                st.session_state.analysis_result = filtered_result
                st.success("åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                
            except Exception as e:
                st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                logger.error(f"Analysis error: {e}")
            
            finally:
                st.session_state.analysis_in_progress = False
    
    def render_main_content(self):
        """ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æç”»"""
        if st.session_state.analysis_result is None:
            self.render_welcome_page()
        else:
            self.render_analysis_results()
    
    def render_welcome_page(self):
        """ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒšãƒ¼ã‚¸ã®æç”»"""
        col1, col2, col3 = st.columns([1, 2, 1])
        
        with col2:
            st.markdown("""
            <div class="analysis-section" style="text-align: center;">
                <h2>ğŸ¯ ReadingAssist Analyzer ã¸ã‚ˆã†ã“ã</h2>
                <p>ã“ã®ã‚¢ãƒ—ãƒªã¯è‹±æ–‡èª­è§£ã«ãŠã‘ã‚‹å˜èªå¸³ã®æœ‰åŠ¹æ€§ã¨æ–‡ç« ã®æ§‹é€ çš„ç‰¹å¾´ã‚’ç·åˆçš„ã«åˆ†æã—ã¾ã™ã€‚</p>
                
                <h3>ğŸ“Š åˆ†ææ©Ÿèƒ½</h3>
                <ul style="text-align: left; display: inline-block;">
                    <li><strong>èªå½™åˆ†æ</strong>: Target 1900/1400ã€ã‚·ã‚¹ãƒ†ãƒ è‹±å˜èªç­‰ã®å˜èªå¸³ã‚«ãƒãƒ¬ãƒƒã‚¸</li>
                    <li><strong>æ–‡æ³•åˆ†æ</strong>: é–¢ä¿‚ä»£åè©ã€ä»®å®šæ³•ã€å®Œäº†å½¢ç­‰ã®å‡ºç¾é »åº¦ã¨é›£æ˜“åº¦</li>
                    <li><strong>æ–‡æ§‹é€ åˆ†æ</strong>: æ–‡é•·ã€è¤‡é›‘ã•ã€èª­ã¿ã‚„ã™ã•æŒ‡æ¨™</li>
                    <li><strong>ç·åˆè©•ä¾¡</strong>: èª­è§£é›£æ˜“åº¦ã¨å­¦ç¿’æ¨å¥¨äº‹é …</li>
                </ul>
                
                <h3>ğŸš€ ä½¿ã„æ–¹</h3>
                <p>å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰åˆ†æã—ãŸã„è‹±æ–‡ã‚’å…¥åŠ›ã—ã€ã€Œåˆ†æå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚</p>
            </div>
            """, unsafe_allow_html=True)
    
    def render_analysis_results(self):
        """åˆ†æçµæœã®æç”»"""
        result = st.session_state.analysis_result
        
        # ã‚¿ãƒ–ã®ä½œæˆ
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“Š ç·åˆè©•ä¾¡", "ğŸ“š èªå½™åˆ†æ", "ğŸ“ æ–‡æ³•åˆ†æ", "ğŸ”— æ–‡æ§‹é€ åˆ†æ", "ğŸ“‹ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ"
        ])
        
        with tab1:
            self.render_comprehensive_assessment(result)
        
        with tab2:
            if 'vocabulary_analysis' in result:
                self.render_vocabulary_analysis(result['vocabulary_analysis'])
            else:
                st.info("èªå½™åˆ†æãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        
        with tab3:
            if 'grammar_analysis' in result:
                self.render_grammar_analysis(result['grammar_analysis'])
            else:
                st.info("æ–‡æ³•åˆ†æãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        
        with tab4:
            if 'sentence_analysis' in result:
                self.render_sentence_analysis(result['sentence_analysis'])
            else:
                st.info("æ–‡æ§‹é€ åˆ†æãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        
        with tab5:
            self.render_detailed_report(result)
    
    def render_comprehensive_assessment(self, result):
        """ç·åˆè©•ä¾¡ã®æç”»"""
        assessment = result.get('comprehensive_assessment', {})
        report = result.get('integrated_report', {})
        
        if not assessment:
            st.warning("ç·åˆè©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ç·åˆè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "ç·åˆé›£æ˜“åº¦",
                assessment.get('difficulty_level', 'ä¸æ˜'),
                help="ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®èª­è§£é›£æ˜“åº¦"
            )
        
        with col2:
            st.metric(
                "èª­è§£ãƒ¬ãƒ™ãƒ«",
                assessment.get('reading_level', 'ä¸æ˜'),
                help="æ¨å¥¨ã•ã‚Œã‚‹å­¦ç¿’ãƒ¬ãƒ™ãƒ«"
            )
        
        with col3:
            st.metric(
                "æ¨å®šèª­è§£æ™‚é–“",
                f"{assessment.get('estimated_reading_time_minutes', 0)}åˆ†",
                help="å¹³å‡çš„ãªèª­è§£ã«ã‹ã‹ã‚‹æ™‚é–“"
            )
        
        with col4:
            st.metric(
                "ç·åˆã‚¹ã‚³ã‚¢",
                f"{assessment.get('overall_difficulty_score', 0):.1f}",
                help="é›£æ˜“åº¦ã‚’æ•°å€¤åŒ–ã—ãŸã‚¹ã‚³ã‚¢"
            )
        
        # ã‚¹ã‚³ã‚¢å†…è¨³ã®å¯è¦–åŒ–
        st.subheader("ğŸ“ˆ é›£æ˜“åº¦ã‚¹ã‚³ã‚¢å†…è¨³")
        
        scores = assessment.get('component_scores', {})
        if scores:
            score_df = pd.DataFrame([
                {'åˆ†æé …ç›®': 'èªå½™', 'ã‚¹ã‚³ã‚¢': scores.get('vocabulary_score', 0)},
                {'åˆ†æé …ç›®': 'æ–‡æ³•', 'ã‚¹ã‚³ã‚¢': scores.get('grammar_score', 0)},
                {'åˆ†æé …ç›®': 'æ–‡æ§‹é€ ', 'ã‚¹ã‚³ã‚¢': scores.get('sentence_score', 0)}
            ])
            
            fig = px.bar(
                score_df, 
                x='åˆ†æé …ç›®', 
                y='ã‚¹ã‚³ã‚¢',
                title="å„åˆ†æé …ç›®ã®é›£æ˜“åº¦ã‚¹ã‚³ã‚¢",
                color='ã‚¹ã‚³ã‚¢',
                color_continuous_scale='RdYlGn_r'
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        # ä¸»è¦ãªèª²é¡Œ
        executive_summary = report.get('executive_summary', {})
        key_challenges = executive_summary.get('key_challenges', [])
        
        if key_challenges:
            st.subheader("âš ï¸ ä¸»è¦ãªå­¦ç¿’èª²é¡Œ")
            for i, challenge in enumerate(key_challenges, 1):
                st.markdown(f"""
                <div class="challenge-box">
                    <strong>{i}.</strong> {challenge}
                </div>
                """, unsafe_allow_html=True)
        
        # å­¦ç¿’æ¨å¥¨äº‹é …
        study_recommendations = report.get('study_recommendations', {})
        if study_recommendations:
            st.subheader("ğŸ’¡ å­¦ç¿’æ¨å¥¨äº‹é …")
            
            priority_areas = study_recommendations.get('priority_areas', [])
            if priority_areas:
                st.markdown(f"""
                <div class="recommendation-box">
                    <strong>å„ªå…ˆå­¦ç¿’åˆ†é‡:</strong> {', '.join(priority_areas)}
                </div>
                """, unsafe_allow_html=True)
            
            # æ¨å¥¨äº‹é …ã®è©³ç´°è¡¨ç¤º
            for area, recommendations in study_recommendations.items():
                if area != 'priority_areas' and recommendations:
                    with st.expander(f"ğŸ“‹ {area}"):
                        for rec in recommendations:
                            st.write(f"â€¢ {rec}")
    
    def render_vocabulary_analysis(self, vocab_analysis):
        """èªå½™åˆ†æçµæœã®æç”»"""
        st.subheader("ğŸ“š èªå½™åˆ†æçµæœ")
        
        summary = vocab_analysis.get('summary', {})
        coverage = vocab_analysis.get('vocabulary_coverage', {})
        
        if not coverage:
            st.warning("èªå½™åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        text_stats = vocab_analysis.get('text_statistics', {})
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æŠ½å‡ºç·å˜èªæ•°", f"{text_stats.get('total_words', 0):,}")
        with col2:
            st.metric("ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°", f"{text_stats.get('unique_words', 0):,}")
        with col3:
            best_book = summary.get('best_coverage_book', {})
            if best_book:
                st.metric("æœ€é©å˜èªå¸³", best_book.get('name', 'ä¸æ˜'))
        
        # å˜èªå¸³ã‚«ãƒãƒ¬ãƒƒã‚¸æ¯”è¼ƒ
        st.subheader("ğŸ“Š å˜èªå¸³åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸æ¯”è¼ƒ")
        
        coverage_data = []
        for book_name, data in coverage.items():
            coverage_data.append({
                'å˜èªå¸³': book_name,
                'èªå½™ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': data.get('vocabulary_coverage_rate', 0),
                'å˜èªå¸³ä½¿ç”¨ç‡(%)': data.get('vocabulary_utilization_rate', 0),
                'ä¸€è‡´èªæ•°': data.get('matched_words_count', 0)
            })
        
        if coverage_data:
            df = pd.DataFrame(coverage_data)
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã®æ£’ã‚°ãƒ©ãƒ•
            fig = px.bar(
                df, 
                x='å˜èªå¸³', 
                y='èªå½™ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                title="èªå½™ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ï¼ˆå…¥è©¦èªå½™ã®ä½•%ãŒå˜èªå¸³ã«å«ã¾ã‚Œã‚‹ã‹ï¼‰",
                color='èªå½™ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                color_continuous_scale='Greens'
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
            
            # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            st.subheader("ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿")
            st.dataframe(df, use_container_width=True)
        
        # é »å‡ºå˜èª
        word_frequencies = vocab_analysis.get('text_statistics', {}).get('word_frequencies', {})
        if word_frequencies:
            st.subheader("ğŸ”¥ é »å‡ºå˜èª Top 20")
            
            freq_df = pd.DataFrame([
                {'å˜èª': word, 'å‡ºç¾å›æ•°': freq}
                for word, freq in list(word_frequencies.items())[:20]
            ])
            
            fig = px.bar(
                freq_df, 
                x='å‡ºç¾å›æ•°', 
                y='å˜èª',
                orientation='h',
                title="é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°"
            )
            fig.update_layout(height=600)
            st.plotly_chart(fig, use_container_width=True)
    
    def render_grammar_analysis(self, grammar_analysis):
        """æ–‡æ³•åˆ†æçµæœã®æç”»"""
        st.subheader("ğŸ“ æ–‡æ³•åˆ†æçµæœ")
        
        summary = grammar_analysis.get('summary', {})
        grammar_items = grammar_analysis.get('grammar_items', {})
        difficulty_analysis = grammar_analysis.get('difficulty_analysis', {})
        
        if not grammar_items:
            st.warning("æ–‡æ³•åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # åŸºæœ¬çµ±è¨ˆ
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ–‡æ³•é …ç›®æ•°", summary.get('total_grammar_items', 0))
        with col2:
            st.metric("æ–‡æ³•å¯†åº¦", f"{summary.get('grammar_density', 0):.2f}%")
        with col3:
            st.metric("è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«", summary.get('complexity_level', 'ä¸æ˜'))
        with col4:
            st.metric("å¹³å‡é›£æ˜“åº¦", f"{summary.get('average_difficulty_score', 0):.2f}")
        
        # æ–‡æ³•é …ç›®å‡ºç¾é »åº¦
        st.subheader("ğŸ“Š æ–‡æ³•é …ç›®å‡ºç¾é »åº¦")
        
        grammar_data = []
        for name, data in grammar_items.items():
            if data.get('count', 0) > 0:
                grammar_data.append({
                    'æ–‡æ³•é …ç›®': name,
                    'å‡ºç¾å›æ•°': data.get('count', 0),
                    'é »åº¦(/100èª)': data.get('frequency_per_100_words', 0),
                    'é›£æ˜“åº¦ãƒ¬ãƒ™ãƒ«': data.get('difficulty_level', 'basic'),
                    'é‡è¦åº¦': data.get('importance', 'medium')
                })
        
        if grammar_data:
            df = pd.DataFrame(grammar_data)
            df = df.sort_values('å‡ºç¾å›æ•°', ascending=False)
            
            # å‡ºç¾é »åº¦ã®æ£’ã‚°ãƒ©ãƒ•
            fig = px.bar(
                df.head(10), 
                x='æ–‡æ³•é …ç›®', 
                y='å‡ºç¾å›æ•°',
                title="æ–‡æ³•é …ç›®åˆ¥å‡ºç¾å›æ•° Top 10",
                color='é›£æ˜“åº¦ãƒ¬ãƒ™ãƒ«',
                color_discrete_map={
                    'basic': '#90EE90',
                    'intermediate': '#FFD700', 
                    'advanced': '#FF6B6B'
                }
            )
            fig.update_layout(height=400)
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
            
            # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            st.subheader("ğŸ“‹ æ–‡æ³•é …ç›®è©³ç´°")
            st.dataframe(df, use_container_width=True)
        
        # é›£æ˜“åº¦åˆ†å¸ƒ
        if difficulty_analysis:
            st.subheader("ğŸ“ˆ é›£æ˜“åº¦åˆ¥åˆ†å¸ƒ")
            
            dist_data = []
            for level, data in difficulty_analysis.items():
                if data.get('count', 0) > 0:
                    dist_data.append({
                        'ãƒ¬ãƒ™ãƒ«': level,
                        'å‡ºç¾å›æ•°': data.get('count', 0),
                        'å‰²åˆ(%)': data.get('percentage', 0)
                    })
            
            if dist_data:
                df_dist = pd.DataFrame(dist_data)
                
                fig = px.pie(
                    df_dist, 
                    values='å‡ºç¾å›æ•°', 
                    names='ãƒ¬ãƒ™ãƒ«',
                    title="æ–‡æ³•é …ç›®é›£æ˜“åº¦åˆ†å¸ƒ"
                )
                st.plotly_chart(fig, use_container_width=True)
    
    def render_sentence_analysis(self, sentence_analysis):
        """æ–‡æ§‹é€ åˆ†æçµæœã®æç”»"""
        st.subheader("ğŸ”— æ–‡æ§‹é€ åˆ†æçµæœ")
        
        basic_stats = sentence_analysis.get('basic_statistics', {})
        length_analysis = sentence_analysis.get('sentence_length_analysis', {})
        complexity = sentence_analysis.get('complexity_analysis', {})
        readability = sentence_analysis.get('readability_scores', {})
        
        # åŸºæœ¬çµ±è¨ˆ
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ç·æ–‡æ•°", basic_stats.get('total_sentences', 0))
        with col2:
            st.metric("å¹³å‡æ–‡é•·", f"{basic_stats.get('avg_words_per_sentence', 0):.1f}èª")
        with col3:
            st.metric("è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«", complexity.get('complexity_level', 'ä¸æ˜'))
        with col4:
            st.metric("èª­ã¿ã‚„ã™ã•", readability.get('reading_level', 'ä¸æ˜'))
        
        # æ–‡é•·åˆ†å¸ƒ
        if length_analysis:
            st.subheader("ğŸ“Š æ–‡é•·åˆ†å¸ƒ")
            
            distribution = length_analysis.get('length_distribution', {})
            if distribution:
                dist_data = []
                for category, data in distribution.items():
                    category_name = {
                        'short_sentences': 'çŸ­æ–‡(1-10èª)',
                        'medium_sentences': 'ä¸­æ–‡(11-20èª)',
                        'long_sentences': 'é•·æ–‡(21-30èª)',
                        'very_long_sentences': 'è¶…é•·æ–‡(31èªä»¥ä¸Š)'
                    }.get(category, category)
                    
                    dist_data.append({
                        'ã‚«ãƒ†ã‚´ãƒª': category_name,
                        'æ–‡æ•°': data.get('count', 0),
                        'å‰²åˆ(%)': data.get('percentage', 0)
                    })
                
                df_dist = pd.DataFrame(dist_data)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    fig_bar = px.bar(
                        df_dist, 
                        x='ã‚«ãƒ†ã‚´ãƒª', 
                        y='æ–‡æ•°',
                        title="æ–‡é•·åˆ¥æ–‡æ•°"
                    )
                    st.plotly_chart(fig_bar, use_container_width=True)
                
                with col2:
                    fig_pie = px.pie(
                        df_dist, 
                        values='æ–‡æ•°', 
                        names='ã‚«ãƒ†ã‚´ãƒª',
                        title="æ–‡é•·åˆ†å¸ƒ"
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
        
        # èª­ã¿ã‚„ã™ã•æŒ‡æ¨™
        if readability:
            st.subheader("ğŸ“ˆ èª­ã¿ã‚„ã™ã•æŒ‡æ¨™")
            
            readability_data = [
                {'æŒ‡æ¨™': 'Flesch Reading Ease', 'å€¤': readability.get('flesch_reading_ease', 0)},
                {'æŒ‡æ¨™': 'Flesch-Kincaid Grade', 'å€¤': readability.get('flesch_kincaid_grade', 0)},
                {'æŒ‡æ¨™': 'Automated Readability', 'å€¤': readability.get('automated_readability_index', 0)},
                {'æŒ‡æ¨™': 'Coleman-Liau Index', 'å€¤': readability.get('coleman_liau_index', 0)},
                {'æŒ‡æ¨™': 'Gunning Fog', 'å€¤': readability.get('gunning_fog', 0)}
            ]
            
            df_read = pd.DataFrame(readability_data)
            
            fig = px.bar(
                df_read, 
                x='æŒ‡æ¨™', 
                y='å€¤',
                title="å„ç¨®èª­ã¿ã‚„ã™ã•æŒ‡æ¨™"
            )
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
        
        # è¤‡é›‘ã•åˆ†æ
        if complexity:
            st.subheader("ğŸ” æ–‡æ§‹é€ è¤‡é›‘ã•åˆ†æ")
            
            indicators = complexity.get('complexity_indicators', {})
            if indicators:
                complexity_df = pd.DataFrame([
                    {'é …ç›®': 'è¤‡æ–‡ãƒ»é‡æ–‡', 'æ•°': indicators.get('complex_sentences', 0)},
                    {'é …ç›®': 'å˜æ–‡', 'æ•°': indicators.get('simple_sentences', 0)},
                    {'é …ç›®': 'ç­‰ä½æ¥ç¶š', 'æ•°': indicators.get('coordination_count', 0)},
                    {'é …ç›®': 'å¾“å±æ¥ç¶š', 'æ•°': indicators.get('subordination_count', 0)},
                    {'é …ç›®': 'é–¢ä¿‚ç¯€', 'æ•°': indicators.get('relative_clauses', 0)}
                ])
                
                fig = px.bar(
                    complexity_df, 
                    x='é …ç›®', 
                    y='æ•°',
                    title="æ–‡æ§‹é€ è¦ç´ ã®å‡ºç¾å›æ•°"
                )
                st.plotly_chart(fig, use_container_width=True)
    
    def render_detailed_report(self, result):
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®æç”»"""
        st.subheader("ğŸ“‹ è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        
        report = result.get('integrated_report', {})
        
        if not report:
            st.warning("è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
        executive_summary = report.get('executive_summary', {})
        if executive_summary:
            st.markdown("### ğŸ“Š ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ç·åˆè©•ä¾¡", executive_summary.get('overall_assessment', 'ä¸æ˜'))
                st.metric("èª­è§£ãƒ¬ãƒ™ãƒ«", executive_summary.get('reading_level', 'ä¸æ˜'))
            with col2:
                st.metric("æ¨å®šèª­è§£æ™‚é–“", f"{executive_summary.get('estimated_reading_time', 0)}åˆ†")
        
        # è©³ç´°æ‰€è¦‹
        detailed_findings = report.get('detailed_findings', {})
        if detailed_findings:
            st.markdown("### ğŸ” è©³ç´°æ‰€è¦‹")
            
            for category, findings in detailed_findings.items():
                category_name = {
                    'vocabulary_insights': 'èªå½™åˆ†ææ‰€è¦‹',
                    'grammar_insights': 'æ–‡æ³•åˆ†ææ‰€è¦‹',
                    'structure_insights': 'æ–‡æ§‹é€ åˆ†ææ‰€è¦‹'
                }.get(category, category)
                
                with st.expander(f"ğŸ“‹ {category_name}"):
                    if isinstance(findings, dict):
                        for key, value in findings.items():
                            if isinstance(value, list):
                                st.write(f"**{key}:**")
                                for item in value:
                                    st.write(f"â€¢ {item}")
                            else:
                                st.write(f"**{key}:** {value}")
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³
        action_plan = report.get('action_plan', {})
        if action_plan:
            st.markdown("### ğŸ¯ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³")
            
            schedule = action_plan.get('study_schedule', {})
            if schedule:
                st.markdown("#### ğŸ“… å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("æ¨å¥¨å­¦ç¿’æœŸé–“", schedule.get('recommended_study_period', 'ä¸æ˜'))
                with col2:
                    st.metric("1æ—¥ã®å­¦ç¿’æ™‚é–“", schedule.get('daily_study_time', 'ä¸æ˜'))
                
                weekly_goals = schedule.get('weekly_goals', [])
                if weekly_goals:
                    st.markdown("#### ğŸ“‹ é€±æ¬¡ç›®æ¨™")
                    for goal in weekly_goals:
                        st.write(f"â€¢ {goal}")
            
            immediate_actions = action_plan.get('immediate_actions', [])
            if immediate_actions:
                st.markdown("#### âš¡ å³åº§ã«å–ã‚Šçµ„ã‚€ã¹ãé …ç›®")
                for action in immediate_actions:
                    st.write(f"â€¢ {action}")
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        with st.expander("ğŸ”§ ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"):
            st.json(result)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    app = ReadingAssistApp()
    app.run()

if __name__ == "__main__":
    main()