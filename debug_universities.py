#!/usr/bin/env python3
"""
å¤§å­¦ãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json

def debug_university_data():
    """ç¾åœ¨ã®å¤§å­¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒãƒƒã‚°"""
    
    # Streamlitç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
    with open('streamlit-vocab-analyzer/data/analysis_data.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print("ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿çŠ¶æ³:")
    print(f"  ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {data['overall_summary']['total_source_files']}")
    print(f"  ç·å˜èªæ•°: {data['overall_summary']['total_words_extracted']:,}")
    
    print("\nğŸ« å¤§å­¦ãƒ»å­¦éƒ¨ä¸€è¦§:")
    universities = list(data.get('university_analysis', {}).keys())
    for i, univ in enumerate(universities, 1):
        univ_data = data['university_analysis'][univ]
        print(f"  {i}. {univ}")
        print(f"     å˜èªæ•°: {univ_data.get('total_words', 0):,}")
        print(f"     OCRä¿¡é ¼åº¦: {univ_data.get('ocr_confidence', 0):.1f}%")
        print(f"     ãƒšãƒ¼ã‚¸æ•°: {univ_data.get('pages_processed', 0)}")
    
    print(f"\nğŸ“ åˆè¨ˆå¤§å­¦ãƒ»å­¦éƒ¨æ•°: {len(universities)}")
    
    # å•é¡Œã®ã‚ã‚‹å¤§å­¦åã‚’ãƒã‚§ãƒƒã‚¯
    print("\nğŸ” åå‰å½¢å¼ãƒã‚§ãƒƒã‚¯:")
    for univ in universities:
        if len(univ) > 50:
            print(f"  âš ï¸  é•·ã™ãã‚‹åå‰: {univ}")
        if any(char in univ for char in ['/', '\\', '<', '>']):
            print(f"  âš ï¸  ç‰¹æ®Šæ–‡å­—å«ã‚€: {univ}")

if __name__ == "__main__":
    debug_university_data()