#!/usr/bin/env python3
"""
å¢—åˆ†PDFå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†
"""

import os
import json
from pathlib import Path
import logging

# å…ƒã®PDFTextExtractorã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from pdf_text_extractor import PDFTextExtractor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_processed_files(result_file: str = "extraction_results_pure_english.json") -> set:
    """
    æ—¢ã«å‡¦ç†æ¸ˆã¿ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—
    
    Args:
        result_file: æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«
        
    Returns:
        å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚»ãƒƒãƒˆ
    """
    processed_files = set()
    
    if os.path.exists(result_file):
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            extracted_data = data.get('extracted_data', [])
            for item in extracted_data:
                source_file = item.get('source_file', '')
                if source_file:
                    processed_files.add(source_file)
            
            logger.info(f"æ—¢å­˜ã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(processed_files)}")
            
        except Exception as e:
            logger.error(f"æ—¢å­˜çµæœãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    return processed_files

def merge_results(existing_file: str, new_results: list) -> dict:
    """
    æ—¢å­˜çµæœã¨æ–°è¦çµæœã‚’ãƒãƒ¼ã‚¸
    
    Args:
        existing_file: æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«
        new_results: æ–°è¦å‡¦ç†çµæœ
        
    Returns:
        ãƒãƒ¼ã‚¸ã•ã‚ŒãŸçµæœ
    """
    merged_data = {
        'extraction_summary': {},
        'extracted_data': []
    }
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    if os.path.exists(existing_file):
        try:
            with open(existing_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            
            merged_data['extracted_data'] = existing_data.get('extracted_data', [])
            logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(merged_data['extracted_data'])}ãƒ•ã‚¡ã‚¤ãƒ«")
            
        except Exception as e:
            logger.error(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
    for result in new_results:
        # æ–°è¦ãƒ‡ãƒ¼ã‚¿ç”¨ã®å½¢å¼ã«å¤‰æ›
        new_item = {
            'source_file': result['source_file'],
            'extracted_words': result['extracted_words'],
            'pure_english_text': result.get('pure_english_text', []),
            'ocr_confidence': result['processing_stats']['average_confidence'],
            'pages_processed': result['processing_stats'].get('pages_processed', result['processing_stats'].get('total_pages', 0)),
            'processing_level': result['processing_stats'].get('enhancement_level', 'aggressive')
        }
        merged_data['extracted_data'].append(new_item)
    
    # å…¨ä½“çµ±è¨ˆã®å†è¨ˆç®—
    all_words = []
    total_confidence = 0
    valid_confidence_count = 0
    
    for item in merged_data['extracted_data']:
        all_words.extend(item.get('extracted_words', []))
        confidence = item.get('ocr_confidence', 0)
        if confidence > 0:
            total_confidence += confidence
            valid_confidence_count += 1
    
    avg_confidence = total_confidence / valid_confidence_count if valid_confidence_count > 0 else 0
    
    merged_data['extraction_summary'] = {
        'total_source_files': len(merged_data['extracted_data']),
        'total_words_extracted': len(all_words),
        'average_ocr_confidence': avg_confidence,
        'processing_level': 'aggressive',
        'extraction_method': 'pure_english_only',
        'japanese_content': 'completely_ignored'
    }
    
    return merged_data

def process_new_pdfs_only():
    """
    æ–°ã—ã„PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†ã™ã‚‹å¢—åˆ†å‡¦ç†
    """
    pdf_folder = "./PDF"
    result_file = "extraction_results_pure_english.json"
    
    # æ—¢å­˜ã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    processed_files = get_processed_files(result_file)
    
    # PDFãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    pdf_folder_path = Path(pdf_folder)
    if not pdf_folder_path.exists():
        raise FileNotFoundError(f"PDFãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_folder}")
    
    all_pdf_files = list(pdf_folder_path.glob("*.pdf"))
    
    # æœªå‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã§æ¯”è¼ƒï¼‰
    new_pdf_files = []
    processed_filenames = set()
    for processed_path in processed_files:
        filename = os.path.basename(processed_path)
        processed_filenames.add(filename)
    
    for pdf_file in all_pdf_files:
        filename = pdf_file.name
        if filename not in processed_filenames:
            new_pdf_files.append(pdf_file)
    
    logger.info(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ç·æ•°: {len(all_pdf_files)}")
    logger.info(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(processed_files)}")
    logger.info(f"æ–°è¦å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(new_pdf_files)}")
    
    if not new_pdf_files:
        logger.info("ğŸ‰ æ–°è¦å‡¦ç†ãŒå¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ï¼")
        return
    
    # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    logger.info(f"ğŸ“„ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†é–‹å§‹:")
    for pdf_file in new_pdf_files:
        logger.info(f"  - {pdf_file.name}")
    
    extractor = PDFTextExtractor()
    new_results = []
    
    for pdf_file in new_pdf_files:
        logger.info(f"ğŸ”„ å‡¦ç†é–‹å§‹: {pdf_file.name}")
        result = extractor.process_pdf(str(pdf_file), "aggressive")
        new_results.append(result)
        logger.info(f"âœ… å‡¦ç†å®Œäº†: {pdf_file.name} - {len(result['extracted_words'])}å˜èªæŠ½å‡º")
    
    # çµæœã®ãƒãƒ¼ã‚¸ã¨ä¿å­˜
    if new_results:
        merged_data = merge_results(result_file, new_results)
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š æœ€çµ‚çµæœ:")
        logger.info(f"  ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {merged_data['extraction_summary']['total_source_files']}")
        logger.info(f"  ç·å˜èªæ•°: {merged_data['extraction_summary']['total_words_extracted']:,}")
        logger.info(f"  å¹³å‡OCRä¿¡é ¼åº¦: {merged_data['extraction_summary']['average_ocr_confidence']:.1%}")
        logger.info(f"ğŸ’¾ çµæœã‚’ä¿å­˜: {result_file}")

if __name__ == "__main__":
    try:
        process_new_pdfs_only()
    except Exception as e:
        logger.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        raise