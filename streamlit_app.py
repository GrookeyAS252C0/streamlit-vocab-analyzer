#!/usr/bin/env python3
"""
å¤§å­¦å…¥è©¦è‹±å˜èªåˆ†æ Streamlit ã‚¢ãƒ—ãƒª
JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»èªå½™åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json
import nltk

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å¤§å­¦å…¥è©¦è‹±å˜èªåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #2E86AB;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #2E86AB;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
    st.markdown('<div class="main-header">ğŸ“š å¤§å­¦å…¥è©¦è‹±å˜èªåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</div>', unsafe_allow_html=True)
    st.markdown("**JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦èªå½™åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„**")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒªã‚¢
    uploaded_files = st.file_uploader(
        "è‹±èªåˆ†æç”¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯èƒ½ï¼‰",
        type=["json"],
        accept_multiple_files=True,
        help="OCRå‡¦ç†æ¸ˆã¿ã®è‹±èªæŠ½å‡ºçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚å¤§å­¦å_å¹´åº¦_è‹±èª_å­¦éƒ¨å.jsonå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚"
    )
    
    if uploaded_files:
        try:
            # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’çµ±åˆ
            combined_data = merge_multiple_json_files(uploaded_files)
            
            if combined_data:
                st.success(f"âœ… {len(uploaded_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º
                with st.expander("ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«", expanded=False):
                    for i, file in enumerate(uploaded_files, 1):
                        st.write(f"{i}. {file.name}")
                
                # åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³
                if st.button("ğŸ“Š èªå½™åˆ†æã‚’å®Ÿè¡Œ", type="primary"):
                    with st.spinner("èªå½™åˆ†æã‚’å®Ÿè¡Œä¸­..."):
                        analysis_data = perform_vocabulary_analysis(combined_data)
                        if analysis_data:
                            st.session_state.analysis_data = analysis_data
                            st.session_state.analysis_completed = True
                            st.success("âœ… èªå½™åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                        else:
                            st.error("âŒ èªå½™åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                # åˆ†æçµæœã®è¡¨ç¤º
                if st.session_state.get('analysis_completed', False) and 'analysis_data' in st.session_state:
                    st.markdown("---")
                    show_analysis_dashboard(st.session_state.analysis_data)
            else:
                st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except json.JSONDecodeError:
            st.error("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
        except Exception as e:
            st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    else:
        st.info("""
        ğŸ‘† **è‹±èªåˆ†æç”¨JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„**
        
        ğŸ“‹ å¯¾å¿œã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:
        - **æ–°å½¢å¼**: `å¤§å­¦å_å¹´åº¦_è‹±èª_å­¦éƒ¨å.json` (file_info + contentæ§‹é€ )
        - **æ—§å½¢å¼**: `extraction_results_pure_english.json` (extraction_summary + extracted_dataæ§‹é€ )
        
        ğŸ’¡ è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã¾ã¨ã‚ã¦é¸æŠã—ã¦ä¸€æ‹¬åˆ†æãŒå¯èƒ½ã§ã™
        """)

def merge_multiple_json_files(uploaded_files):
    """è¤‡æ•°ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆï¼ˆå®Ÿéš›ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œï¼‰"""
    try:
        combined_data = {
            'extraction_summary': {
                'total_source_files': 0,
                'total_words_extracted': 0
            },
            'extracted_data': []
        }
        
        for uploaded_file in uploaded_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
            uploaded_file.seek(0)
            file_content = json.load(uploaded_file)
            
            # æ–°ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ
            if 'file_info' in file_content and 'content' in file_content:
                # æ–°ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {"file_info": {...}, "content": {"extracted_words": [...]}}
                file_info = file_content.get('file_info', {})
                content = file_content.get('content', {})
                extraction_results = file_content.get('extraction_results', {})
                
                # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’çµ±åˆ
                combined_data['extraction_summary']['total_source_files'] += 1
                combined_data['extraction_summary']['total_words_extracted'] += extraction_results.get('total_words', 0)
                
                # æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
                extracted_entry = {
                    'source_file': file_info.get('source_file', uploaded_file.name),
                    'pages_processed': file_info.get('processed_pages', 0),
                    'ocr_confidence': file_info.get('ocr_confidence', 0),
                    'extracted_words': content.get('extracted_words', []),
                    'english_passages': content.get('english_passages', [])
                }
                combined_data['extracted_data'].append(extracted_entry)
                
            else:
                # æ—§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ
                file_summary = file_content.get('extraction_summary', {})
                combined_data['extraction_summary']['total_source_files'] += file_summary.get('total_source_files', 0)
                combined_data['extraction_summary']['total_words_extracted'] += file_summary.get('total_words_extracted', 0)
                
                file_extracted_data = file_content.get('extracted_data', [])
                combined_data['extracted_data'].extend(file_extracted_data)
        
        return combined_data
        
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.error(f"è©³ç´°: {str(e)}")
        return None

@st.cache_data
def load_vocabulary_books():
    """å˜èªå¸³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆçµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰"""
    try:
        from vocab_data import get_embedded_vocabulary_data
        
        # çµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        embedded_data = get_embedded_vocabulary_data()
        
        # å°æ–‡å­—åŒ–ã—ã¦æ­£è¦åŒ–
        vocab_data = {}
        for name, word_set in embedded_data.items():
            vocab_data[name] = {word.lower().strip() for word in word_set if word and len(word) > 1}
        
        return vocab_data
    except Exception as e:
        st.error(f"å˜èªå¸³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}")
        return {}

def setup_analysis_sidebar(analysis_data):
    """åˆ†æç”¨ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š"""
    st.sidebar.title("ğŸ“Š åˆ†æè¨­å®š")
    
    # åˆ†æå¯¾è±¡é¸æŠ
    st.sidebar.subheader("ğŸ« åˆ†æå¯¾è±¡")
    available_universities = list(analysis_data.get('university_analysis', {}).keys())
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
    if len(available_universities) == 0:
        st.sidebar.error("âŒ å¤§å­¦ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.sidebar.write("**ãƒ‡ãƒãƒƒã‚°æƒ…å ±:**")
        st.sidebar.write(f"analysis_data keys: {list(analysis_data.keys())}")
        st.sidebar.write(f"university_analysis keys: {list(analysis_data.get('university_analysis', {}).keys())}")
    else:
        st.sidebar.success(f"âœ… {len(available_universities)}å¤§å­¦ãƒ»å­¦éƒ¨ã‚’æ¤œå‡º")
    
    # å‰å›ã®é¸æŠçŠ¶æ…‹ã‚’å–å¾—
    previous_selection = st.session_state.get('selected_universities', [])
    
    selected_universities = st.sidebar.multiselect(
        "å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠ",
        available_universities,
        default=available_universities[:3] if len(available_universities) >= 3 else available_universities,
        help="æ¯”è¼ƒåˆ†æã™ã‚‹å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦ãã ã•ã„",
        key="university_selector"
    )
    
    # é¸æŠãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã®æ¤œå‡º
    if selected_universities != previous_selection:
        st.session_state.selected_universities = selected_universities
        st.session_state.selection_changed = True
        st.sidebar.success("ğŸ”„ é¸æŠã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    else:
        st.session_state.selected_universities = selected_universities
        st.session_state.selection_changed = False
    
    st.sidebar.markdown("---")
    
    # æŒ‡æ¨™èª¬æ˜
    st.sidebar.subheader("ğŸ’¡ æŒ‡æ¨™ã®æ„å‘³")
    st.sidebar.markdown("""
    **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡**: å˜èªå¸³ã®ä½•%ãŒå…¥è©¦ã«å‡ºç¾
    
    **æŠ½å‡ºç²¾åº¦**: å­¦ç¿’èªå½™ã®ä½•%ãŒå…¥è©¦ã«å‡ºç¾
    
    **ä¸€è‡´èªæ•°**: å®Ÿéš›ã«ä¸€è‡´ã—ãŸèªæ•°
    """)
    
    # åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³
    st.sidebar.subheader("ğŸ”§ åˆ†æã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    
    exclude_basic = st.sidebar.checkbox(
        "åŸºç¤èªå½™ã‚’é™¤å¤–ã—ã¦åˆ†æ",
        value=False,
        help="Target 1200ã®åŸºç¤èªå½™ã‚’é™¤å¤–ã—ã¦ã€ã‚ˆã‚Šé«˜åº¦ãªèªå½™ã®ã¿ã‚’åˆ†æã—ã¾ã™"
    )
    
    if exclude_basic:
        st.sidebar.info("ğŸ“ Target 1200ã®åŸºç¤èªå½™ã‚’é™¤å¤–ã—ãŸé«˜åº¦èªå½™åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™")
    else:
        st.sidebar.info("ğŸ“ å…¨èªå½™ã‚’å«ã‚€æ¨™æº–åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™")
    
    # é™¤å¤–è¨­å®šã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
    st.session_state.exclude_basic_vocab = exclude_basic
    
    st.sidebar.markdown("---")
    
    # ãƒ‡ãƒ¼ã‚¿æƒ…å ±
    st.sidebar.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æƒ…å ±")
    overall_summary = analysis_data.get('overall_summary', {})
    st.sidebar.write(f"**ç·å¤§å­¦æ•°**: {len(available_universities)}")
    st.sidebar.write(f"**é¸æŠä¸­**: {len(selected_universities)}å¤§å­¦ãƒ»å­¦éƒ¨")
    st.sidebar.write(f"**å˜èªå¸³æ•°**: 5ç¨®é¡")
    st.sidebar.write(f"**ç·å˜èªæ•°**: {overall_summary.get('total_words_extracted', 0):,}")
    
    if exclude_basic:
        st.sidebar.write(f"**åˆ†æãƒ¢ãƒ¼ãƒ‰**: é«˜åº¦èªå½™ã®ã¿")

def recalculate_vocabulary_analysis_with_basic_exclusion(analysis_data, exclude_basic_vocab=False):
    """åŸºç¤èªå½™é™¤å¤–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åŸºã¥ã„ã¦èªå½™åˆ†æã‚’å†è¨ˆç®—"""
    if not exclude_basic_vocab:
        return analysis_data
    
    # åŸºç¤èªå½™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    from vocab_data import get_embedded_vocabulary_data
    vocab_books = get_embedded_vocabulary_data()
    basic_vocab = vocab_books.get('Target 1200', set())
    
    # åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
    recalculated_data = {
        'overall_summary': analysis_data.get('overall_summary', {}),
        'vocabulary_summary': {},
        'university_analysis': {}
    }
    
    # å„å¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºç¤èªå½™é™¤å¤–ã§å†è¨ˆç®—
    university_analysis = analysis_data.get('university_analysis', {})
    
    for univ_name, univ_data in university_analysis.items():
        # å…ƒã®æŠ½å‡ºèªå½™ã‹ã‚‰åŸºç¤èªå½™ã‚’é™¤å¤–
        original_vocab_coverage = univ_data.get('vocabulary_coverage', {})
        
        # å„å˜èªå¸³ã«å¯¾ã—ã¦å†è¨ˆç®—
        new_vocab_coverage = {}
        
        # å…ƒã®å…¨æŠ½å‡ºèªå½™ã‚’å–å¾—ï¼ˆæ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ï¼‰
        all_extracted_words = univ_data.get('all_extracted_words', [])
        
        # ã‚‚ã—all_extracted_wordsãŒãªã„å ´åˆã¯ã€æ—§æ–¹å¼ã§å¾©å…ƒã‚’è©¦ã¿ã‚‹
        if not all_extracted_words:
            all_extracted_words = set()
            for vocab_name, coverage in original_vocab_coverage.items():
                all_extracted_words.update(coverage.get('matched_words', []))
            if 'Target 1900' in original_vocab_coverage:
                all_extracted_words.update(original_vocab_coverage['Target 1900'].get('unmatched_words', []))
            all_extracted_words = list(all_extracted_words)
        
        # åŸºç¤èªå½™ã‚’é™¤å¤–ï¼ˆå…ƒã®èªå½™æ•°ã¨é™¤å¤–å¾Œã®æ•°ã‚’è¨˜éŒ²ï¼‰
        original_count = len(all_extracted_words)
        filtered_words = [word for word in all_extracted_words if word not in basic_vocab]
        excluded_count = original_count - len(filtered_words)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        if original_count > 0:
            st.caption(f"ğŸ” {univ_name}: å…ƒã®èªå½™{original_count}èª â†’ åŸºç¤èªå½™{excluded_count}èªé™¤å¤– â†’ é«˜åº¦èªå½™{len(filtered_words)}èª")
        
        # å„å˜èªå¸³ã¨ã®å†æ¯”è¼ƒ
        for vocab_name, vocab_set in vocab_books.items():
            matched_words = [word for word in filtered_words if word in vocab_set]
            matched_count = len(matched_words)
            
            target_coverage_rate = (matched_count / len(vocab_set)) * 100 if vocab_set else 0
            extraction_precision = (matched_count / len(filtered_words)) * 100 if filtered_words else 0
            
            unmatched_words = [word for word in filtered_words if word not in vocab_set]
            
            new_vocab_coverage[vocab_name] = {
                'matched_words_count': matched_count,
                'target_coverage_rate': target_coverage_rate,
                'extraction_precision': extraction_precision,
                'matched_words': matched_words[:20],  # è¡¨ç¤ºç”¨ã«20èªã®ã¿ä¿å­˜
                'unmatched_words': unmatched_words,
                'unmatched_count': len(unmatched_words)
            }
        
        # å¤§å­¦ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
        recalculated_data['university_analysis'][univ_name] = {
            'source_file': univ_data.get('source_file', ''),
            'total_words': univ_data.get('total_words', 0),
            'unique_words': len(filtered_words),  # åŸºç¤èªå½™é™¤å¤–å¾Œã®èªå½™æ•°
            'original_unique_words': original_count,  # å…ƒã®èªå½™æ•°
            'excluded_basic_words': excluded_count,  # é™¤å¤–ã•ã‚ŒãŸåŸºç¤èªå½™æ•°
            'vocabulary_coverage': new_vocab_coverage,
            'pages_processed': univ_data.get('pages_processed', 0),
            'basic_vocab_excluded': True  # é™¤å¤–ãƒ•ãƒ©ã‚°
        }
    
    # å…¨ä½“ã‚µãƒãƒªãƒ¼ã‚’å†è¨ˆç®—
    if recalculated_data['university_analysis']:
        vocab_summary = {}
        
        for vocab_name in vocab_books.keys():
            coverage_rates = []
            precisions = []
            total_matched = 0
            
            for univ_data in recalculated_data['university_analysis'].values():
                vocab_coverage = univ_data.get('vocabulary_coverage', {}).get(vocab_name, {})
                if vocab_coverage:
                    coverage_rates.append(vocab_coverage.get('target_coverage_rate', 0))
                    precisions.append(vocab_coverage.get('extraction_precision', 0))
                    total_matched += vocab_coverage.get('matched_words_count', 0)
            
            if coverage_rates:
                vocab_summary[vocab_name] = {
                    'average_coverage_rate': sum(coverage_rates) / len(coverage_rates),
                    'average_extraction_precision': sum(precisions) / len(precisions),
                    'total_matched_words': total_matched
                }
        
        recalculated_data['vocabulary_summary'] = vocab_summary
    
    return recalculated_data

def filter_analysis_data_by_selection(analysis_data, selected_universities):
    """é¸æŠã•ã‚ŒãŸå¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸåˆ†æçµæœã‚’ä½œæˆ"""
    if not selected_universities:
        return analysis_data
    
    # å…ƒã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¸æŠã•ã‚ŒãŸå¤§å­¦ã®ã¿ã‚’æŠ½å‡º
    filtered_data = {
        'overall_summary': analysis_data.get('overall_summary', {}),
        'vocabulary_summary': {},
        'university_analysis': {}
    }
    
    # é¸æŠã•ã‚ŒãŸå¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    university_analysis = analysis_data.get('university_analysis', {})
    for univ_name in selected_universities:
        if univ_name in university_analysis:
            filtered_data['university_analysis'][univ_name] = university_analysis[univ_name]
    
    # é¸æŠã•ã‚ŒãŸå¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èªå½™ã‚µãƒãƒªãƒ¼ã‚’å†è¨ˆç®—
    if filtered_data['university_analysis']:
        vocab_books = load_vocabulary_books()
        vocab_summary = {}
        
        for vocab_name in vocab_books.keys():
            coverage_rates = []
            precisions = []
            total_matched = 0
            
            for univ_data in filtered_data['university_analysis'].values():
                vocab_coverage = univ_data.get('vocabulary_coverage', {}).get(vocab_name, {})
                if vocab_coverage:
                    coverage_rates.append(vocab_coverage.get('target_coverage_rate', 0))
                    precisions.append(vocab_coverage.get('extraction_precision', 0))
                    total_matched += vocab_coverage.get('matched_words_count', 0)
            
            if coverage_rates:
                vocab_summary[vocab_name] = {
                    'average_coverage_rate': sum(coverage_rates) / len(coverage_rates),
                    'average_extraction_precision': sum(precisions) / len(precisions),
                    'total_matched_words': total_matched
                }
        
        filtered_data['vocabulary_summary'] = vocab_summary
    
    return filtered_data

def perform_vocabulary_analysis(extraction_data):
    """JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èªå½™åˆ†æã‚’å®Ÿè¡Œ"""
    try:
        st.write("ğŸ”„ åˆ†æé–‹å§‹...")
        
        # NLTK ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
        except Exception as e:
            st.warning(f"NLTK ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è­¦å‘Š: {str(e)}")
        
        # å˜èªå¸³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        st.write("ğŸ“š å˜èªå¸³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        vocab_books = load_vocabulary_books()
        if not vocab_books:
            st.error("å˜èªå¸³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        st.success(f"âœ… {len(vocab_books)}å€‹ã®å˜èªå¸³ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        st.write("ğŸ“‹ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ä¸­...")
        if not extraction_data:
            st.error("âŒ extraction_data ãŒç©ºã§ã™")
            return None
        
        st.write(f"extraction_data keys: {list(extraction_data.keys())}")
        
        # extraction_summary ã®è©³ç´°ç¢ºèª
        summary = extraction_data.get('extraction_summary', {})
        st.write(f"extraction_summary: {summary}")
        
        # extracted_data ã®è©³ç´°ç¢ºèª
        extracted_data_list = extraction_data.get('extracted_data', [])
        st.write(f"extracted_data type: {type(extracted_data_list)}")
        st.write(f"extracted_data length: {len(extracted_data_list)}")
        
        if not extracted_data_list:
            st.error("âŒ extracted_data ãƒªã‚¹ãƒˆãŒç©ºã§ã™")
            st.write("**JSONãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®ç¢ºèª:**")
            st.write("å¯¾å¿œã—ã¦ã„ã‚‹å½¢å¼:")
            st.code("""æ–°å½¢å¼:
{
  "file_info": {
    "source_file": "å¤§å­¦å_å¹´åº¦_è‹±èª_å­¦éƒ¨å.pdf",
    "processed_pages": 7,
    "ocr_confidence": 0.95
  },
  "content": {
    "extracted_words": ["word1", "word2", ...]
  }
}

æ—§å½¢å¼:
{
  "extraction_summary": {...},
  "extracted_data": [
    {
      "source_file": "å¤§å­¦å_å¹´åº¦_è‹±èª_å­¦éƒ¨å.pdf",
      "extracted_words": ["word1", "word2", ...]
    }
  ]
}""")
            st.write("**ç¾åœ¨ã®JSONãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ï¼ˆä¸€éƒ¨ï¼‰:**")
            import json
            st.code(json.dumps(extraction_data, indent=2, ensure_ascii=False)[:1000] + "...")
            return None
        
        st.success(f"âœ… {len(extracted_data_list)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªã‚’ç¢ºèª")
        
        # æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã®è©³ç´°ç¢ºèª
        if len(extracted_data_list) > 0:
            first_entry = extracted_data_list[0]
            st.write("**æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã®ç¢ºèª:**")
            st.write(f"  - Keys: {list(first_entry.keys()) if isinstance(first_entry, dict) else 'Not a dict'}")
            if isinstance(first_entry, dict):
                st.write(f"  - source_file: '{first_entry.get('source_file', 'Missing')}'")
                words = first_entry.get('extracted_words', [])
                st.write(f"  - extracted_words count: {len(words) if isinstance(words, list) else 'Not a list'}")
                if isinstance(words, list) and len(words) > 0:
                    st.write(f"  - Sample words: {words[:5]}")
                else:
                    st.write(f"  - extracted_words content: {words}")
        
        # åˆ†æçµæœã®åˆæœŸåŒ–
        analysis_result = {
            'overall_summary': {
                'total_source_files': extraction_data.get('extraction_summary', {}).get('total_source_files', 0),
                'total_words_extracted': extraction_data.get('extraction_summary', {}).get('total_words_extracted', 0),
                'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            'vocabulary_summary': {},
            'university_analysis': {}
        }
        
        # å„å¤§å­¦ãƒ»å­¦éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
        st.write("ğŸ« å¤§å­¦ãƒ»å­¦éƒ¨ãƒ‡ãƒ¼ã‚¿ã®åˆ†æé–‹å§‹...")
        
        for i, entry in enumerate(extracted_data_list):
            try:
                st.write(f"--- ã‚¨ãƒ³ãƒˆãƒª {i+1}/{len(extracted_data_list)} ã®å‡¦ç† ---")
                
                source_file = entry.get('source_file', '')
                if not source_file:
                    st.warning(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: source_file ãŒç©ºã§ã™")
                    continue
                
                university_name = extract_university_name_from_filename(source_file)
                if not university_name or university_name == "ä¸æ˜ãªå¤§å­¦":
                    st.warning(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: å¤§å­¦åæŠ½å‡ºã«å¤±æ•— - '{source_file}'")
                    continue
                
                # æŠ½å‡ºã•ã‚ŒãŸå˜èªã‚’æ­£è¦åŒ–ï¼ˆLemmatizationå«ã‚€ï¼‰
                extracted_words = entry.get('extracted_words', [])
                if not extracted_words:
                    st.warning(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: extracted_words ãŒç©ºã§ã™")
                    continue
                
                # åŸºæœ¬çš„ãªæ­£è¦åŒ–
                cleaned_words = [word.lower().strip() for word in extracted_words if word and len(word) > 1]
                
                # åŸºç¤èªå½™é™¤å¤–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª
                exclude_basic = st.session_state.get('exclude_basic_vocab', False)
                if exclude_basic:
                    # Target 1200ã®åŸºç¤èªå½™ã‚’é™¤å¤–
                    basic_vocab = vocab_books.get('Target 1200', set())
                    cleaned_words = [word for word in cleaned_words if word not in basic_vocab]
                    st.write(f"  ğŸ”§ åŸºç¤èªå½™é™¤å¤–: Target 1200ã®{len(basic_vocab)}èªã‚’é™¤å¤–")
                
                # Lemmatizationå‡¦ç†
                try:
                    from nltk.stem import WordNetLemmatizer
                    
                    lemmatizer = WordNetLemmatizer()
                    normalized_words = []
                    
                    for word in cleaned_words:
                        # è¤‡æ•°å“è©ã§lemmatizeã‚’è©¦è¡Œ
                        lemma_verb = lemmatizer.lemmatize(word, pos='v')  # å‹•è©
                        lemma_noun = lemmatizer.lemmatize(lemma_verb, pos='n')  # åè©
                        
                        # ã‚ˆã‚ŠåŠ¹æœçš„ãªæ­£è¦åŒ–ã‚’é¸æŠ
                        if len(lemma_noun) < len(word):
                            normalized_words.append(lemma_noun)
                        elif len(lemma_verb) < len(word):
                            normalized_words.append(lemma_verb)
                        else:
                            normalized_words.append(word)
                    
                    unique_words = list(set(normalized_words))
                    
                    # LemmatizationåŠ¹æœã‚’è¡¨ç¤º
                    original_unique = len(set(cleaned_words))
                    lemmatized_unique = len(unique_words)
                    if original_unique != lemmatized_unique:
                        st.write(f"  ğŸ“ LemmatizationåŠ¹æœ: {original_unique}èª â†’ {lemmatized_unique}èª")
                    
                except Exception as lemma_error:
                    st.warning(f"âš ï¸ Lemmatizationå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ã€åŸºæœ¬æ­£è¦åŒ–ã‚’ä½¿ç”¨: {str(lemma_error)}")
                    normalized_words = cleaned_words
                    unique_words = list(set(normalized_words))
                
                # åŸºç¤èªå½™é™¤å¤–ã®åŠ¹æœã‚’è¡¨ç¤º
                exclude_basic = st.session_state.get('exclude_basic_vocab', False)
                if exclude_basic:
                    basic_vocab = vocab_books.get('Target 1200', set())
                    original_unique = len(set([word.lower().strip() for word in extracted_words if word and len(word) > 1]))
                    excluded_count = original_unique - len(unique_words)
                    st.write(f"âœ… {university_name}: {len(extracted_words)}èª â†’ {original_unique}ãƒ¦ãƒ‹ãƒ¼ã‚¯èª â†’ {len(unique_words)}é«˜åº¦èªå½™ (åŸºç¤èªå½™{excluded_count}èªé™¤å¤–)")
                else:
                    st.write(f"âœ… {university_name}: {len(extracted_words)}èª â†’ {len(unique_words)}ãƒ¦ãƒ‹ãƒ¼ã‚¯èª")
                
                # å„å˜èªå¸³ã¨ã®æ¯”è¼ƒåˆ†æ
                vocab_coverage = {}
                for vocab_name, vocab_set in vocab_books.items():
                    matched_words = [word for word in unique_words if word in vocab_set]
                    matched_count = len(matched_words)
                    
                    target_coverage_rate = (matched_count / len(vocab_set)) * 100 if vocab_set else 0
                    extraction_precision = (matched_count / len(unique_words)) * 100 if unique_words else 0
                    
                    # ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„å˜èªï¼ˆå˜èªå¸³ã«ãªã„å˜èªï¼‰
                    unmatched_words = [word for word in unique_words if word not in vocab_set]
                    
                    vocab_coverage[vocab_name] = {
                        'matched_words_count': matched_count,
                        'target_coverage_rate': target_coverage_rate,
                        'extraction_precision': extraction_precision,
                        'matched_words': matched_words[:20],  # è¡¨ç¤ºç”¨ã«20èªã®ã¿ä¿å­˜
                        'unmatched_words': unmatched_words,  # å…¨ã¦ã®ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„å˜èªã‚’ä¿å­˜
                        'unmatched_count': len(unmatched_words)
                    }
            
                # å¤§å­¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                try:
                    analysis_result['university_analysis'][university_name] = {
                        'source_file': source_file,
                        'total_words': len(extracted_words),
                        'unique_words': len(unique_words),
                        'all_extracted_words': unique_words,  # å…¨æŠ½å‡ºèªå½™ã‚’ä¿å­˜ï¼ˆåŸºç¤èªå½™é™¤å¤–ç”¨ï¼‰
                        'vocabulary_coverage': vocab_coverage,
                        'pages_processed': entry.get('pages_processed', 0)
                    }
                    st.success(f"âœ… {university_name} ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
                    st.write(f"ç¾åœ¨ã®å¤§å­¦æ•°: {len(analysis_result['university_analysis'])}")
                except Exception as save_error:
                    st.error(f"âŒ {university_name} ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(save_error)}")
                    continue
                    
            except Exception as entry_error:
                st.error(f"âŒ ã‚¨ãƒ³ãƒˆãƒª {i+1} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(entry_error)}")
                continue
        
        # å…¨ä½“ã‚µãƒãƒªãƒ¼ã®è¨ˆç®—
        all_coverage_data = {vocab_name: [] for vocab_name in vocab_books.keys()}
        for univ_data in analysis_result['university_analysis'].values():
            for vocab_name in vocab_books.keys():
                coverage = univ_data['vocabulary_coverage'][vocab_name]
                all_coverage_data[vocab_name].append({
                    'coverage_rate': coverage['target_coverage_rate'],
                    'precision': coverage['extraction_precision'],
                    'matched_count': coverage['matched_words_count']
                })
        
        # èªå½™ã‚µãƒãƒªãƒ¼ã®è¨ˆç®—
        for vocab_name, coverage_list in all_coverage_data.items():
            if coverage_list:
                avg_coverage = sum(item['coverage_rate'] for item in coverage_list) / len(coverage_list)
                avg_precision = sum(item['precision'] for item in coverage_list) / len(coverage_list)
                total_matched = sum(item['matched_count'] for item in coverage_list)
                
                analysis_result['vocabulary_summary'][vocab_name] = {
                    'average_coverage_rate': avg_coverage,
                    'average_extraction_precision': avg_precision,
                    'total_matched_words': total_matched
                }
        
        # æœ€çµ‚çµæœã®ç¢ºèª
        st.success(f"âœ… åˆ†æå®Œäº†: {len(analysis_result['university_analysis'])}å¤§å­¦ãƒ»å­¦éƒ¨")
        st.write("**æœ€çµ‚çµæœ:**")
        st.write(f"  - university_analysis keys: {list(analysis_result['university_analysis'].keys())}")
        
        return analysis_result
        
    except Exception as e:
        st.error(f"èªå½™åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None

def extract_university_name_from_filename(filename):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨åã‚’æŠ½å‡º"""
    if not filename:
        return "ä¸æ˜ãªå¤§å­¦"
    
    # PDFãƒ•ã‚¡ã‚¤ãƒ«åã®ä¾‹: "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_2024å¹´åº¦_è‹±èª_è–¬å­¦éƒ¨.pdf"
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ‘ã‚¹éƒ¨åˆ†ã‚’å–ã‚Šé™¤ã
    basename = filename.split('/')[-1] if '/' in filename else filename
    parts = basename.replace('.pdf', '').split('_')
    
    if len(parts) >= 4:
        university = parts[0]
        department = parts[3]
        return f"{university}_{department}"
    elif len(parts) >= 1:
        return parts[0]
    
    return basename.replace('.pdf', '')

def show_analysis_dashboard(analysis_data):
    """åˆ†æçµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®è¡¨ç¤º"""
    if not analysis_data:
        st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    setup_analysis_sidebar(analysis_data)
    
    # åŸºç¤èªå½™é™¤å¤–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ç¢ºèª
    exclude_basic = st.session_state.get('exclude_basic_vocab', False)
    
    # åŸºç¤èªå½™é™¤å¤–ãŒæœ‰åŠ¹ãªå ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚’å†è¨ˆç®—
    if exclude_basic:
        with st.spinner("åŸºç¤èªå½™é™¤å¤–ãƒ¢ãƒ¼ãƒ‰ã§å†è¨ˆç®—ä¸­..."):
            analysis_data = recalculate_vocabulary_analysis_with_basic_exclusion(analysis_data, exclude_basic_vocab=True)
    
    # é¸æŠã•ã‚ŒãŸå¤§å­¦ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    selected_universities = st.session_state.get('selected_universities', [])
    filtered_data = filter_analysis_data_by_selection(analysis_data, selected_universities)
    
    # é¸æŠå¤‰æ›´æ™‚ã®é€šçŸ¥
    if st.session_state.get('selection_changed', False):
        mode_text = "ï¼ˆé«˜åº¦èªå½™ã®ã¿ï¼‰" if exclude_basic else "ï¼ˆå…¨èªå½™ï¼‰"
        st.info(f"ğŸ”„ {len(selected_universities)}å¤§å­¦ãƒ»å­¦éƒ¨ã®åˆ†æçµæœã‚’è¡¨ç¤ºä¸­... {mode_text}")
    
    # åˆ†æãƒ¢ãƒ¼ãƒ‰ã®è¡¨ç¤º
    if exclude_basic:
        st.success("ğŸ¯ **é«˜åº¦èªå½™åˆ†æãƒ¢ãƒ¼ãƒ‰**: Target 1200ã®åŸºç¤èªå½™ã‚’é™¤å¤–ã—ãŸåˆ†æçµæœã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™")
        st.caption("ğŸ’¡ ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã¨æŠ½å‡ºç²¾åº¦ã¯ã€Target 1200ã®1,400èªã‚’é™¤å¤–ã—ãŸé«˜åº¦èªå½™ã§å†è¨ˆç®—ã•ã‚Œã¦ã„ã¾ã™")
        
        # åŸºç¤èªå½™é™¤å¤–çµ±è¨ˆã®è¡¨ç¤º
        show_basic_exclusion_stats(filtered_data)
        
    else:
        st.info("ğŸ“Š **æ¨™æº–åˆ†æãƒ¢ãƒ¼ãƒ‰**: å…¨èªå½™ã‚’å«ã‚€åˆ†æçµæœã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™")
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–ã®ä½œæˆ
    tab1, tab2, tab3 = st.tabs(["ğŸ  æ¦‚è¦åˆ†æ", "ğŸ« å¤§å­¦åˆ¥è©³ç´°", "ğŸ“Š æ¯”è¼ƒåˆ†æ"])
    
    with tab1:
        show_overview_analysis(filtered_data)
    
    with tab2:
        show_university_analysis(filtered_data)
    
    with tab3:
        show_comparison_analysis(filtered_data)

def show_basic_exclusion_stats(analysis_data: dict):
    """åŸºç¤èªå½™é™¤å¤–ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    university_analysis = analysis_data.get('university_analysis', {})
    
    if not university_analysis:
        return
    
    # çµ±è¨ˆæƒ…å ±ã‚’é›†è¨ˆ
    total_original = 0
    total_excluded = 0
    total_remaining = 0
    
    for univ_data in university_analysis.values():
        if univ_data.get('basic_vocab_excluded', False):
            total_original += univ_data.get('original_unique_words', 0)
            total_excluded += univ_data.get('excluded_basic_words', 0)
            total_remaining += univ_data.get('unique_words', 0)
    
    if total_original > 0:
        exclusion_rate = (total_excluded / total_original) * 100
        
        st.markdown("### ğŸ”§ åŸºç¤èªå½™é™¤å¤–çµ±è¨ˆ")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "ğŸ“Š å…ƒã®èªå½™æ•°",
                f"{total_original:,}èª",
                help="åŸºç¤èªå½™é™¤å¤–å‰ã®ç·ãƒ¦ãƒ‹ãƒ¼ã‚¯èªå½™æ•°"
            )
        
        with col2:
            st.metric(
                "âŒ é™¤å¤–ã•ã‚ŒãŸèªå½™",
                f"{total_excluded:,}èª",
                delta=f"-{exclusion_rate:.1f}%",
                help="Target 1200ã«å«ã¾ã‚Œã‚‹åŸºç¤èªå½™æ•°"
            )
        
        with col3:
            st.metric(
                "âœ… é«˜åº¦èªå½™æ•°",
                f"{total_remaining:,}èª",
                help="åŸºç¤èªå½™é™¤å¤–å¾Œã®é«˜åº¦èªå½™æ•°"
            )
        
        with col4:
            st.metric(
                "ğŸ¯ é«˜åº¦èªå½™ç‡",
                f"{100-exclusion_rate:.1f}%",
                help="å…¨èªå½™ã«å ã‚ã‚‹é«˜åº¦èªå½™ã®å‰²åˆ"
            )
        
        st.markdown("---")

def show_overview_analysis(analysis_data: dict):
    """æ¦‚è¦åˆ†æã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    
    # ç°¡æ½”ãªå®šç¾©ï¼ˆå¸¸æ™‚è¡¨ç¤ºï¼‰
    exclude_basic = st.session_state.get('exclude_basic_vocab', False)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.info("""
        **ğŸ“ˆ ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã¨ã¯ï¼Ÿ**  
        å˜èªå¸³ã®ä½•%ã®èªå½™ãŒå®Ÿéš›ã®å…¥è©¦å•é¡Œã«å‡ºç¾ã—ãŸã‹ã‚’ç¤ºã™æŒ‡æ¨™ã€‚é«˜ã„ã»ã©å®Ÿç”¨çš„ã€‚
        """)
    
    with col2:
        st.info("""
        **ğŸ¯ æŠ½å‡ºç²¾åº¦ã¨ã¯ï¼Ÿ**  
        æŠ½å‡ºã—ãŸå˜èªã®ã†ã¡ã€å˜èªå¸³ã«å«ã¾ã‚Œã‚‹å‰²åˆã€‚é«˜ã„ã»ã©å­¦ç¿’åŠ¹ç‡ãŒè‰¯ã„ã€‚
        """)
    
    with col3:
        if exclude_basic:
            st.warning("""
            **ğŸ¯ é«˜åº¦èªå½™åˆ†æãƒ¢ãƒ¼ãƒ‰**  
            Target 1200ã®åŸºç¤èªå½™ï¼ˆ1,400èªï¼‰ã‚’é™¤å¤–ã—ã€é«˜åº¦èªå½™ã®ã¿ã§ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã¨æŠ½å‡ºç²¾åº¦ã‚’å†è¨ˆç®—ã€‚
            """)
        else:
            st.info("""
            **ğŸ“Š æ¨™æº–åˆ†æãƒ¢ãƒ¼ãƒ‰**  
            å…¨èªå½™ã‚’å«ã‚€æ¨™æº–çš„ãªåˆ†æã€‚
            """)
    
    st.markdown("---")
    
    # å¤§å­¦é¸æŠçŠ¶æ³ã‚’ç¢ºèª
    selected_universities = st.session_state.get('selected_universities', [])
    
    if not selected_universities:
        # å¤§å­¦ãŒé¸æŠã•ã‚Œã¦ã„ãªã„å ´åˆ
        st.info("""
        ğŸ‘ˆ **å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦ãã ã•ã„**
        
        é¸æŠã—ãŸå¤§å­¦ãƒ»å­¦éƒ¨ã®èªå½™åˆ†æçµæœã¨ãƒãƒ£ãƒ¼ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
        
        - è¤‡æ•°ã®å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦æ¯”è¼ƒåˆ†æã‚‚å¯èƒ½ã§ã™
        - ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã®é–¾å€¤è¨­å®šã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚‚ã§ãã¾ã™
        """)
        return
    
    # é¸æŠã•ã‚ŒãŸå¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    university_analysis = analysis_data.get('university_analysis', {})
    vocabulary_summary = analysis_data.get('vocabulary_summary', {})
    
    # é¸æŠã•ã‚ŒãŸå¤§å­¦ã®çµ±è¨ˆ
    selected_total_words = sum([info.get('total_words', 0) for univ, info in university_analysis.items() if univ in selected_universities])
    selected_total_pages = sum([info.get('pages_processed', 0) for univ, info in university_analysis.items() if univ in selected_universities])
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="é¸æŠå¤§å­¦ç·å˜èªæ•°",
            value=f"{selected_total_words:,}",
            delta=f"{len(selected_universities)}å¤§å­¦ãƒ»å­¦éƒ¨"
        )
    
    with col2:
        # æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã‚’è¨ˆç®—
        best_coverage = 0
        best_vocab = "N/A"
        for vocab_name, summary in vocabulary_summary.items():
            if summary.get('average_coverage_rate', 0) > best_coverage:
                best_coverage = summary.get('average_coverage_rate', 0)
                best_vocab = vocab_name
        
        st.metric(
            label="æœ€é©å˜èªå¸³",
            value=best_vocab,
            delta=f"{best_coverage:.1f}%"
        )
    
    with col3:
        st.metric(
            label="å‡¦ç†ãƒšãƒ¼ã‚¸æ•°",
            value=f"{selected_total_pages}",
            delta=None
        )
    
    st.markdown("---")
    
    # å˜èªå¸³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆ
    if vocabulary_summary:
        st.subheader("ğŸ“š å˜èªå¸³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        vocab_data = []
        for vocab_name, summary in vocabulary_summary.items():
            vocab_data.append({
                'å˜èªå¸³': vocab_name,
                'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': summary.get('average_coverage_rate', 0),
                'æŠ½å‡ºç²¾åº¦(%)': summary.get('average_extraction_precision', 0),
                'ä¸€è‡´èªæ•°': summary.get('total_matched_words', 0)
            })
        
        vocab_df = pd.DataFrame(vocab_data)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡æ£’ã‚°ãƒ©ãƒ•
            fig_coverage = px.bar(
                vocab_df, 
                x='å˜èªå¸³', 
                y='ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                title='å˜èªå¸³åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡',
                color='ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                color_continuous_scale='RdYlGn'
            )
            fig_coverage.update_layout(height=400)
            st.plotly_chart(fig_coverage, use_container_width=True)
        
        with col2:
            # æŠ½å‡ºç²¾åº¦æ£’ã‚°ãƒ©ãƒ•
            fig_precision = px.bar(
                vocab_df, 
                x='å˜èªå¸³', 
                y='æŠ½å‡ºç²¾åº¦(%)',
                title='å˜èªå¸³åˆ¥æŠ½å‡ºç²¾åº¦',
                color='æŠ½å‡ºç²¾åº¦(%)',
                color_continuous_scale='RdYlBu'
            )
            fig_precision.update_layout(height=400)
            st.plotly_chart(fig_precision, use_container_width=True)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        st.subheader("ğŸ“Š å˜èªå¸³ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«")
        try:
            st.dataframe(
                vocab_df.style.format({
                    'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}',
                    'æŠ½å‡ºç²¾åº¦(%)': '{:.1f}',
                    'ä¸€è‡´èªæ•°': '{:,}'
                }).background_gradient(subset=['ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)', 'æŠ½å‡ºç²¾åº¦(%)'], cmap='RdYlGn'),
                use_container_width=True
            )
        except ImportError:
            st.dataframe(
                vocab_df.style.format({
                    'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}',
                    'æŠ½å‡ºç²¾åº¦(%)': '{:.1f}',
                    'ä¸€è‡´èªæ•°': '{:,}'
                }),
                use_container_width=True
            )
        
        # ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„å˜èªã®çµ±è¨ˆ
        st.markdown("---")
        st.subheader("ğŸ“ ã‚«ãƒãƒ¼å¤–èªå½™ã®çµ±è¨ˆ")
        st.info("""
        å„å˜èªå¸³ã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„èªå½™ã®çµ±è¨ˆæƒ…å ±ã§ã™ã€‚ã“ã‚Œã‚‰ã¯è¿½åŠ å­¦ç¿’å¯¾è±¡ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        """)
        
        # é¸æŠã•ã‚ŒãŸå¤§å­¦ã®ã‚«ãƒãƒ¼å¤–èªå½™çµ±è¨ˆã‚’è¨ˆç®—
        selected_universities = st.session_state.get('selected_universities', [])
        university_analysis = analysis_data.get('university_analysis', {})
        
        if selected_universities and university_analysis:
            uncovered_stats = []
            
            for vocab_name in vocabulary_summary.keys():
                total_unmatched = 0
                total_words = 0
                
                for univ_name in selected_universities:
                    univ_data = university_analysis.get(univ_name, {})
                    vocab_coverage = univ_data.get('vocabulary_coverage', {}).get(vocab_name, {})
                    
                    total_unmatched += vocab_coverage.get('unmatched_count', 0)
                    total_words += univ_data.get('unique_words', 0)
                
                if total_words > 0:
                    uncovered_rate = (total_unmatched / len(selected_universities) / total_words * len(selected_universities)) * 100
                    uncovered_stats.append({
                        'å˜èªå¸³': vocab_name,
                        'ã‚«ãƒãƒ¼å¤–èªæ•°': total_unmatched // len(selected_universities),
                        'ã‚«ãƒãƒ¼å¤–ç‡(%)': round(uncovered_rate, 1)
                    })
            
            if uncovered_stats:
                uncovered_df = pd.DataFrame(uncovered_stats)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # ã‚«ãƒãƒ¼å¤–èªæ•°ã®æ£’ã‚°ãƒ©ãƒ•
                    fig_uncovered = px.bar(
                        uncovered_df,
                        x='å˜èªå¸³',
                        y='ã‚«ãƒãƒ¼å¤–èªæ•°',
                        title='å˜èªå¸³åˆ¥ã‚«ãƒãƒ¼å¤–èªæ•°',
                        color='ã‚«ãƒãƒ¼å¤–èªæ•°',
                        color_continuous_scale='Reds'
                    )
                    fig_uncovered.update_layout(height=400)
                    st.plotly_chart(fig_uncovered, use_container_width=True)
                
                with col2:
                    # ã‚«ãƒãƒ¼å¤–ç‡ã®æ£’ã‚°ãƒ©ãƒ•
                    fig_uncovered_rate = px.bar(
                        uncovered_df,
                        x='å˜èªå¸³',
                        y='ã‚«ãƒãƒ¼å¤–ç‡(%)',
                        title='å˜èªå¸³åˆ¥ã‚«ãƒãƒ¼å¤–ç‡',
                        color='ã‚«ãƒãƒ¼å¤–ç‡(%)',
                        color_continuous_scale='OrRd'
                    )
                    fig_uncovered_rate.update_layout(height=400)
                    st.plotly_chart(fig_uncovered_rate, use_container_width=True)
                
                # çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
                st.dataframe(uncovered_df, use_container_width=True)

def show_university_analysis(analysis_data: dict):
    """å¤§å­¦åˆ¥è©³ç´°ã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    
    # ç°¡æ½”ãªæŒ‡æ¨™èª¬æ˜
    st.info("""
    ğŸ’¡ **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡**: å˜èªå¸³ã®ä½•%ãŒå…¥è©¦ã«å‡ºç¾ | **æŠ½å‡ºç²¾åº¦**: å­¦ç¿’ã—ãŸèªå½™ã®ä½•%ãŒå…¥è©¦ã«å‡ºç¾ | è©³ç´°ã¯æ¦‚è¦ã‚¿ãƒ–ã§ç¢ºèª
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠã•ã‚ŒãŸå¤§å­¦ã‚’å–å¾—
    selected_universities = st.session_state.get('selected_universities', [])
    
    if not selected_universities:
        st.info("""
        ğŸ‘ˆ **å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦ãã ã•ã„**
        
        é¸æŠã—ãŸå¤§å­¦ãƒ»å­¦éƒ¨ã®è©³ç´°åˆ†æãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
        
        - 1ã¤ã®å¤§å­¦ã‚’é¸æŠã™ã‚‹ã¨ã€ãã®å¤§å­¦ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
        - è¤‡æ•°é¸æŠæ™‚ã¯æœ€åˆã®å¤§å­¦ã®è©³ç´°ã‚’è¡¨ç¤º
        """)
        return
    
    # æœ€åˆã«é¸æŠã•ã‚ŒãŸå¤§å­¦ã®è©³ç´°ã‚’è¡¨ç¤º
    selected_university = selected_universities[0]
    if len(selected_universities) > 1:
        st.info(f"è¤‡æ•°é¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚{selected_university} ã®è©³ç´°ã‚’è¡¨ç¤ºä¸­ï¼ˆä»– {len(selected_universities)-1} å¤§å­¦ï¼‰")
    
    university_data = analysis_data.get('university_analysis', {}).get(selected_university, {})
    university_meta = {}  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ç¾åœ¨åˆ©ç”¨ä¸å¯
    
    # å¤§å­¦æƒ…å ±ã‚«ãƒ¼ãƒ‰
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.info(f"""
        **ğŸ« {selected_university}**
        - ãƒ•ã‚¡ã‚¤ãƒ«: {university_data.get('source_file', 'N/A').split('/')[-1] if university_data.get('source_file') else 'N/A'}
        """)
    
    with col2:
        st.info(f"""
        **ğŸ“Š å‡¦ç†çµ±è¨ˆ**
        - ç·å˜èªæ•°: {university_data.get('total_words', 0):,}
        - ãƒ¦ãƒ‹ãƒ¼ã‚¯èªæ•°: {university_data.get('unique_words', 0):,}
        - å‡¦ç†ãƒšãƒ¼ã‚¸: {university_data.get('pages_processed', 0)}
        """)
    
    with col3:
        st.info(f"""
        **ğŸ“š èªå½™åˆ†æ**
        - å˜èªå¸³æ•°: {len(university_data.get('vocabulary_coverage', {}))}
        """)
    
    st.markdown("---")
    
    # å˜èªå¸³åˆ¥è©³ç´°
    st.subheader("ğŸ“š å˜èªå¸³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
    
    vocab_coverage = university_data.get('vocabulary_coverage', {})
    vocab_df_data = []
    
    for vocab_name, vocab_stats in vocab_coverage.items():
        vocab_df_data.append({
            'å˜èªå¸³': vocab_name,
            'ä¸€è‡´èªæ•°': vocab_stats.get('matched_words_count', 0),
            'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': round(vocab_stats.get('target_coverage_rate', 0), 1),
            'æŠ½å‡ºç²¾åº¦(%)': round(vocab_stats.get('extraction_precision', 0), 1)
        })
    
    vocab_df = pd.DataFrame(vocab_df_data)
    
    # ã‚¹ã‚¿ã‚¤ãƒ«ä»˜ããƒ†ãƒ¼ãƒ–ãƒ«
    try:
        st.dataframe(
            vocab_df.style.format({
                'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}',
                'æŠ½å‡ºç²¾åº¦(%)': '{:.1f}'
            }).background_gradient(subset=['ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)', 'æŠ½å‡ºç²¾åº¦(%)'], cmap='RdYlGn'),
            use_container_width=True
        )
    except ImportError:
        # matplotlibãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        st.dataframe(
            vocab_df.style.format({
                'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}',
                'æŠ½å‡ºç²¾åº¦(%)': '{:.1f}'
            }),
            use_container_width=True
        )
    
    # å˜èªå¸³æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ
    if len(vocab_df_data) > 1:
        col1, col2 = st.columns(2)
        
        with col1:
            fig_coverage = px.bar(
                vocab_df, 
                x='å˜èªå¸³', 
                y='ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                title='å˜èªå¸³åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡',
                color='ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                color_continuous_scale='RdYlGn'
            )
            fig_coverage.update_layout(height=400)
            st.plotly_chart(fig_coverage, use_container_width=True)
        
        with col2:
            fig_precision = px.bar(
                vocab_df, 
                x='å˜èªå¸³', 
                y='æŠ½å‡ºç²¾åº¦(%)',
                title='å˜èªå¸³åˆ¥æŠ½å‡ºç²¾åº¦',
                color='æŠ½å‡ºç²¾åº¦(%)',
                color_continuous_scale='RdYlBu'
            )
            fig_precision.update_layout(height=400)
            st.plotly_chart(fig_precision, use_container_width=True)
    
    # ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„å˜èªã®è¡¨ç¤º
    st.markdown("---")
    st.subheader("ğŸ“ å˜èªå¸³ã§ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„èªå½™")
    st.info("""
    ä»¥ä¸‹ã¯å…¥è©¦å•é¡Œã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãŒã€å„å˜èªå¸³ã«ã¯å«ã¾ã‚Œã¦ã„ãªã„èªå½™ã§ã™ã€‚  
    ã“ã‚Œã‚‰ã®èªå½™ã¯è¿½åŠ å­¦ç¿’ãŒå¿…è¦ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
    """)
    
    # å˜èªå¸³é¸æŠ
    vocab_tabs = st.tabs([f"ğŸ“– {vocab_name}" for vocab_name in vocab_coverage.keys()])
    
    for i, (vocab_name, vocab_stats) in enumerate(vocab_coverage.items()):
        with vocab_tabs[i]:
            unmatched_words = vocab_stats.get('unmatched_words', [])
            unmatched_count = vocab_stats.get('unmatched_count', 0)
            matched_count = vocab_stats.get('matched_words_count', 0)
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.metric(
                    label="ã‚«ãƒãƒ¼å¤–èªæ•°",
                    value=f"{unmatched_count:,}èª",
                    delta=f"ä¸€è‡´: {matched_count}èª"
                )
                
                if unmatched_count > 0:
                    coverage_ratio = (matched_count / (matched_count + unmatched_count)) * 100
                    st.write(f"**èªå½™ã‚«ãƒãƒ¼ç‡**: {coverage_ratio:.1f}%")
            
            with col2:
                if unmatched_words:
                    # è¡¨ç¤ºæ–¹æ³•ã®é¸æŠ
                    display_mode = st.radio(
                        "è¡¨ç¤ºå½¢å¼ã‚’é¸æŠ:",
                        ["ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º", "å…¨èªå½™è¡¨ç¤º", "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨"],
                        key=f"display_mode_{vocab_name}_{selected_university}",
                        horizontal=True
                    )
                    
                    if display_mode == "ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º":
                        st.write("**ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„ä¸»ãªèªå½™ï¼ˆæœ€åˆã®30èªï¼‰:**")
                        displayed_words = unmatched_words[:30]
                        for j in range(0, len(displayed_words), 5):
                            word_group = displayed_words[j:j+5]
                            st.write("â€¢ " + " â€¢ ".join(word_group))
                        
                        if len(unmatched_words) > 30:
                            st.write(f"... ä»– {len(unmatched_words) - 30}èª")
                    
                    elif display_mode == "å…¨èªå½™è¡¨ç¤º":
                        st.write(f"**ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„å…¨èªå½™ï¼ˆ{len(unmatched_words)}èªï¼‰:**")
                        
                        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
                        words_per_page = 100
                        total_pages = (len(unmatched_words) + words_per_page - 1) // words_per_page
                        
                        if total_pages > 1:
                            page = st.selectbox(
                                f"ãƒšãƒ¼ã‚¸ã‚’é¸æŠ (1-{total_pages})",
                                range(1, total_pages + 1),
                                key=f"page_{vocab_name}_{selected_university}"
                            )
                            start_idx = (page - 1) * words_per_page
                            end_idx = min(start_idx + words_per_page, len(unmatched_words))
                            page_words = unmatched_words[start_idx:end_idx]
                            
                            st.write(f"**ãƒšãƒ¼ã‚¸ {page}/{total_pages} ({start_idx + 1}-{end_idx}èª):**")
                        else:
                            page_words = unmatched_words
                        
                        # 10èªãšã¤ã®è¡Œã§è¡¨ç¤º
                        for j in range(0, len(page_words), 10):
                            word_group = page_words[j:j+10]
                            st.write("â€¢ " + " â€¢ ".join(word_group))
                    
                    elif display_mode == "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨":
                        st.write("**ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚³ãƒ”ãƒ¼ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:**")
                        
                        format_option = st.selectbox(
                            "ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’é¸æŠ:",
                            ["CSVå½¢å¼", "ãƒªã‚¹ãƒˆå½¢å¼", "æ”¹è¡ŒåŒºåˆ‡ã‚Š"],
                            key=f"format_{vocab_name}_{selected_university}"
                        )
                        
                        if format_option == "CSVå½¢å¼":
                            formatted_text = ", ".join(unmatched_words)
                        elif format_option == "ãƒªã‚¹ãƒˆå½¢å¼":
                            formatted_text = " â€¢ ".join(unmatched_words)
                        else:  # æ”¹è¡ŒåŒºåˆ‡ã‚Š
                            formatted_text = "\n".join(unmatched_words)
                        
                        st.text_area(
                            f"ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ãªã„å…¨èªå½™ ({len(unmatched_words)}èª)",
                            value=formatted_text,
                            height=300,
                            key=f"download_{vocab_name}_{selected_university}",
                            help="ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã‹ã‚‰å…¨ã¦ã®å˜èªã‚’ã‚³ãƒ”ãƒ¼ã§ãã¾ã™"
                        )
                        
                        # çµ±è¨ˆæƒ…å ±
                        st.write(f"**çµ±è¨ˆ:** ç·èªæ•° {len(unmatched_words)}èª")
                else:
                    st.success("ğŸ‰ ã™ã¹ã¦ã®èªå½™ãŒã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã¾ã™ï¼")

def show_comparison_analysis(analysis_data: dict):
    """æ¯”è¼ƒåˆ†æã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    
    # ç°¡æ½”ãªæŒ‡æ¨™èª¬æ˜
    st.info("""
    ğŸ“Š **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡**: å˜èªå¸³ã®å®Ÿç”¨æ€§ï¼ˆé«˜ã„ã»ã©å…¥è©¦é »å‡ºèªã‚’å¤šãå«ã‚€ï¼‰ | **æŠ½å‡ºç²¾åº¦**: å­¦ç¿’åŠ¹ç‡æ€§ï¼ˆé«˜ã„ã»ã©å­¦ç¿’åŠ¹æœå¤§ï¼‰
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠã•ã‚ŒãŸå¤§å­¦ã‚’å–å¾—
    selected_universities = st.session_state.get('selected_universities', [])
    
    if not selected_universities:
        st.info("""
        ğŸ‘ˆ **å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦ãã ã•ã„**
        
        é¸æŠã—ãŸå¤§å­¦ãƒ»å­¦éƒ¨ã®æ¯”è¼ƒåˆ†æãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
        
        - 2ã¤ä»¥ä¸Šã®å¤§å­¦ã‚’é¸æŠã™ã‚‹ã¨æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™
        - 1ã¤ã®å ´åˆã¯å€‹åˆ¥åˆ†æã‚’è¡¨ç¤º
        """)
        return
    
    if len(selected_universities) < 2:
        st.warning("æ¯”è¼ƒåˆ†æã«ã¯2ã¤ä»¥ä¸Šã®å¤§å­¦ãŒå¿…è¦ã§ã™")
        return
    
    # å¤§å­¦é–“æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
    st.subheader("ğŸ“Š å¤§å­¦é–“æ¯”è¼ƒ")
    
    comparison_data = []
    university_analysis = analysis_data.get('university_analysis', {})
    
    for univ in selected_universities:
        univ_data = university_analysis.get(univ, {})
        vocab_coverage = univ_data.get('vocabulary_coverage', {})
        
        # æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã¨æœ€é©å˜èªå¸³ã‚’æ¢ã™
        best_coverage = 0
        best_vocab = "N/A"
        
        for vocab_name, stats in vocab_coverage.items():
            coverage = stats.get('target_coverage_rate', 0)
            if coverage > best_coverage:
                best_coverage = coverage
                best_vocab = vocab_name
        
        comparison_data.append({
            'å¤§å­¦ãƒ»å­¦éƒ¨': univ,
            'ç·å˜èªæ•°': univ_data.get('total_words', 0),
            'ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°': univ_data.get('unique_words', 0),
            'æœ€é©å˜èªå¸³': best_vocab,
            'æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': round(best_coverage, 1),
            'å‡¦ç†ãƒšãƒ¼ã‚¸': univ_data.get('pages_processed', 0)
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    
    # ã‚¹ã‚¿ã‚¤ãƒ«ä»˜ããƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    try:
        st.dataframe(
            comparison_df.style.format({
                'ç·å˜èªæ•°': '{:,}',
                'ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°': '{:,}',
                'æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}'
            }).background_gradient(subset=['æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)'], cmap='RdYlGn'),
            use_container_width=True
        )
    except ImportError:
        st.dataframe(
            comparison_df.style.format({
                'ç·å˜èªæ•°': '{:,}',
                'ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°': '{:,}',
                'æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}'
            }),
            use_container_width=True
        )
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤º
    st.subheader("ğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    
    ranking_criteria = st.selectbox(
        "ãƒ©ãƒ³ã‚­ãƒ³ã‚°åŸºæº–",
        ["æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)", "ç·å˜èªæ•°", "ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°"]
    )
    
    sorted_df = comparison_df.sort_values(ranking_criteria, ascending=False)
    
    for i, (_, row) in enumerate(sorted_df.iterrows(), 1):
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}ä½"
        
        criteria_value = row[ranking_criteria]
        if "(%" in ranking_criteria:
            criteria_display = f"{criteria_value}%"
        else:
            criteria_display = f"{criteria_value:,}"
        
        with st.container():
            st.markdown(f"""
            ### {medal} {row['å¤§å­¦ãƒ»å­¦éƒ¨']}
            - **{ranking_criteria}**: {criteria_display}
            - **æœ€é©å˜èªå¸³**: {row['æœ€é©å˜èªå¸³']}
            - **å‡¦ç†ãƒšãƒ¼ã‚¸**: {row['å‡¦ç†ãƒšãƒ¼ã‚¸']}
            """)

if __name__ == "__main__":
    main()