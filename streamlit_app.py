#!/usr/bin/env python3
"""
å¤§å­¦å…¥è©¦è‹±å˜èªåˆ†æ Streamlit ã‚¢ãƒ—ãƒª
JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»èªå½™åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import json
import nltk

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å¤§å­¦å…¥è©¦è‹±å˜èªåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #2E86AB;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color: #2E86AB;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
    st.markdown('<div class="main-header">ğŸ“š å¤§å­¦å…¥è©¦è‹±å˜èªåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰</div>', unsafe_allow_html=True)
    st.markdown("**JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦èªå½™åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„**")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒªã‚¢
    uploaded_files = st.file_uploader(
        "extraction_results_pure_english.json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯èƒ½ï¼‰",
        type=["json"],
        accept_multiple_files=True,
        help="OCRå‡¦ç†æ¸ˆã¿ã®è‹±èªæŠ½å‡ºçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã¦åˆ†æã—ã¾ã™ã€‚"
    )
    
    if uploaded_files:
        try:
            # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’çµ±åˆ
            combined_data = merge_multiple_json_files(uploaded_files)
            
            if combined_data:
                st.success(f"âœ… {len(uploaded_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§è¡¨ç¤º
                with st.expander("ğŸ“ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«", expanded=False):
                    for i, file in enumerate(uploaded_files, 1):
                        st.write(f"{i}. {file.name}")
                
                # åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³
                if st.button("ğŸ“Š èªå½™åˆ†æã‚’å®Ÿè¡Œ", type="primary"):
                    with st.spinner("èªå½™åˆ†æã‚’å®Ÿè¡Œä¸­..."):
                        analysis_data = perform_vocabulary_analysis(combined_data)
                        if analysis_data:
                            st.session_state.analysis_data = analysis_data
                            st.session_state.analysis_completed = True
                            st.success("âœ… èªå½™åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                        else:
                            st.error("âŒ èªå½™åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")
                
                # åˆ†æçµæœã®è¡¨ç¤º
                if st.session_state.get('analysis_completed', False) and 'analysis_data' in st.session_state:
                    st.markdown("---")
                    show_analysis_dashboard(st.session_state.analysis_data)
            else:
                st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
        except json.JSONDecodeError:
            st.error("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
        except Exception as e:
            st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    else:
        st.info("""
        ğŸ‘† **extraction_results_pure_english.json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„**
        
        ğŸ“‹ å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:
        - OCRå‡¦ç†æ¸ˆã¿ã®è‹±èªæŠ½å‡ºçµæœ
        - extracted_data ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å„å¤§å­¦ãƒ»å­¦éƒ¨ã®ãƒ‡ãƒ¼ã‚¿
        - pure_english_text ã¨ extracted_words ã‚’å«ã‚€
        """)

def merge_multiple_json_files(uploaded_files):
    """è¤‡æ•°ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ"""
    try:
        combined_data = {
            'extraction_summary': {
                'total_source_files': 0,
                'total_words_extracted': 0
            },
            'extracted_data': []
        }
        
        for uploaded_file in uploaded_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
            uploaded_file.seek(0)
            file_content = json.load(uploaded_file)
            
            # ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’çµ±åˆ
            file_summary = file_content.get('extraction_summary', {})
            combined_data['extraction_summary']['total_source_files'] += file_summary.get('total_source_files', 0)
            combined_data['extraction_summary']['total_words_extracted'] += file_summary.get('total_words_extracted', 0)
            
            # æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
            file_extracted_data = file_content.get('extracted_data', [])
            combined_data['extracted_data'].extend(file_extracted_data)
        
        return combined_data
        
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None

@st.cache_data
def load_vocabulary_books():
    """å˜èªå¸³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆçµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰"""
    try:
        from vocab_data import get_embedded_vocabulary_data
        
        # çµ„ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        embedded_data = get_embedded_vocabulary_data()
        
        # å°æ–‡å­—åŒ–ã—ã¦æ­£è¦åŒ–
        vocab_data = {}
        for name, word_set in embedded_data.items():
            vocab_data[name] = {word.lower().strip() for word in word_set if word and len(word) > 1}
        
        return vocab_data
    except Exception as e:
        st.error(f"å˜èªå¸³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}")
        return {}

def setup_analysis_sidebar(analysis_data):
    """åˆ†æç”¨ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š"""
    st.sidebar.title("ğŸ“Š åˆ†æè¨­å®š")
    
    # åˆ†æå¯¾è±¡é¸æŠ
    st.sidebar.subheader("ğŸ« åˆ†æå¯¾è±¡")
    available_universities = list(analysis_data.get('university_analysis', {}).keys())
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
    if len(available_universities) == 0:
        st.sidebar.error("âŒ å¤§å­¦ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.sidebar.write("**ãƒ‡ãƒãƒƒã‚°æƒ…å ±:**")
        st.sidebar.write(f"analysis_data keys: {list(analysis_data.keys())}")
        st.sidebar.write(f"university_analysis keys: {list(analysis_data.get('university_analysis', {}).keys())}")
    else:
        st.sidebar.success(f"âœ… {len(available_universities)}å¤§å­¦ãƒ»å­¦éƒ¨ã‚’æ¤œå‡º")
    
    selected_universities = st.sidebar.multiselect(
        "å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠ",
        available_universities,
        default=available_universities[:3] if len(available_universities) >= 3 else available_universities,
        help="æ¯”è¼ƒåˆ†æã™ã‚‹å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦ãã ã•ã„"
    )
    
    st.session_state.selected_universities = selected_universities
    
    st.sidebar.markdown("---")
    
    # æŒ‡æ¨™èª¬æ˜
    st.sidebar.subheader("ğŸ’¡ æŒ‡æ¨™ã®æ„å‘³")
    st.sidebar.markdown("""
    **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡**: å˜èªå¸³ã®ä½•%ãŒå…¥è©¦ã«å‡ºç¾
    
    **æŠ½å‡ºç²¾åº¦**: å­¦ç¿’èªå½™ã®ä½•%ãŒå…¥è©¦ã«å‡ºç¾
    
    **ä¸€è‡´èªæ•°**: å®Ÿéš›ã«ä¸€è‡´ã—ãŸèªæ•°
    """)
    
    # ãƒ‡ãƒ¼ã‚¿æƒ…å ±
    st.sidebar.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿æƒ…å ±")
    overall_summary = analysis_data.get('overall_summary', {})
    st.sidebar.write(f"**ç·å¤§å­¦æ•°**: {len(available_universities)}")
    st.sidebar.write(f"**å˜èªå¸³æ•°**: 5ç¨®é¡")
    st.sidebar.write(f"**ç·å˜èªæ•°**: {overall_summary.get('total_words_extracted', 0):,}")

def perform_vocabulary_analysis(extraction_data):
    """JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èªå½™åˆ†æã‚’å®Ÿè¡Œ"""
    try:
        st.write("ğŸ”„ åˆ†æé–‹å§‹...")
        
        # NLTK ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('wordnet', quiet=True)
            nltk.download('averaged_perceptron_tagger', quiet=True)
        except Exception as e:
            st.warning(f"NLTK ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è­¦å‘Š: {str(e)}")
        
        # å˜èªå¸³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        st.write("ğŸ“š å˜èªå¸³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        vocab_books = load_vocabulary_books()
        if not vocab_books:
            st.error("å˜èªå¸³ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
        st.success(f"âœ… {len(vocab_books)}å€‹ã®å˜èªå¸³ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
        st.write("ğŸ“‹ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ä¸­...")
        if not extraction_data:
            st.error("âŒ extraction_data ãŒç©ºã§ã™")
            return None
        
        st.write(f"extraction_data keys: {list(extraction_data.keys())}")
        extracted_data_list = extraction_data.get('extracted_data', [])
        if not extracted_data_list:
            st.error("âŒ extracted_data ãƒªã‚¹ãƒˆãŒç©ºã§ã™")
            return None
        
        st.success(f"âœ… {len(extracted_data_list)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒˆãƒªã‚’ç¢ºèª")
        
        # åˆ†æçµæœã®åˆæœŸåŒ–
        analysis_result = {
            'overall_summary': {
                'total_source_files': extraction_data.get('extraction_summary', {}).get('total_source_files', 0),
                'total_words_extracted': extraction_data.get('extraction_summary', {}).get('total_words_extracted', 0),
                'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            'vocabulary_summary': {},
            'university_analysis': {}
        }
        
        # å„å¤§å­¦ãƒ»å­¦éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
        st.write("ğŸ« å¤§å­¦ãƒ»å­¦éƒ¨ãƒ‡ãƒ¼ã‚¿ã®åˆ†æé–‹å§‹...")
        
        for i, entry in enumerate(extracted_data_list):
            try:
                st.write(f"--- ã‚¨ãƒ³ãƒˆãƒª {i+1}/{len(extracted_data_list)} ã®å‡¦ç† ---")
                
                source_file = entry.get('source_file', '')
                if not source_file:
                    st.warning(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: source_file ãŒç©ºã§ã™")
                    continue
                
                university_name = extract_university_name_from_filename(source_file)
                if not university_name or university_name == "ä¸æ˜ãªå¤§å­¦":
                    st.warning(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: å¤§å­¦åæŠ½å‡ºã«å¤±æ•— - '{source_file}'")
                    continue
                
                # æŠ½å‡ºã•ã‚ŒãŸå˜èªã‚’æ­£è¦åŒ–
                extracted_words = entry.get('extracted_words', [])
                if not extracted_words:
                    st.warning(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒª {i+1}: extracted_words ãŒç©ºã§ã™")
                    continue
                
                normalized_words = [word.lower().strip() for word in extracted_words if word and len(word) > 1]
                unique_words = list(set(normalized_words))
                
                st.write(f"âœ… {university_name}: {len(extracted_words)}èª â†’ {len(unique_words)}ãƒ¦ãƒ‹ãƒ¼ã‚¯èª")
                
                # å„å˜èªå¸³ã¨ã®æ¯”è¼ƒåˆ†æ
                vocab_coverage = {}
                for vocab_name, vocab_set in vocab_books.items():
                matched_words = [word for word in unique_words if word in vocab_set]
                matched_count = len(matched_words)
                
                target_coverage_rate = (matched_count / len(vocab_set)) * 100 if vocab_set else 0
                extraction_precision = (matched_count / len(unique_words)) * 100 if unique_words else 0
                
                vocab_coverage[vocab_name] = {
                    'matched_words_count': matched_count,
                    'target_coverage_rate': target_coverage_rate,
                    'extraction_precision': extraction_precision,
                    'matched_words': matched_words[:20]  # æœ€åˆã®20èªã®ã¿ä¿å­˜
                }
            
                # å¤§å­¦ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                try:
                    analysis_result['university_analysis'][university_name] = {
                        'source_file': source_file,
                        'total_words': len(extracted_words),
                        'unique_words': len(unique_words),
                        'vocabulary_coverage': vocab_coverage,
                        'pages_processed': entry.get('pages_processed', 0)
                    }
                    st.success(f"âœ… {university_name} ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
                    st.write(f"ç¾åœ¨ã®å¤§å­¦æ•°: {len(analysis_result['university_analysis'])}")
                except Exception as save_error:
                    st.error(f"âŒ {university_name} ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(save_error)}")
                    continue
                    
            except Exception as entry_error:
                st.error(f"âŒ ã‚¨ãƒ³ãƒˆãƒª {i+1} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(entry_error)}")
                continue
        
        # å…¨ä½“ã‚µãƒãƒªãƒ¼ã®è¨ˆç®—
        all_coverage_data = {vocab_name: [] for vocab_name in vocab_books.keys()}
        for univ_data in analysis_result['university_analysis'].values():
            for vocab_name in vocab_books.keys():
                coverage = univ_data['vocabulary_coverage'][vocab_name]
                all_coverage_data[vocab_name].append({
                    'coverage_rate': coverage['target_coverage_rate'],
                    'precision': coverage['extraction_precision'],
                    'matched_count': coverage['matched_words_count']
                })
        
        # èªå½™ã‚µãƒãƒªãƒ¼ã®è¨ˆç®—
        for vocab_name, coverage_list in all_coverage_data.items():
            if coverage_list:
                avg_coverage = sum(item['coverage_rate'] for item in coverage_list) / len(coverage_list)
                avg_precision = sum(item['precision'] for item in coverage_list) / len(coverage_list)
                total_matched = sum(item['matched_count'] for item in coverage_list)
                
                analysis_result['vocabulary_summary'][vocab_name] = {
                    'average_coverage_rate': avg_coverage,
                    'average_extraction_precision': avg_precision,
                    'total_matched_words': total_matched
                }
        
        # æœ€çµ‚çµæœã®ç¢ºèª
        st.success(f"âœ… åˆ†æå®Œäº†: {len(analysis_result['university_analysis'])}å¤§å­¦ãƒ»å­¦éƒ¨")
        st.write("**æœ€çµ‚çµæœ:**")
        st.write(f"  - university_analysis keys: {list(analysis_result['university_analysis'].keys())}")
        
        return analysis_result
        
    except Exception as e:
        st.error(f"èªå½™åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None

def extract_university_name_from_filename(filename):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨åã‚’æŠ½å‡º"""
    if not filename:
        return "ä¸æ˜ãªå¤§å­¦"
    
    # PDFãƒ•ã‚¡ã‚¤ãƒ«åã®ä¾‹: "æ…¶æ‡‰ç¾©å¡¾å¤§å­¦_2024å¹´åº¦_è‹±èª_è–¬å­¦éƒ¨.pdf"
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ‘ã‚¹éƒ¨åˆ†ã‚’å–ã‚Šé™¤ã
    basename = filename.split('/')[-1] if '/' in filename else filename
    parts = basename.replace('.pdf', '').split('_')
    
    if len(parts) >= 4:
        university = parts[0]
        department = parts[3]
        return f"{university}_{department}"
    elif len(parts) >= 1:
        return parts[0]
    
    return basename.replace('.pdf', '')

def show_analysis_dashboard(analysis_data):
    """åˆ†æçµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®è¡¨ç¤º"""
    if not analysis_data:
        st.error("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    setup_analysis_sidebar(analysis_data)
    
    # ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–ã®ä½œæˆ
    tab1, tab2, tab3 = st.tabs(["ğŸ  æ¦‚è¦åˆ†æ", "ğŸ« å¤§å­¦åˆ¥è©³ç´°", "ğŸ“Š æ¯”è¼ƒåˆ†æ"])
    
    with tab1:
        show_overview_analysis(analysis_data)
    
    with tab2:
        show_university_analysis(analysis_data)
    
    with tab3:
        show_comparison_analysis(analysis_data)

def show_overview_analysis(analysis_data: dict):
    """æ¦‚è¦åˆ†æã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    
    # ç°¡æ½”ãªå®šç¾©ï¼ˆå¸¸æ™‚è¡¨ç¤ºï¼‰
    col1, col2 = st.columns(2)
    with col1:
        st.info("""
        **ğŸ“ˆ ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã¨ã¯ï¼Ÿ**  
        å˜èªå¸³ã®ä½•%ã®èªå½™ãŒå®Ÿéš›ã®å…¥è©¦å•é¡Œã«å‡ºç¾ã—ãŸã‹ã‚’ç¤ºã™æŒ‡æ¨™ã€‚é«˜ã„ã»ã©å®Ÿç”¨çš„ã€‚
        """)
    
    with col2:
        st.info("""
        **ğŸ¯ æŠ½å‡ºç²¾åº¦ã¨ã¯ï¼Ÿ**  
        æŠ½å‡ºã—ãŸå˜èªã®ã†ã¡ã€å˜èªå¸³ã«å«ã¾ã‚Œã‚‹å‰²åˆã€‚é«˜ã„ã»ã©å­¦ç¿’åŠ¹ç‡ãŒè‰¯ã„ã€‚
        """)
    
    st.markdown("---")
    
    # å¤§å­¦é¸æŠçŠ¶æ³ã‚’ç¢ºèª
    selected_universities = st.session_state.get('selected_universities', [])
    
    if not selected_universities:
        # å¤§å­¦ãŒé¸æŠã•ã‚Œã¦ã„ãªã„å ´åˆ
        st.info("""
        ğŸ‘ˆ **å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦ãã ã•ã„**
        
        é¸æŠã—ãŸå¤§å­¦ãƒ»å­¦éƒ¨ã®èªå½™åˆ†æçµæœã¨ãƒãƒ£ãƒ¼ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
        
        - è¤‡æ•°ã®å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦æ¯”è¼ƒåˆ†æã‚‚å¯èƒ½ã§ã™
        - ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã®é–¾å€¤è¨­å®šã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚‚ã§ãã¾ã™
        """)
        return
    
    # é¸æŠã•ã‚ŒãŸå¤§å­¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    university_analysis = analysis_data.get('university_analysis', {})
    vocabulary_summary = analysis_data.get('vocabulary_summary', {})
    
    # é¸æŠã•ã‚ŒãŸå¤§å­¦ã®çµ±è¨ˆ
    selected_total_words = sum([info.get('total_words', 0) for univ, info in university_analysis.items() if univ in selected_universities])
    selected_total_pages = sum([info.get('pages_processed', 0) for univ, info in university_analysis.items() if univ in selected_universities])
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="é¸æŠå¤§å­¦ç·å˜èªæ•°",
            value=f"{selected_total_words:,}",
            delta=f"{len(selected_universities)}å¤§å­¦ãƒ»å­¦éƒ¨"
        )
    
    with col2:
        # æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã‚’è¨ˆç®—
        best_coverage = 0
        best_vocab = "N/A"
        for vocab_name, summary in vocabulary_summary.items():
            if summary.get('average_coverage_rate', 0) > best_coverage:
                best_coverage = summary.get('average_coverage_rate', 0)
                best_vocab = vocab_name
        
        st.metric(
            label="æœ€é©å˜èªå¸³",
            value=best_vocab,
            delta=f"{best_coverage:.1f}%"
        )
    
    with col3:
        st.metric(
            label="å‡¦ç†ãƒšãƒ¼ã‚¸æ•°",
            value=f"{selected_total_pages}",
            delta=None
        )
    
    st.markdown("---")
    
    # å˜èªå¸³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ£ãƒ¼ãƒˆ
    if vocabulary_summary:
        st.subheader("ğŸ“š å˜èªå¸³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        vocab_data = []
        for vocab_name, summary in vocabulary_summary.items():
            vocab_data.append({
                'å˜èªå¸³': vocab_name,
                'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': summary.get('average_coverage_rate', 0),
                'æŠ½å‡ºç²¾åº¦(%)': summary.get('average_extraction_precision', 0),
                'ä¸€è‡´èªæ•°': summary.get('total_matched_words', 0)
            })
        
        vocab_df = pd.DataFrame(vocab_data)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡æ£’ã‚°ãƒ©ãƒ•
            fig_coverage = px.bar(
                vocab_df, 
                x='å˜èªå¸³', 
                y='ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                title='å˜èªå¸³åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡',
                color='ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                color_continuous_scale='RdYlGn'
            )
            fig_coverage.update_layout(height=400)
            st.plotly_chart(fig_coverage, use_container_width=True)
        
        with col2:
            # æŠ½å‡ºç²¾åº¦æ£’ã‚°ãƒ©ãƒ•
            fig_precision = px.bar(
                vocab_df, 
                x='å˜èªå¸³', 
                y='æŠ½å‡ºç²¾åº¦(%)',
                title='å˜èªå¸³åˆ¥æŠ½å‡ºç²¾åº¦',
                color='æŠ½å‡ºç²¾åº¦(%)',
                color_continuous_scale='RdYlBu'
            )
            fig_precision.update_layout(height=400)
            st.plotly_chart(fig_precision, use_container_width=True)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        st.subheader("ğŸ“Š å˜èªå¸³ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«")
        try:
            st.dataframe(
                vocab_df.style.format({
                    'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}',
                    'æŠ½å‡ºç²¾åº¦(%)': '{:.1f}',
                    'ä¸€è‡´èªæ•°': '{:,}'
                }).background_gradient(subset=['ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)', 'æŠ½å‡ºç²¾åº¦(%)'], cmap='RdYlGn'),
                use_container_width=True
            )
        except ImportError:
            st.dataframe(
                vocab_df.style.format({
                    'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}',
                    'æŠ½å‡ºç²¾åº¦(%)': '{:.1f}',
                    'ä¸€è‡´èªæ•°': '{:,}'
                }),
                use_container_width=True
            )

def show_university_analysis(analysis_data: dict):
    """å¤§å­¦åˆ¥è©³ç´°ã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    
    # ç°¡æ½”ãªæŒ‡æ¨™èª¬æ˜
    st.info("""
    ğŸ’¡ **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡**: å˜èªå¸³ã®ä½•%ãŒå…¥è©¦ã«å‡ºç¾ | **æŠ½å‡ºç²¾åº¦**: å­¦ç¿’ã—ãŸèªå½™ã®ä½•%ãŒå…¥è©¦ã«å‡ºç¾ | è©³ç´°ã¯æ¦‚è¦ã‚¿ãƒ–ã§ç¢ºèª
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠã•ã‚ŒãŸå¤§å­¦ã‚’å–å¾—
    selected_universities = st.session_state.get('selected_universities', [])
    
    if not selected_universities:
        st.info("""
        ğŸ‘ˆ **å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦ãã ã•ã„**
        
        é¸æŠã—ãŸå¤§å­¦ãƒ»å­¦éƒ¨ã®è©³ç´°åˆ†æãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
        
        - 1ã¤ã®å¤§å­¦ã‚’é¸æŠã™ã‚‹ã¨ã€ãã®å¤§å­¦ã®è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º
        - è¤‡æ•°é¸æŠæ™‚ã¯æœ€åˆã®å¤§å­¦ã®è©³ç´°ã‚’è¡¨ç¤º
        """)
        return
    
    # æœ€åˆã«é¸æŠã•ã‚ŒãŸå¤§å­¦ã®è©³ç´°ã‚’è¡¨ç¤º
    selected_university = selected_universities[0]
    if len(selected_universities) > 1:
        st.info(f"è¤‡æ•°é¸æŠã•ã‚Œã¦ã„ã¾ã™ã€‚{selected_university} ã®è©³ç´°ã‚’è¡¨ç¤ºä¸­ï¼ˆä»– {len(selected_universities)-1} å¤§å­¦ï¼‰")
    
    university_data = analysis_data.get('university_analysis', {}).get(selected_university, {})
    university_meta = {}  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ç¾åœ¨åˆ©ç”¨ä¸å¯
    
    # å¤§å­¦æƒ…å ±ã‚«ãƒ¼ãƒ‰
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.info(f"""
        **ğŸ« {selected_university}**
        - ãƒ•ã‚¡ã‚¤ãƒ«: {university_data.get('source_file', 'N/A').split('/')[-1] if university_data.get('source_file') else 'N/A'}
        """)
    
    with col2:
        st.info(f"""
        **ğŸ“Š å‡¦ç†çµ±è¨ˆ**
        - ç·å˜èªæ•°: {university_data.get('total_words', 0):,}
        - ãƒ¦ãƒ‹ãƒ¼ã‚¯èªæ•°: {university_data.get('unique_words', 0):,}
        - å‡¦ç†ãƒšãƒ¼ã‚¸: {university_data.get('pages_processed', 0)}
        """)
    
    with col3:
        st.info(f"""
        **ğŸ“š èªå½™åˆ†æ**
        - å˜èªå¸³æ•°: {len(university_data.get('vocabulary_coverage', {}))}
        """)
    
    st.markdown("---")
    
    # å˜èªå¸³åˆ¥è©³ç´°
    st.subheader("ğŸ“š å˜èªå¸³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹")
    
    vocab_coverage = university_data.get('vocabulary_coverage', {})
    vocab_df_data = []
    
    for vocab_name, vocab_stats in vocab_coverage.items():
        vocab_df_data.append({
            'å˜èªå¸³': vocab_name,
            'ä¸€è‡´èªæ•°': vocab_stats.get('matched_words_count', 0),
            'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': round(vocab_stats.get('target_coverage_rate', 0), 1),
            'æŠ½å‡ºç²¾åº¦(%)': round(vocab_stats.get('extraction_precision', 0), 1)
        })
    
    vocab_df = pd.DataFrame(vocab_df_data)
    
    # ã‚¹ã‚¿ã‚¤ãƒ«ä»˜ããƒ†ãƒ¼ãƒ–ãƒ«
    try:
        st.dataframe(
            vocab_df.style.format({
                'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}',
                'æŠ½å‡ºç²¾åº¦(%)': '{:.1f}'
            }).background_gradient(subset=['ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)', 'æŠ½å‡ºç²¾åº¦(%)'], cmap='RdYlGn'),
            use_container_width=True
        )
    except ImportError:
        # matplotlibãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        st.dataframe(
            vocab_df.style.format({
                'ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}',
                'æŠ½å‡ºç²¾åº¦(%)': '{:.1f}'
            }),
            use_container_width=True
        )
    
    # å˜èªå¸³æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ
    if len(vocab_df_data) > 1:
        col1, col2 = st.columns(2)
        
        with col1:
            fig_coverage = px.bar(
                vocab_df, 
                x='å˜èªå¸³', 
                y='ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                title='å˜èªå¸³åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡',
                color='ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)',
                color_continuous_scale='RdYlGn'
            )
            fig_coverage.update_layout(height=400)
            st.plotly_chart(fig_coverage, use_container_width=True)
        
        with col2:
            fig_precision = px.bar(
                vocab_df, 
                x='å˜èªå¸³', 
                y='æŠ½å‡ºç²¾åº¦(%)',
                title='å˜èªå¸³åˆ¥æŠ½å‡ºç²¾åº¦',
                color='æŠ½å‡ºç²¾åº¦(%)',
                color_continuous_scale='RdYlBu'
            )
            fig_precision.update_layout(height=400)
            st.plotly_chart(fig_precision, use_container_width=True)

def show_comparison_analysis(analysis_data: dict):
    """æ¯”è¼ƒåˆ†æã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    
    # ç°¡æ½”ãªæŒ‡æ¨™èª¬æ˜
    st.info("""
    ğŸ“Š **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡**: å˜èªå¸³ã®å®Ÿç”¨æ€§ï¼ˆé«˜ã„ã»ã©å…¥è©¦é »å‡ºèªã‚’å¤šãå«ã‚€ï¼‰ | **æŠ½å‡ºç²¾åº¦**: å­¦ç¿’åŠ¹ç‡æ€§ï¼ˆé«˜ã„ã»ã©å­¦ç¿’åŠ¹æœå¤§ï¼‰
    """)
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠã•ã‚ŒãŸå¤§å­¦ã‚’å–å¾—
    selected_universities = st.session_state.get('selected_universities', [])
    
    if not selected_universities:
        st.info("""
        ğŸ‘ˆ **å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰å¤§å­¦ãƒ»å­¦éƒ¨ã‚’é¸æŠã—ã¦ãã ã•ã„**
        
        é¸æŠã—ãŸå¤§å­¦ãƒ»å­¦éƒ¨ã®æ¯”è¼ƒåˆ†æãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
        
        - 2ã¤ä»¥ä¸Šã®å¤§å­¦ã‚’é¸æŠã™ã‚‹ã¨æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆãŒè¡¨ç¤ºã•ã‚Œã¾ã™
        - 1ã¤ã®å ´åˆã¯å€‹åˆ¥åˆ†æã‚’è¡¨ç¤º
        """)
        return
    
    if len(selected_universities) < 2:
        st.warning("æ¯”è¼ƒåˆ†æã«ã¯2ã¤ä»¥ä¸Šã®å¤§å­¦ãŒå¿…è¦ã§ã™")
        return
    
    # å¤§å­¦é–“æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
    st.subheader("ğŸ“Š å¤§å­¦é–“æ¯”è¼ƒ")
    
    comparison_data = []
    university_analysis = analysis_data.get('university_analysis', {})
    
    for univ in selected_universities:
        univ_data = university_analysis.get(univ, {})
        vocab_coverage = univ_data.get('vocabulary_coverage', {})
        
        # æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã¨æœ€é©å˜èªå¸³ã‚’æ¢ã™
        best_coverage = 0
        best_vocab = "N/A"
        
        for vocab_name, stats in vocab_coverage.items():
            coverage = stats.get('target_coverage_rate', 0)
            if coverage > best_coverage:
                best_coverage = coverage
                best_vocab = vocab_name
        
        comparison_data.append({
            'å¤§å­¦ãƒ»å­¦éƒ¨': univ,
            'ç·å˜èªæ•°': univ_data.get('total_words', 0),
            'ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°': univ_data.get('unique_words', 0),
            'æœ€é©å˜èªå¸³': best_vocab,
            'æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': round(best_coverage, 1),
            'å‡¦ç†ãƒšãƒ¼ã‚¸': univ_data.get('pages_processed', 0)
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    
    # ã‚¹ã‚¿ã‚¤ãƒ«ä»˜ããƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    try:
        st.dataframe(
            comparison_df.style.format({
                'ç·å˜èªæ•°': '{:,}',
                'ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°': '{:,}',
                'æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}'
            }).background_gradient(subset=['æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)'], cmap='RdYlGn'),
            use_container_width=True
        )
    except ImportError:
        st.dataframe(
            comparison_df.style.format({
                'ç·å˜èªæ•°': '{:,}',
                'ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°': '{:,}',
                'æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)': '{:.1f}'
            }),
            use_container_width=True
        )
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤º
    st.subheader("ğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    
    ranking_criteria = st.selectbox(
        "ãƒ©ãƒ³ã‚­ãƒ³ã‚°åŸºæº–",
        ["æœ€é«˜ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡(%)", "ç·å˜èªæ•°", "ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°"]
    )
    
    sorted_df = comparison_df.sort_values(ranking_criteria, ascending=False)
    
    for i, (_, row) in enumerate(sorted_df.iterrows(), 1):
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}ä½"
        
        criteria_value = row[ranking_criteria]
        if "(%" in ranking_criteria:
            criteria_display = f"{criteria_value}%"
        else:
            criteria_display = f"{criteria_value:,}"
        
        with st.container():
            st.markdown(f"""
            ### {medal} {row['å¤§å­¦ãƒ»å­¦éƒ¨']}
            - **{ranking_criteria}**: {criteria_display}
            - **æœ€é©å˜èªå¸³**: {row['æœ€é©å˜èªå¸³']}
            - **å‡¦ç†ãƒšãƒ¼ã‚¸**: {row['å‡¦ç†ãƒšãƒ¼ã‚¸']}
            """)

if __name__ == "__main__":
    main()