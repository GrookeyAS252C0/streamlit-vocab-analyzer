#!/usr/bin/env python3
"""
Target 1900å˜èªå¸³ã¨ã®ä¸€è‡´ç‡åˆ†æãƒ„ãƒ¼ãƒ«
extraction_results_pure_english.jsonã‹ã‚‰æŠ½å‡ºã—ãŸå˜èªã¨Target 1900ã®æ¯”è¼ƒåˆ†æ
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import defaultdict, Counter
import logging

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import pandas as pd

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VocabularyAnalyzer:
    def __init__(self):
        """
        Target 1900ã¨ã®ä¸€è‡´ç‡åˆ†æã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
        """
        # NLTK ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        self._download_nltk_data()
        
        # LemmatizeråˆæœŸåŒ–
        self.lemmatizer = WordNetLemmatizer()
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
        self.stop_words = set(stopwords.words('english'))
        
    def _download_nltk_data(self):
        """å¿…è¦ãªNLTKãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        required_data = ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger']
        
        for data_name in required_data:
            try:
                nltk.data.find(f'tokenizers/{data_name}' if data_name == 'punkt' 
                              else f'corpora/{data_name}' if data_name in ['stopwords', 'wordnet'] 
                              else f'taggers/{data_name}')
            except LookupError:
                logger.info(f"Downloading NLTK data: {data_name}")
                nltk.download(data_name)
    
    def load_target1900(self, file_path: str) -> Set[str]:
        """
        Target 1900å˜èªãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
        
        Args:
            file_path: Target 1900ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            æ­£è¦åŒ–æ¸ˆã¿Target 1900å˜èªã®ã‚»ãƒƒãƒˆ
        """
        target_words = set()
        
        try:
            # CSVå½¢å¼ã‹TXTå½¢å¼ã‹ã‚’åˆ¤å®š
            if file_path.endswith('.csv'):
                # CSVå½¢å¼ã®å ´åˆï¼ˆBOMå¯¾å¿œï¼‰
                df = pd.read_csv(file_path, encoding='utf-8-sig')
                # 'word'åˆ—ã‹ã‚‰å˜èªã‚’å–å¾—
                if 'word' in df.columns:
                    for word in df['word'].dropna():
                        word = str(word).strip().lower()
                        if word:
                            # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                            cleaned_word = re.sub(r'[^\w]', '', word.lower())
                            if len(cleaned_word) >= 2 and not cleaned_word.isdigit():
                                # lemmatizationï¼ˆåŸå½¢åŒ–ï¼‰
                                lemmatized = self.lemmatizer.lemmatize(cleaned_word, pos='v')  # å‹•è©ã¨ã—ã¦
                                lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')    # åè©ã¨ã—ã¦
                                target_words.add(lemmatized)
                else:
                    logger.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«'word'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    raise ValueError("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«'word'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            else:
                # TXTå½¢å¼ã®å ´åˆï¼ˆå¾“æ¥ã®å‡¦ç†ï¼‰
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        word = line.strip().lower()
                        if word and not word.startswith('#'):  # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’é™¤å¤–
                            normalized_word = self._normalize_word(word)
                            if normalized_word:
                                target_words.add(normalized_word)
            
            logger.info(f"Target 1900å˜èªæ•°: {len(target_words)}")
            return target_words
            
        except FileNotFoundError:
            logger.error(f"Target 1900ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            raise
        except Exception as e:
            logger.error(f"Target 1900èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def load_extracted_data(self, file_path: str) -> Tuple[List[str], Dict]:
        """
        extraction_results_pure_english.jsonã‹ã‚‰æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Args:
            file_path: æŠ½å‡ºçµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            (æ­£è¦åŒ–æ¸ˆã¿æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆ, å…ƒãƒ‡ãƒ¼ã‚¿)
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å…¨ã¦ã®æŠ½å‡ºå˜èªã‚’åé›†
            all_extracted_words = []
            for item in data.get('extracted_data', []):
                words = item.get('extracted_words', [])
                all_extracted_words.extend(words)
            
            # æ­£è¦åŒ–ï¼ˆlemmatizationä»˜ãï¼‰
            normalized_words = []
            for word in all_extracted_words:
                # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
                cleaned_word = re.sub(r'[^\w]', '', word.lower())
                if len(cleaned_word) >= 2 and not cleaned_word.isdigit():
                    # lemmatizationï¼ˆåŸå½¢åŒ–ï¼‰
                    lemmatized = self.lemmatizer.lemmatize(cleaned_word, pos='v')  # å‹•è©ã¨ã—ã¦
                    lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')    # åè©ã¨ã—ã¦
                    normalized_words.append(lemmatized)
            
            logger.info(f"æŠ½å‡ºã•ã‚ŒãŸç·å˜èªæ•°: {len(all_extracted_words)}")
            logger.info(f"æ­£è¦åŒ–å¾Œãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {len(set(normalized_words))}")
            
            return normalized_words, data
            
        except FileNotFoundError:
            logger.error(f"æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            raise
        except Exception as e:
            logger.error(f"æŠ½å‡ºçµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _normalize_word(self, word: str) -> str:
        """
        å˜èªã®æ­£è¦åŒ–å‡¦ç†
        
        Args:
            word: æ­£è¦åŒ–ã™ã‚‹å˜èª
            
        Returns:
            æ­£è¦åŒ–æ¸ˆã¿å˜èªï¼ˆç©ºæ–‡å­—åˆ—ã®å ´åˆã¯é™¤å¤–å¯¾è±¡ï¼‰
        """
        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        word = re.sub(r'[^\w]', '', word.lower())
        
        # çŸ­ã™ãã‚‹å˜èªã‚„æ•°å­—ã®ã¿ã®å˜èªã‚’é™¤å¤–
        if len(word) < 2 or word.isdigit():
            return ""
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
        if word in self.stop_words:
            return ""
        
        # ãƒ¬ãƒ³ãƒåŒ–ï¼ˆåŸå½¢åŒ–ï¼‰
        try:
            lemmatized = self.lemmatizer.lemmatize(word, pos='v')  # å‹•è©ã¨ã—ã¦
            lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')  # åè©ã¨ã—ã¦
            return lemmatized
        except:
            return word
    
    def calculate_coverage_stats(self, target_words: Set[str], extracted_words: List[str]) -> Dict:
        """
        ä¸€è‡´ç‡çµ±è¨ˆã®è¨ˆç®—
        
        Args:
            target_words: Target 1900å˜èªã‚»ãƒƒãƒˆ
            extracted_words: æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆ
            
        Returns:
            çµ±è¨ˆæƒ…å ±è¾æ›¸
        """
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯æŠ½å‡ºå˜èª
        unique_extracted = set(extracted_words)
        
        # ä¸€è‡´å˜èª
        matched_words = target_words.intersection(unique_extracted)
        
        # çµ±è¨ˆè¨ˆç®—
        target_coverage_rate = len(matched_words) / len(target_words) * 100
        extraction_precision = len(matched_words) / len(unique_extracted) * 100 if unique_extracted else 0
        
        # é »åº¦åˆ†æ
        word_frequencies = Counter(extracted_words)
        matched_frequencies = {word: word_frequencies[word] for word in matched_words}
        
        stats = {
            'target_total_words': len(target_words),
            'extracted_total_words': len(extracted_words),
            'extracted_unique_words': len(unique_extracted),
            'matched_words_count': len(matched_words),
            'target_coverage_rate': round(target_coverage_rate, 2),
            'extraction_precision': round(extraction_precision, 2),
            'matched_words': sorted(list(matched_words)),
            'unmatched_from_target': sorted(list(target_words - matched_words)),
            'unmatched_from_extracted': sorted(list(unique_extracted - target_words)),
            'word_frequencies': dict(word_frequencies.most_common(50)),
            'matched_word_frequencies': dict(sorted(matched_frequencies.items(), 
                                                   key=lambda x: x[1], reverse=True)[:30])
        }
        
        return stats
    
    def analyze_by_frequency_tiers(self, target_words: Set[str], extracted_words: List[str]) -> Dict:
        """
        é »åº¦å±¤åˆ¥ã®åˆ†æ
        
        Args:
            target_words: Target 1900å˜èªã‚»ãƒƒãƒˆ
            extracted_words: æŠ½å‡ºå˜èªãƒªã‚¹ãƒˆ
            
        Returns:
            é »åº¦å±¤åˆ¥çµ±è¨ˆ
        """
        word_frequencies = Counter(extracted_words)
        unique_extracted = set(extracted_words)
        matched_words = target_words.intersection(unique_extracted)
        
        # é »åº¦ã«ã‚ˆã‚‹å±¤åˆ†ã‘
        frequency_tiers = {
            'high_frequency': [],    # 10å›ä»¥ä¸Š
            'medium_frequency': [],  # 3-9å›
            'low_frequency': []      # 1-2å›
        }
        
        for word in unique_extracted:
            freq = word_frequencies[word]
            if freq >= 10:
                frequency_tiers['high_frequency'].append(word)
            elif freq >= 3:
                frequency_tiers['medium_frequency'].append(word)
            else:
                frequency_tiers['low_frequency'].append(word)
        
        # å„å±¤ã§ã®ä¸€è‡´ç‡è¨ˆç®—
        tier_analysis = {}
        for tier_name, tier_words in frequency_tiers.items():
            tier_set = set(tier_words)
            tier_matched = target_words.intersection(tier_set)
            
            tier_analysis[tier_name] = {
                'total_words': len(tier_words),
                'matched_words': len(tier_matched),
                'coverage_rate': round(len(tier_matched) / len(tier_words) * 100, 2) if tier_words else 0,
                'matched_word_list': sorted(list(tier_matched))
            }
        
        return tier_analysis
    
    def analyze_by_university(self, target_words: Set[str], original_data: Dict) -> Dict:
        """
        å¤§å­¦åˆ¥ã®èªå½™åˆ†æ
        
        Args:
            target_words: Target 1900å˜èªã‚»ãƒƒãƒˆ
            original_data: æŠ½å‡ºçµæœã®å…ƒãƒ‡ãƒ¼ã‚¿
            
        Returns:
            å¤§å­¦åˆ¥åˆ†æçµæœ
        """
        university_analysis = {}
        
        for item in original_data.get('extracted_data', []):
            source_file = item.get('source_file', '')
            words = item.get('extracted_words', [])
            
            # å¤§å­¦åã‚’æŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ï¼‰
            university_name = self._extract_university_name(source_file)
            
            # å˜èªã‚’æ­£è¦åŒ–ï¼ˆlemmatizationä»˜ãï¼‰
            normalized_words = []
            for word in words:
                cleaned_word = re.sub(r'[^\w]', '', word.lower())
                if len(cleaned_word) >= 2 and not cleaned_word.isdigit():
                    # lemmatizationï¼ˆåŸå½¢åŒ–ï¼‰
                    lemmatized = self.lemmatizer.lemmatize(cleaned_word, pos='v')  # å‹•è©ã¨ã—ã¦
                    lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')    # åè©ã¨ã—ã¦
                    normalized_words.append(lemmatized)
            
            # ä¸€è‡´åˆ†æ
            unique_words = set(normalized_words)
            matched_words = target_words.intersection(unique_words)
            
            # çµ±è¨ˆè¨ˆç®—
            word_count = len(normalized_words)
            unique_count = len(unique_words)
            matched_count = len(matched_words)
            coverage_rate = (matched_count / len(target_words) * 100) if target_words else 0
            precision = (matched_count / unique_count * 100) if unique_count else 0
            
            university_analysis[university_name] = {
                'source_file': source_file,
                'total_words': word_count,
                'unique_words': unique_count,
                'matched_words_count': matched_count,
                'target_coverage_rate': round(coverage_rate, 2),
                'extraction_precision': round(precision, 2),
                'matched_words': sorted(list(matched_words)),
                'word_frequencies': dict(Counter(normalized_words).most_common(20)),
                'ocr_confidence': item.get('ocr_confidence', 0),
                'pages_processed': item.get('pages_processed', 0)
            }
        
        return university_analysis
    
    def _extract_university_name(self, filename: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤§å­¦åãƒ»å­¦éƒ¨åã‚’æŠ½å‡º
        
        Args:
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å
            
        Returns:
            å¤§å­¦åãƒ»å­¦éƒ¨å
        """
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å¤§å­¦åãƒ»å­¦éƒ¨åã‚’æŠ½å‡ºï¼ˆå­¦éƒ¨åˆ¥ã«åˆ†é›¢ï¼‰
        if 'æ—©ç¨²ç”°å¤§å­¦' in filename:
            if 'æ³•å­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_æ³•å­¦éƒ¨'
            elif 'æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_æ”¿æ²»çµŒæ¸ˆå­¦éƒ¨'
            elif 'å•†å­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_å•†å­¦éƒ¨'
            elif 'æ–‡å­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_æ–‡å­¦éƒ¨'
            elif 'ç†å·¥å­¦éƒ¨' in filename:
                return 'æ—©ç¨²ç”°å¤§å­¦_ç†å·¥å­¦éƒ¨'
            else:
                return 'æ—©ç¨²ç”°å¤§å­¦'
        elif 'æ±äº¬å¤§å­¦' in filename:
            return 'æ±äº¬å¤§å­¦'
        elif 'æ…¶æ‡‰ç¾©å¡¾å¤§å­¦' in filename or 'æ…¶æ‡‰' in filename:
            return 'æ…¶æ‡‰ç¾©å¡¾å¤§å­¦'
        elif 'äº¬éƒ½å¤§å­¦' in filename:
            return 'äº¬éƒ½å¤§å­¦'
        elif 'ä¸€æ©‹å¤§å­¦' in filename:
            return 'ä¸€æ©‹å¤§å­¦'
        elif 'å¤§é˜ªå¤§å­¦' in filename:
            return 'å¤§é˜ªå¤§å­¦'
        elif 'æ˜æ²»å¤§å­¦' in filename:
            return 'æ˜æ²»å¤§å­¦'
        elif 'ç«‹æ•™å¤§å­¦' in filename:
            return 'ç«‹æ•™å¤§å­¦'
        elif 'ä¸Šæ™ºå¤§å­¦' in filename:
            return 'ä¸Šæ™ºå¤§å­¦'
        elif 'é’å±±å­¦é™¢å¤§å­¦' in filename:
            return 'é’å±±å­¦é™¢å¤§å­¦'
        else:
            # ãã®ä»–ã®å ´åˆã¯æœ€åˆã®å˜èªã‚’ä½¿ç”¨
            base_name = filename.replace('.pdf', '')
            parts = base_name.split('_')
            return parts[0] if parts else filename

    def generate_detailed_report(
        self, 
        target_file: str, 
        extraction_file: str, 
        output_file: str = None
    ) -> Dict:
        """
        è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆï¼ˆå¤§å­¦åˆ¥åˆ†æã‚’å«ã‚€ï¼‰
        
        Args:
            target_file: Target 1900ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            extraction_file: æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
            
        Returns:
            åˆ†æçµæœè¾æ›¸
        """
        logger.info("åˆ†æé–‹å§‹...")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        target_words = self.load_target1900(target_file)
        extracted_words, original_data = self.load_extracted_data(extraction_file)
        
        # åŸºæœ¬çµ±è¨ˆè¨ˆç®—
        basic_stats = self.calculate_coverage_stats(target_words, extracted_words)
        
        # é »åº¦å±¤åˆ¥åˆ†æ
        frequency_analysis = self.analyze_by_frequency_tiers(target_words, extracted_words)
        
        # å¤§å­¦åˆ¥åˆ†æ
        university_analysis = self.analyze_by_university(target_words, original_data)
        
        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = {
            'analysis_metadata': {
                'target_file': target_file,
                'extraction_file': extraction_file,
                'analysis_timestamp': pd.Timestamp.now().isoformat(),
                'extraction_summary': original_data.get('extraction_summary', {})
            },
            'coverage_statistics': basic_stats,
            'frequency_tier_analysis': frequency_analysis,
            'university_analysis': university_analysis,
            'recommendations': self._generate_recommendations(basic_stats, frequency_analysis)
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if output_file is None:
            output_file = "vocabulary_analysis_report.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ†æå®Œäº†ã€‚çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        
        # çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        self._print_summary(basic_stats)
        
        # å¤§å­¦åˆ¥ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        self._print_university_summary(university_analysis)
        
        return report
    
    def _generate_recommendations(self, basic_stats: Dict, frequency_analysis: Dict) -> List[str]:
        """
        åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ç”Ÿæˆ
        """
        recommendations = []
        
        coverage_rate = basic_stats['target_coverage_rate']
        precision = basic_stats['extraction_precision']
        
        if coverage_rate < 30:
            recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä½ã„ã§ã™ã€‚ã‚ˆã‚Šå¤šæ§˜ãªæ•™æã§ã®å­¦ç¿’ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        elif coverage_rate < 50:
            recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯ä¸­ç¨‹åº¦ã§ã™ã€‚é‡è¦å˜èªã®å¾©ç¿’ã«é‡ç‚¹ã‚’ç½®ã„ã¦ãã ã•ã„ã€‚")
        else:
            recommendations.append("Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯è‰¯å¥½ã§ã™ã€‚å¿œç”¨ãƒ¬ãƒ™ãƒ«ã®èªå½™å­¦ç¿’ã«é€²ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        if precision < 40:
            recommendations.append("æŠ½å‡ºã•ã‚ŒãŸå˜èªã®å¤šããŒTarget 1900å¤–ã§ã™ã€‚åŸºç¤èªå½™ã®å¼·åŒ–ãŒå¿…è¦ã§ã™ã€‚")
        
        # é »åº¦åˆ†æã«åŸºã¥ãæ¨å¥¨
        high_freq = frequency_analysis.get('high_frequency', {})
        if high_freq.get('coverage_rate', 0) > 80:
            recommendations.append("é«˜é »åº¦å˜èªã®Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ã¯å„ªç§€ã§ã™ã€‚")
        else:
            recommendations.append("é »å‡ºå˜èªã§Target 1900å¤–ã®èªå½™ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®ç¿’å¾—ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚")
        
        return recommendations
    
    def _print_summary(self, stats: Dict):
        """åˆ†æçµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ“Š Target 1900 èªå½™åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        print(f"Target 1900ç·å˜èªæ•°: {stats['target_total_words']:,}")
        print(f"æŠ½å‡ºå˜èªç·æ•°: {stats['extracted_total_words']:,}")
        print(f"æŠ½å‡ºãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {stats['extracted_unique_words']:,}")
        print(f"ä¸€è‡´å˜èªæ•°: {stats['matched_words_count']:,}")
        print()
        print(f"ğŸ¯ Target 1900ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {stats['target_coverage_rate']:.2f}%")
        print(f"ğŸ” æŠ½å‡ºèªå½™ç²¾åº¦: {stats['extraction_precision']:.2f}%")
        print()
        print("ä¸Šä½ä¸€è‡´å˜èª(é »åº¦é †):")
        for word, freq in list(stats['matched_word_frequencies'].items())[:10]:
            print(f"  â€¢ {word}: {freq}å›")
        print("="*60)
        
    def _print_university_summary(self, university_analysis: Dict):
        """å¤§å­¦ãƒ»å­¦éƒ¨åˆ¥åˆ†æçµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "="*60)
        print("ğŸ« å¤§å­¦ãƒ»å­¦éƒ¨åˆ¥ Target 1900 èªå½™åˆ†æçµæœ")
        print("="*60)
        
        for university_name, data in university_analysis.items():
            print(f"\nã€{university_name}ã€‘")
            print(f"  ãƒ•ã‚¡ã‚¤ãƒ«å: {data['source_file']}")
            print(f"  ç·å˜èªæ•°: {data['total_words']:,}")
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°: {data['unique_words']:,}")
            print(f"  Target 1900ä¸€è‡´æ•°: {data['matched_words_count']:,}")
            print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {data['target_coverage_rate']:.2f}%")
            print(f"  æŠ½å‡ºç²¾åº¦: {data['extraction_precision']:.2f}%")
            print(f"  OCRä¿¡é ¼åº¦: {data['ocr_confidence']:.1%}")
            print(f"  å‡¦ç†ãƒšãƒ¼ã‚¸æ•°: {data['pages_processed']}")
            
            # ä¸Šä½é »å‡ºå˜èªï¼ˆä¸€è‡´èªã®ã¿ï¼‰
            matched_words = set(data['matched_words'])
            top_matched = []
            for word, freq in data['word_frequencies'].items():
                if word in matched_words:
                    top_matched.append((word, freq))
                if len(top_matched) >= 5:
                    break
            
            if top_matched:
                print(f"  ä¸Šä½ä¸€è‡´å˜èª: {', '.join([f'{word}({freq})' for word, freq in top_matched])}")
        
        print("="*60)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    analyzer = VocabularyAnalyzer()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
    target_file = "target1900.csv"
    extraction_file = "extraction_results_pure_english.json"
    output_file = "vocabulary_analysis_report.json"
    
    try:
        # åˆ†æå®Ÿè¡Œ
        report = analyzer.generate_detailed_report(
            target_file=target_file,
            extraction_file=extraction_file,
            output_file=output_file
        )
        
        print(f"\nâœ… åˆ†æå®Œäº†ï¼è©³ç´°ã¯ {output_file} ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        
    except Exception as e:
        logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        raise

if __name__ == "__main__":
    main()